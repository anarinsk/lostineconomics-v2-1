<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Five (Deadly!) Questions on Regression and PCA | lostineconomics.com</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Five (Deadly!) Questions on Regression and PCA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="PCA에 관해 알고 싶은 다섯 가지" />
<meta property="og:description" content="PCA에 관해 알고 싶은 다섯 가지" />
<link rel="canonical" href="https://anarinsk.github.io/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html" />
<meta property="og:url" content="https://anarinsk.github.io/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html" />
<meta property="og:site_name" content="lostineconomics.com" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-12-30T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://anarinsk.github.io/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html","@type":"BlogPosting","headline":"Five (Deadly!) Questions on Regression and PCA","dateModified":"2019-12-30T00:00:00-06:00","datePublished":"2019-12-30T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://anarinsk.github.io/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html"},"description":"PCA에 관해 알고 싶은 다섯 가지","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/lostineconomics-v2-1/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://anarinsk.github.io/lostineconomics-v2-1/feed.xml" title="lostineconomics.com" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-159914345-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-159914345-1');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/lostineconomics-v2-1/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/lostineconomics-v2-1/">lostineconomics.com</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/lostineconomics-v2-1/about/">About</a><a class="page-link" href="/lostineconomics-v2-1/search/">Search</a><a class="page-link" href="/lostineconomics-v2-1/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Five (Deadly!) Questions on Regression and PCA</h1><p class="page-description">PCA에 관해 알고 싶은 다섯 가지</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2019-12-30T00:00:00-06:00" itemprop="datePublished">
        Dec 30, 2019
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/lostineconomics-v2-1/categories/#machine-learning">machine-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#question-one">Question One</a>
<ul>
<li class="toc-entry toc-h3"><a href="#in-common">In common</a>
<ul>
<li class="toc-entry toc-h4"><a href="#regression">Regression</a></li>
<li class="toc-entry toc-h4"><a href="#pca">PCA</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#difference">Difference</a>
<ul>
<li class="toc-entry toc-h4"><a href="#objective-function">Objective function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#question-two">Question Two</a>
<ul>
<li class="toc-entry toc-h3"><a href="#eigenvectors-in-pca">Eigenvectors in PCA</a></li>
<li class="toc-entry toc-h3"><a href="#sub-question">Sub-question</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#question-three">Question Three</a></li>
<li class="toc-entry toc-h2"><a href="#question-four">Question Four</a></li>
<li class="toc-entry toc-h2"><a href="#question-five">Question Five</a></li>
</ul><h2 id="question-one">
<a class="anchor" href="#question-one" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question One</h2>

<blockquote>
  <p>PCA는 Regression과 다른가요?</p>
</blockquote>

<p>좋은 질문이다. 아예 둘을 다른 방법이라고 보면 속편하겠지만 한번 쯤 이 질문을 고민해봤을 것이다.  답은 같기도 하고 다르기도 하다, 되겠다.</p>

<h3 id="in-common">
<a class="anchor" href="#in-common" aria-hidden="true"><span class="octicon octicon-link"></span></a>In common</h3>

<p>우선 둘 다 MSE(Mean Squared Error)를 극소화하는 목적함수를 지니고 있다. 차례로 살펴보자.</p>

<h4 id="regression">
<a class="anchor" href="#regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regression</h4>

<p>회귀분석의 목적함수와 극소화는 아래와 같다.</p>

\[\min_{\beta} (y - X \beta)^T (y - X \beta).\]

<p>여기서 각각 행렬 및 벡터의 크기를 확인해보자.</p>

<ul>
  <li>$y \in {\mathbb R}^n$, $X \in  {\mathbb R}^{n \times k}$, $\beta \in {\mathbb R}^k$</li>
</ul>

<p>개념적으로 말하면 regression은 일종의 목표 변수인 $y$와 이를 설명하기 위한 설명 변수인 regressors로 구성된 벡터 공간의  한 점 사이의 거리를 최소화하는 $\beta$를 찾는 것이다. 기하학적으로 보면 이 최소화는 $n$ 차원인 한 점에서 $y$에서 $k$ 차원인 $X$로 구성된 초평면으로 수선의 발을 내릴 경우 달성된다.</p>

\[X \beta = 
\begin{bmatrix}
x^1 ~ \dotsc ~ x^k
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
\vdots \\
\beta_k
\end{bmatrix} = 
\beta_1 x^1 + \dotsb + \beta_k x^k,\]

<p>where $x^j = [x_1^j, \dotsc, x_n^j]^T$ for $j = 1, \dotsc, k$. 즉 $x^j \in {\mathbb R}^n$ 벡터 $k$ 개로 이루어진 벡터 공간, 즉 $X$의 열 공간(column space)으로 $y$를 투영(project)한다는 의미이다. 이 열 공간은 $n$ 차원 안에 속한 $k$ 차원의 초평면이라는 점을 기억해두자. 아래 그림을 참고하자.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup></p>

<p align="center"><kbd>  <img src="https://github.com/anarinsk/lie-regression/blob/master/assets/imgs/reg-in-vectorspace.png?raw=true" width="350"></kbd></p>
<p>한편 위의 식을 연립방정식의 관점에서 바라보자.</p>

\[X \beta = y\]

<p>$X \in {\mathbb n \times k}$의 경우 $n &gt; k$이므로, 위 연립방정식의 해 $\beta$는 일반적으로 존재할 수 없다.  따라서 해 대신에 MSE를 극소화하는 새로운 목적 함수를 잡았다고 보면 좋다.</p>

<h4 id="pca">
<a class="anchor" href="#pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>PCA</h4>

<p>한편, PCA의 최소화는 다음의 목적 함수로 구현된다.</p>

<p>\(\begin{aligned}
\min_{w} &amp; \dfrac{1}{n}\sum_{i=1}^n \left( \Vert x_i \Vert^2 - 2(w \cdot x_i)^2 + 1 \right),
\end{aligned}\)</p>
<ul>
  <li>$w \in  {\mathbb R}^{k}$, $x_i \in  {\mathbb R}^{k}$</li>
</ul>

<p>목적 함수의 괄호 부분은 아래와 같이 도출된다.</p>

\[\begin{aligned}
\Vert x_i - (w \cdot x_i) w \Vert^2 &amp; =  
\Vert x_i \Vert^2 - 2 (w \cdot x_i)(w \cdot x_i) +  \Vert w \Vert^2 \\
&amp; = \Vert x_i \Vert^2 - 2(w \cdot x_i)^2 + 1
\end{aligned}\]

<p>역시 개념적으로 말하면 PCA는 $k$ 차원의 벡터가 $n$ 개 있을 때, $n$ 개의 벡터들을 길이 1의 유닛 벡터 $w( \in {\mathbb R}^{k})$로 프로젝션할 때, 그 거리 제곱의 합을 최소화하는 $w$를 찾는 것이다.</p>

<h3 id="difference">
<a class="anchor" href="#difference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Difference</h3>

<p>MSE를 극소화한다는 점은 같지만, 목적 함수의 형태와 목적 함수를 극대화하는 선택 변수가 다르다.</p>

<h4 id="objective-function">
<a class="anchor" href="#objective-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Objective function</h4>

<p>회귀 분석부터 보자. 회귀 분석은 $n$ 차원의 $y$를 $k$ 차원의 컬럼 스페이스를 지닌 $X$의 컬럼 스페이스로 프로젝션하는 것이다. 이때 프로젝션되는 위치가 곧 MSE를 극소화해주는 가중치, $k$ 차원의 $\beta$가 된다. 위 그림에서 보듯이 $y$에서 X 평면의 거리를 최소화하는 방법은 평면으로 수선의 발을 내리는 것 이외에는 없다. 즉 프로젝션 자체가 거리 최소화가 된다.</p>

<p>이제 PCA를 보자. PCA는 $X \in {\mathbb R}^{n \times k}$가 있을 때 이를 투영할 길이 1의 또다른 벡터 $w(\in {\mathbb R}^{k})$를 찾는 것이다. PCA의 경우 타겟 변수 같은 것이 없다. 그저 $k$ 의 벡터가 $n$ 개 있을 때 이들을 투영한 거리가 최소화되도록 유닛 벡터 $w$를 잡는다.</p>

<h2 id="question-two">
<a class="anchor" href="#question-two" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Two</h2>

<blockquote>
  <p>PCA에서 아이겐밸류와 아이겐벡터는 어떻게 등장하게 되나요?</p>
</blockquote>

<p>자세한 설명은 <a href="https://anarinsk.github.io/lie-math_pca/">여기</a>를 참고하면 된다. 핵심만 요약해보자.</p>

<ul>
  <li>PCA의 목적 함수를 최적화하는 벡터 $w$를 잡기 위해서 $X w$의 분산을 최대화하면 된다.</li>
  <li>분산은 $w^T \Sigma w$가 된다.</li>
  <li>$w \cdot w =1$의 조건을 넣어 제약 아래의 극대화 조건을 찾으면 정확히 아이겐벨류와 아이겐벡터를 찾는 계산과 동일하다.</li>
</ul>

<p>극대화 문제에서 도출되는 라그랑지 승수 $\lambda$는 아이겐밸류가 된다. 아이겐밸류 $k$ 개가 있을 때 이를 큰 순서대로 나열했다고 하자.  즉  $\lambda_1 &gt; \lambda_2 &gt; \dotsc &gt; \lambda_k$. 이제 아이겐밸류가 큰 순서대로 아이겐벡터를 잡으면 MSE가 작은 순서대로 $w$를 택하는 것이 된다.</p>

<p>여기서 PCA가 MSE를 극소화하는 문제에서 출발했지만 왜 차원 축소의 문제로 변했는지 알 수 있다. MSE 최소화라는 목적 함수에서 볼 때 이를 달성하는 $k$ 개의 아이겐벡터 중에서 임의로 $l(&lt;k)$ 개를 선택할 수 있게 해준다.</p>

<h3 id="eigenvectors-in-pca">
<a class="anchor" href="#eigenvectors-in-pca" aria-hidden="true"><span class="octicon octicon-link"></span></a>Eigenvectors in PCA</h3>

<p>PCA 분석에서 등장하는 아이겐벡터는 너무 좋은 특징을 지니고 있다. 우선, 해당 아이겐벡터는 서로 직교한다. 즉,</p>

\[w^i \cdot w^j = 
\begin{cases}
1 &amp; \text{for $i = j$} \\
0 &amp; \text{for $i \neq j$}
\end{cases}\]

<p>아마도 PCA를 2차원에 도해할 경우 아래와 같은 그림을 많이 봤을 것이다.</p>

<p align="center"><kbd>
  <img src="https://github.com/anarinsk/lie-qa_reg_pca/blob/master/assets/imgs/PCA-2D.png?raw=true" width="350">
</kbd></p>

<p>1개의 관찰이 2차원 벡터이므로 이 자료에서는 두 개의 주성분이 나올 것이다. 이 두 개의 주성분은 반드시 직교(orthogonal)해야 할까? 꼭 그렇다는 장담은 없다. 그런데 앞서 살펴본 MSE의 극소화 과정에서 도출되는 결과에 따라서 주성분들끼리는 반드시 직교해야 한다. 이른바 ‘주성분’이 대칭 행렬(분산-공분산 행렬)에서 나온 아이겐밸류이기 때문이다. 그림에서 보듯이 하나의 주성분(북동향)이 다른 주성분(북서향)보다 크다. 만일 여기서 차원축소를 한다면 첫번째 주성분을 택하게 될 것이다. 일반적으로 $n$ 개의 주성분이 있을 때 이중 $k$ 개를 택한다면, 분산이 큰 순서대로 $k$개를 택하면 된다.</p>

<h3 id="sub-question">
<a class="anchor" href="#sub-question" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sub-question</h3>

<blockquote>
  <p>PCA에서 차원 축소의 의미를 보다 구체적으로 설명해주실 수 있을까요?</p>
</blockquote>

<p>이렇게 보자. 앞서 보았던 대로 $X \in {\mathbb R}^{$는 $(n \times k}$ 행렬다. 이를 행 벡터로 잘라서 보자. 즉,</p>

\[X = 
\begin{bmatrix}
x_1 \\
\vdots \\
x_n 
\end{bmatrix},~\text{where~} 
x_i = [x^1,~\dotsc~, x^k].\]

<p>즉. 행 벡터는 $k$ 개의 피처를 지닌 하나의 관찰이 된다. 이제 앞서 우리가 구한 아이겐벡터를 여기에 적용해보자. 편의상 1개로 차원 축소를 한 경우를 가정하겠다. 이때 차원 축소에 활용되는 아이겐벡터는 아이겐밸류가 가장 큰 값에 해당하는 벡터 $w^1$라고 하자. $w^1 \in {\mathbb R}^{k}$이다. 이제,</p>

\[X w^1 = \underset{n \times 1}{\rm{PC}^1}\]

<p>첫번째 PC로 변환된 $X$는 $k$ 개의 피처에서 1개의 피처로 변환된다. 만일 주성분(PC)으로 $l$ 개를 택했다면 아이겐밸류가 큰 순서대로 위와 같은 과정을 거치면 되겠다. 각 아이겐밸류와 아이겐벡터의 쌍이 $w^j$, $\lambda^j$라고 할 때,</p>

\[\begin{aligned}
X w^1 &amp; ={\rm PC}^1 \\
&amp;~\vdots \\
X w^l &amp; ={\rm PC}^l,~\text{where $\lambda^1 &gt; \dotsc &gt; \lambda^l$. }
\end{aligned}\]

<p>그리고 이 PC 값들을 모으면 $k$ 개에서 $l$ 개로 차원이 축소된 행렬을 얻을 수 있다.</p>

\[\underset{n \times l}{ [{\rm PC}^1,~\dotsc~,{\rm PC}^l] }\]

<h2 id="question-three">
<a class="anchor" href="#question-three" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Three</h2>

<blockquote>
  <p>그림으로 보다 쉽게 볼 수는 없을까요?</p>
</blockquote>

<p>그림으로 간단하게 살펴보자.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup></p>

<p align="center"><kbd>
  <img src="https://github.com/anarinsk/lie-qa_reg_pca/blob/master/assets/imgs/regression.png?raw=true" width="700">
</kbd></p>

<p>먼저 회귀 분석이다. $y$를 $x$에 대해서 회귀($y \sim x$)하는 경우와 $x$를 $y$에 대해서 회귀($x \sim y$)하는 경우는 다르다.</p>

<ul>
  <li>$y \sim x$:  벡터 $y(\in {\mathbb R}^{n})$를 $x$ 평면으로 프로젝션한다. 여기서 $y$는 관찰의 $y$ 값만을 모아서 만든 벡터다. 즉, 개별 $y$를 $\alpha + \beta x$ 벡터로 수선의 발을 내릴 때 이를 최소화하는 $\alpha$, $\beta$를 찾는 것이다. 그림에서 보라색이 나타내는 바와 같다.</li>
  <li>$x \sim y$: 벡터 $x(\in {\mathbb R}^{n})$를 $y$ 평면으로 프로젝션한다. 개별 $x$를 $\alpha + \beta y$ 벡터로 수선의 발을 내릴 때 이를 최소화하는 $\alpha$, $\beta$를 찾는 것이다. 위의 그림에서 초록색이 이를 나타낸다.</li>
</ul>

<p align="center"><kbd>
  <img src="https://github.com/anarinsk/lie-qa_reg_pca/blob/master/assets/imgs/pca.png?raw=true" width="700">
</kbd></p>

<p>PCA는 그림에서 보듯이 관찰에서 임의의 벡터 $w = (w_1, w_2)$로 프로젝션한다. 즉, 그림에서 분홍색이 나타내는 바와 같다.</p>

<h2 id="question-four">
<a class="anchor" href="#question-four" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Four</h2>

<blockquote>
  <p>하나는 지도 학습, 다른 하나는 비지도 학습으로 이해하면 될까요?</p>
</blockquote>

<p>얼추 맞는 이야기라고 생각한다. 기계 학습에 조예가 없어서 자신있게 답하기는 힘들다. 다만 회귀 분석이 일종의 정답지($y$)를 갖고 있는 형태이기 때문에 지도 학습이 될 것이고 PCA의 경우 정답지 없이 관찰 전체를 대상으로 하기 때문에 비지도 학습으로 볼 수도 있겠다.</p>

<h2 id="question-five">
<a class="anchor" href="#question-five" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Five</h2>

<blockquote>
  <p>회귀 분석과 PCA를 섞을 수 있는 방법은 없나요?</p>
</blockquote>

<p>왜 없겠는가! 이른바 PCA 회귀 분석이라는 게 있다. 지금까지 잘 따라왔다면 PCA 회귀 분석이 어떤 형태가 될지 쉽게 짐작할 수 있다.</p>

<ul>
  <li>행렬 $X(\in {\mathbb R}^{n \times k})$를 PCA를 통해서 $X’(\in {\mathbb R}^{n \times l})$으로 축소한다. ($l &lt;k$)</li>
  <li>$y$를 $X’$에 대해서 회귀한다(“Do Regression $y$ on $X’$”).</li>
</ul>

<p>이런 흐름으로 진행되는 것이 PCA 회귀다.</p>

<p>그런데 PCA 회귀는 그다지 많이 사용되지 않는다. 왜 그럴까? 보통 기계 학습에서는 회귀 분석을 예측(prediction)의 한 가지 방법으로 이해하곤 한다. 회귀 분석은 선형이기 때문에 사실 여타의 비선형 기계 학습 방법에 비해서 예측 기법으로서는 원시적인 방법이다. 하지만 회귀 분석은 높은 해석력을 지니고 있다. 만일 적절 $X$를 선정할 수 한다면, 인과관계를 해석하는 틀로서도 활용할 수도 있다. 회귀 분석에서 $\beta$를 살피고 이를 해석하는 경우가 많은 것이 이 때문이다. 그런데 PCA 회귀를 할 경우 이러한 회귀 분석의 장점이 거의 사라지고 만다. 축소된 PC를 해석하는 것이 쉽지 않기 때문이다.</p>

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3NTI1OTIzMDZdfQ==
-->
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">
      <p>보다 상세한 내용은 <a href="https://anarinsk.github.io/lie-regression/">여기</a>를 참고하라. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://shankarmsy.github.io/posts/pca-vs-lr.html">여기</a>에서 가져왔다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="anarinsk/lostineconomics-v2-1"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/lostineconomics-v2-1/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/lostineconomics-v2-1/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/lostineconomics-v2-1/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A useless and aimless blog</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/anarinsk" title="anarinsk"><svg class="svg-icon grey"><use xlink:href="/lostineconomics-v2-1/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/anarinsk" title="anarinsk"><svg class="svg-icon grey"><use xlink:href="/lostineconomics-v2-1/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
