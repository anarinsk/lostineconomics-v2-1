<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://anarinsk.github.io/lostineconomics-v2-1/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anarinsk.github.io/lostineconomics-v2-1/" rel="alternate" type="text/html" /><updated>2021-08-31T16:22:13-05:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/feed.xml</id><title type="html">lostineconomics.com</title><subtitle>A useless and aimless blog</subtitle><entry><title type="html">Test of Katex in Fastpages</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/2021/08/31/Testing-Katex.html" rel="alternate" type="text/html" title="Test of Katex in Fastpages" /><published>2021-08-31T00:00:00-05:00</published><updated>2021-08-31T00:00:00-05:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/2021/08/31/Testing-Katex</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/2021/08/31/Testing-Katex.html">&lt;h2 id=&quot;purpose&quot;&gt;Purpose&lt;/h2&gt;

&lt;p&gt;This post serves only to show the problem of Katex in fastpages. If the problem is solved, this post will be deleted.&lt;/p&gt;

&lt;h2 id=&quot;setting&quot;&gt;Setting&lt;/h2&gt;

&lt;p&gt;I’m not touch anything in default setting for latest version of fastpage. Using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;displaystyle&lt;/code&gt; by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$$
\begin{aligned}
R^*=\underset{RR^t=I,\det(R)=1}{\operatorname{argmin}}\sum_{i=1}^n|RX_i-Y_i\|^2_2.
\end{aligned}
$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is rendered with default fastpage option in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;math_engine: null&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
R^*=\underset{RR^t=I,\det(R)=1}{\operatorname{argmin}}\sum_{i=1}^n|RX_i-Y_i\|^2_2.
\end{aligned}
$$&lt;/div&gt;

&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; is set as&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kramdown&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;math_engine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;katex&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;GFM&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;auto_ids&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hard_wrap&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;syntax_highlighter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rouge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/etc/with_katex.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; is set as&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kramdown&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;math_engine&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;null&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;GFM&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;auto_ids&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hard_wrap&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;syntax_highlighter&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rouge&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/etc/with_mathjax.png?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The equation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;katex&lt;/code&gt; option is not &lt;strong&gt;Italicized&lt;/strong&gt;, and the font size is smaller than the one with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;null&lt;/code&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Purpose This post serves only to show the problem of Katex in fastpages. If the problem is solved, this post will be deleted. Setting I’m not touch anything in default setting for latest version of fastpage. Using displaystyle by using $$. $$ \begin{aligned} R^*=\underset{RR^t=I,\det(R)=1}{\operatorname{argmin}}\sum_{i=1}^n|RX_i-Y_i\|^2_2. \end{aligned} $$ This is rendered with default fastpage option in _config.yml; math_engine: null. $$ \begin{aligned} R^*=\underset{RR^t=I,\det(R)=1}{\operatorname{argmin}}\sum_{i=1}^n|RX_i-Y_i\|^2_2. \end{aligned} $$ Problem _config.yml is set as kramdown: math_engine: katex input: GFM auto_ids: true hard_wrap: false syntax_highlighter: rouge The result is _config.yml is set as kramdown: math_engine: null input: GFM auto_ids: true hard_wrap: false syntax_highlighter: rouge The result is The equation with katex option is not Italicized, and the font size is smaller than the one with null.</summary></entry><entry><title type="html">Docker + Jupyter + 한글 폰트</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/coding/docker/jupyter/2021/05/09/docker_korfont.html" rel="alternate" type="text/html" title="Docker + Jupyter + 한글 폰트" /><published>2021-05-09T00:00:00-05:00</published><updated>2021-05-09T00:00:00-05:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/coding/docker/jupyter/2021/05/09/docker_korfont</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/coding/docker/jupyter/2021/05/09/docker_korfont.html">&lt;h2 id=&quot;build도-어렵지-않다&quot;&gt;Build도 어렵지 않다!&lt;/h2&gt;

&lt;p&gt;이 글은 &lt;a href=&quot;https://anarinsk.github.io/lostineconomics-v2-1/docker/data-science/2020/09/24/install-hangul-in-docker.html&quot;&gt;이 포스팅&lt;/a&gt;에서 이어진다. 컨테이너를 올린 뒤 컨테이너 내 터미널에서 sh 스크립트를 실행하지 않고 한글 문제 처음부터 해결할 수 없을까?&lt;/p&gt;

&lt;p&gt;사실 이 포스팅을 쓰게 된 동기는 따로 있다. 글을 쓰는 시점에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matplotlib.font_manager._rebuild()&lt;/code&gt;가 사라졌다! Jupyter에서 폰트 목록을 다시 생성할 다른 방법을 찾아야 했다. docker-compose로 필요한 이미지를 끌어올 때 단순히 이미지를 끌어오는 대신 특화된 형태로 build를 할 수도 있다. 이전 포스팅에서 소개한 방법에서는 build 옵션을 사용하지 않았다. 일단 Build가 꽤 거창하게 느껴졌기 때문이다. 적당한 ubuntu 버전을 끌어오고 여기에 Python, Jupyter를 깔고… 이런 빌드 과정이 꽤 험난하고 불필요해 보였다. Jupyter의 경우 데이터사이언스를 위한 잘 갖춰진 도커 이미지가 이미 있고, 이를 그대로 쓰면 큰 문제는 없다.&lt;/p&gt;

&lt;p&gt;이제 문제가 생겼으니 해결책을 찾아야 한다. 이 특화된 이미지에 기반해서 빌드를 하면 한글 문제와 같은 특정하게 발생하는 문제를 미리 해결할 수 있지 않을까? 이후 소개하는 방법을 통해 확인한 내용은 다음과 같다. 이용 조건이 비슷하다면 참고해보시라.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;docker-compose를 쓸 때 이미지를 지정하는 image 부분을 제외한 나머지는 거의 그대로 재사용이 가능하다.&lt;/li&gt;
  &lt;li&gt;image를 그대로 끌어오는 것이나 몇 가지 명령어를 넣어서 build를 하는 것이나 시간 상으로는 크게 차이가 없다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-implement&quot;&gt;How to Implement&lt;/h2&gt;

&lt;p&gt;본론이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;docker-compose를 위한 file이 있는 디렉토리 아래 dockerfiles 디렉토리를 하나 더 만든다 (디렉토리 이름은 각자 알아서). 여기에 docker build를 위한 파일을 넣어어둔다.&lt;/li&gt;
  &lt;li&gt;docker-compose용 파일을 약간 수정한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;실행-환경&quot;&gt;실행 환경&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Windows 10 WSL 2 + Ubuntu 20.04&lt;/li&gt;
  &lt;li&gt;Docker for Desktop (Windows)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dockerfiles&quot;&gt;dockerfiles&lt;/h3&gt;

&lt;p&gt;데이터 사이언스를 위한 Jupyter docker 파일을 예시로 들겠다. 다른 이미지라면 응용해서 쓰면 된다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM jupyter/datascience-notebook:latest
&lt;span class=&quot;c&quot;&gt;# Declare root as user &lt;/span&gt;
USER root
&lt;span class=&quot;c&quot;&gt;# Update Ubuntu &lt;/span&gt;
RUN &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/archive.ubuntu.com/mirror.kakao.com/g'&lt;/span&gt; /etc/apt/sources.list &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'s/security.ubuntu.com/mirror.kakao.com/g'&lt;/span&gt; /etc/apt/sources.list
&lt;span class=&quot;c&quot;&gt;# Install Nanum for Korean Font &lt;/span&gt;
RUN apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; upgrade &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; fonts-nanum&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; fc-cache &lt;span class=&quot;nt&quot;&gt;-fv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-fr&lt;/span&gt; ~/.cache/matplotlib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM jupyter/datascience-notebook:latest&lt;/code&gt; 끌어올 이미지를 지정한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;USER root&lt;/code&gt; 이미지 내에서 root 권한을 부여한다. 이후 sudo는 안 써도 된다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN sed -i...&lt;/code&gt; 끌어온 우분투 이미지가 미국 기준이기 때문에 업데이트 서버 주소 역시 미국이다. 이걸 국내에서 가장 안정적이고 빠른 카카오 서버로 바꾼다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get -y upgrade...&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;우분투 배포판의 업데이트 및 업그레이드를 실행한다.&lt;/li&gt;
      &lt;li&gt;나눔 폰트를 깔아준다.&lt;/li&gt;
      &lt;li&gt;docker 이미지 배포판의 폰트 캐시를 지운다.&lt;/li&gt;
      &lt;li&gt;Jupyter의 폰트 캐시를 지운다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;docker-composeyml&quot;&gt;docker-compose.yml&lt;/h3&gt;

&lt;p&gt;docker-compose를 위한 yml을 예시한다. 이 내용 역시 각자 환경에 맞게 변형해서 쓰면 된다. 파일 이름을 “docker-jupyter.yml”이라고 하자.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: &lt;span class=&quot;s1&quot;&gt;'3'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
services:
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
    jupyter-ds:
      build:
        context: &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
        dockerfile: ./dockerfiles/dockerfile-jupyter
      user: root
      environment:
        - &lt;span class=&quot;nv&quot;&gt;GRANT_SUDO&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt;
        - &lt;span class=&quot;nv&quot;&gt;JUPYTER_ENABLE_LAB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt;
        - &lt;span class=&quot;nv&quot;&gt;JUPYTER_TOKEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;={&lt;/span&gt;YOUR-PASSWORD&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      volumes:
        - /mnt/c/Users/&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;YOUR-DIR&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:/home/jovyan/github-anari
      ports:
        - &lt;span class=&quot;s2&quot;&gt;&quot;8888:8888&quot;&lt;/span&gt;
      container_name: &lt;span class=&quot;s2&quot;&gt;&quot;jupyter-ds&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# End of yml&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;위의 yml 파일에서 “{}”로 처리된 부분은 각자 채워 넣으면 된다.&lt;/li&gt;
  &lt;li&gt;image 대신 build 명령어를 사용했다. 앞서 지정한 dockerfiles 디렉토리 내의 도커 명령어를 통해 빌드를 수행한다. 이렇게 빌드된 이미지는 처음부터 matplotlib 사용 시 한글 구현에 아무런 문제가 없다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;docker-compose &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; /mnt/&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;YOUR DIR&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;/docker-jupyter.yml &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jupyter-ds&quot;&lt;/span&gt; up &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실행 옵션은 다음과 같다. 역시 “{}”는 각자의 환경에 맞게 바꾸면 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-f&lt;/code&gt; 도커 콤포즈를 특정 파일로 실행하기 위한 옵션이다. 만일 이를 안쓰려면 yml 파일의 이름을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;로 두고 해당 디렉토리 안에서 실행하면 된다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-p&lt;/code&gt; 콤포즈 안에 묶인 서비스의 이름을 나타낸다. 같은 네트워크로 묶이며 이 이름을 네트워크 이름으로 갖는다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;up&lt;/code&gt; yml 내에 있는 포함된 콘테이너를 가동한다.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-d&lt;/code&gt; 디태치 모드, 즉 별도의 실행되는 과정으로 실행한다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="coding" /><category term="docker" /><category term="jupyter" /><summary type="html">Build도 어렵지 않다!</summary></entry><entry><title type="html">VS Code로 천하통일!</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/coding/2021/02/02/all_for_vs_code.html" rel="alternate" type="text/html" title="VS Code로 천하통일!" /><published>2021-02-02T00:00:00-06:00</published><updated>2021-02-02T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/coding/2021/02/02/all_for_vs_code</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/coding/2021/02/02/all_for_vs_code.html">&lt;p&gt;각자 언어에 맞는 IDE가 있다. 예를 들어 R에는 RStudio가 가장 잘 어울린다. 언어 별로 별도의 IDE 보다는 모든 것을 한방에 해결할 수는 없을까? 이러한 목적에 복무하는 코더를 위한 좋은 에디터들이 많이 있다. 하지만 설정이 쉽지 않고 아무래도 나 같은 문송한 존재들에게는 접근성이 떨어진다. 그리고 대체로 유료다! 이 사이에서 타협할 수 있는 제품이 없을까?&lt;/p&gt;

&lt;p&gt;MS에서 제공하는 무료 (만능) 에디터 VS Code는 여기서 제법 괜찮은 대안이다. 각설하고 파이썬, 줄리아, R을 VS code로 부리는 데 필요한 준비물을 살펴보자.&lt;/p&gt;

&lt;h2 id=&quot;필요-조건&quot;&gt;필요 조건&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Windows 10 with WSL 2&lt;/li&gt;
  &lt;li&gt;VS Code
    &lt;ul&gt;
      &lt;li&gt;Global extension
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Remote-Container&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Container extension
        &lt;ul&gt;
          &lt;li&gt;Julia&lt;/li&gt;
          &lt;li&gt;Jupyter&lt;/li&gt;
          &lt;li&gt;Python&lt;/li&gt;
          &lt;li&gt;R&lt;/li&gt;
          &lt;li&gt;R LSP Client&lt;/li&gt;
          &lt;li&gt;Radian&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Docker Desktop&lt;/li&gt;
  &lt;li&gt;jupyter/datascience-notebook:latest (from dockerhub)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개별 소프트웨어를 로컬 머신에 깔아서 쓸 수도 있고, 이 경우 역시 아래를 참고해 설정하는 데 어려움이 없으리라 본다. 여기서는&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;WSL 2를 통해 docker를 활성화하고&lt;/li&gt;
  &lt;li&gt;jupyter 개발자가 직접 관리하는 Data Science 노트북을 끌어와&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VS Code를 통해 부리는 방법을 소개한다. 위의 적은 두 가지 사항은 &lt;a href=&quot;https://anarinsk.github.io/lostineconomics-v2-1/docker/data-science/2020/09/23/docker-humble-practice.html&quot;&gt;이 포스팅&lt;/a&gt;을 참고하자. 간단히 결론만 요약하면 다음과 같다. 도커를 쓰면 별도의 인스톨이 필요 없고 뭔가 꼬였을 때 해당 컨테이너만 날려버리면 된다. 위에 소개한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jupyter/datascience&lt;/code&gt; 컨테이너는 다소 큰 용량이지만 잘 갖춰진 파이썬, 줄리아, R의 프리셋을 제공한다.&lt;/p&gt;

&lt;h2 id=&quot;핵심&quot;&gt;핵심&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/all-in-vs_code/avscode_1.png?raw=true&quot; alt=&quot;&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;VS Code의 익스텐션 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Remote-Container&lt;/code&gt; 컨테이너 접속 기능을 활용해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datascience-notebook&lt;/code&gt; 컨테이너 접속한다.
    &lt;ul&gt;
      &lt;li&gt;오른쪽 하단에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&amp;lt;&lt;/code&gt; 표시된 부분을 클릭하자. 위의 그림을 볼 수 있다. 여기에서 “Remote-Containers: Attach to Running Container…“를 클릭하면 현재 돌아가고 있는 docker 컨테이너를 볼 수 있다. 미리 docker를 통해 돌려 둔 datascience 컨테이너를 선택하자.&lt;/li&gt;
      &lt;li&gt;이 컨테이너는 안에 python, jupyterlab, R, Julia를 모두 갖고 있다. 따라서 이 녀석 하나만 도커에 올리면 된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;각각의 언어에 접근하기 위해서 필요한 VS Code Extension을 설치한다.
    &lt;ul&gt;
      &lt;li&gt;Julia, Python의 경우 공식 확장이 있어서 그대로 쓰면 된다.&lt;/li&gt;
      &lt;li&gt;R의 경우 비공식 확장이지만 약간의 세팅을 거치면 꽤 근사하게 사용할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;for-julia&quot;&gt;For Julia&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;필요한 줄리아 작업 파일을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.jl&lt;/code&gt; 확장자로 만든다.&lt;/li&gt;
  &lt;li&gt;VS Code가 알아서 잘 잡아서 줄리아 커널과 연결시켜준다.&lt;/li&gt;
  &lt;li&gt;간략한 실행 명령 체계를 살펴보자. 자세한 내용은 &lt;a href=&quot;https://www.julia-vscode.org/docs/stable/userguide/runningcode/&quot;&gt;여기&lt;/a&gt;를 참고하자.
    &lt;ul&gt;
      &lt;li&gt;코드 셀 구분은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;##&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;CTRL + ENTER 해당 라인 실행&lt;/li&gt;
      &lt;li&gt;ALT + ENTER 코드 블럭 실행 (드래그앤드롭 선택)&lt;/li&gt;
      &lt;li&gt;SHIFT + ENTER 코드 셀 실행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;패키지 설치는 두가지로 가능하다.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Julia 콘솔에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;]&lt;/code&gt;를 치면 패키지 관리 모드로 들어간다.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkg&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SOMETHING&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;ul&gt;
      &lt;li&gt;이렇게 설치할 경우는 별도로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pkg&lt;/code&gt;를 호출하지 않도 설치가 가능하다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;ol&gt;
      &lt;li&gt;아래와 같이 설치할 수도 있다.&lt;/li&gt;
    &lt;/ol&gt;

    &lt;div class=&quot;language-julia highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pkg&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;Pkg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Plots&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Pkg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PyPlot&quot;&lt;/span&gt;&lt;span class=&quot;x&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Julia는 JIT를 쓰기 때문에 패키지를 설치하고 명령을 구동하는 데 시간이 오래 걸린다. 이 과정을 작업을 시작하기 전에 미래 해두는 것이 좋다. 즉, 필요한 경우 미리 컴파일을 해놓는 것이 좋다.&lt;/li&gt;
  &lt;li&gt;QuantEcon에 소개된 &lt;a href=&quot;https://julia.quantecon.org/getting_started_julia/julia_by_example.html&quot;&gt;사례&lt;/a&gt;를 따르자.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;for-python&quot;&gt;For Python&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;필요한 파일을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.py&lt;/code&gt; 확장자로 만든다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;간략한 명령어&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Code 블럭의 구별은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#%%&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Python은 VS Code에서 거의 완벽하고 편리하게 지원이 된다.&lt;/li&gt;
      &lt;li&gt;별다른 설명이 필요하지 않다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;for-rstat&quot;&gt;For Rstat&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;익스텐션 두 개를 깐다.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=Ikuyadeu.r&quot;&gt;R Support&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=REditorSupport.r-lsp&quot;&gt;R LSP Client&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다음으로 R 내에서 LSP와 연결할 패키지를 설치한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-r highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;remotes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;install_github&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;REditorSupport/languageserver&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;마지막으로 Rstat에서 편리하게 사용할 터미널 앱을 깐다. 파이썬으로 제작되었다. 다행스럽게도 DS 도커는 이미 파이썬을 잘 지원하기 때문에 쉽게 쓸 수 있다. 주의할 것은 Rstat 내부가 아니라 그냥 docker의 bash에 접속한 상태에서 깔아야 한다는 것이다. Radian은 텍스트 상에서 모든 정보를 편리하게 보여주기 때문에 VS Code의 부족함을 잘 메워준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; radian 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;이제 VS Code에서 설정 몇 가지를 바로 잡아 줘야 한다.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;File &amp;gt; Preferences &amp;gt; Settings&lt;/code&gt;로 가자&lt;/li&gt;
      &lt;li&gt;옆에 탭을 보면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Extensions&lt;/code&gt;가 있을 것이다. 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt;로 찾아가자.&lt;/li&gt;
      &lt;li&gt;Extension의 설정을
        &lt;ul&gt;
          &lt;li&gt;User, Remote, Workspace 등 상황에 맞게 다양하게 정할 수 있다.&lt;/li&gt;
          &lt;li&gt;여기서는 Workspace에 하도록 하겠다.
            &lt;ul&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R: Bracketed Paste&lt;/code&gt; 항목 체크&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R › Rterm: Linux&lt;/code&gt;
                &lt;ul&gt;
                  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/opt/conda/bin/radian&lt;/code&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R: Session Watcher&lt;/code&gt; 항목 체크&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;설정 항목에서 json 파일로 한방에 해결할 수도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;jupyterlab&quot;&gt;Jupyterlab&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipynb&lt;/code&gt;를 확장자로 해서 노트북을 쓰고 싶다면, 그냥 만들어 쓰면 된다!&lt;/li&gt;
  &lt;li&gt;VS Code에 Jupyter 확장이 있기 때문에 보통의 웹 브라우저에서 쓰는 Jupyter 노트북과 거의 동일한 기능을 제공한다. 웹 노트북에서와 마찬가지로 파이썬, 줄리아, R의 커널을 선택하면 된다 (아래 그림의 오른쪽 상단 박스). jupyterlab과 거의 비슷한 인터페이스를 지니고 있다. 오히려 주피터에 비해 복잡하지 않아서 좋다고 느낄지도 모르겠다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/all-in-vs_code/avscode_3.png?raw=true&quot; alt=&quot;&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;setting-for-remote-connection&quot;&gt;Setting for Remote Connection&lt;/h2&gt;

&lt;p&gt;앞서 보았듯이 도커를 활용하면 윈도우 혹은 다른 플랫폼의 VS Code를 통해 네트워크로 도커 내의 컨테이너로 접근하게 된다. 따라서 작업을 마치고 나오게 되면 매번 해당 컨테이너에 다시 접속을 해주고 워크 스페이스 등 여러가지 작업을 해줘야 하는 번거로움이 있다. 항상 도커 컨테이너를 거쳐 작업한다면 그냥 한방에 작업하던 환경이 뜨는 편이 좋다면 아래와 같이 하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/all-in-vs_code/avscode_2.png?raw=true&quot; alt=&quot;&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;만일 위와 같은 화면에 Remote Exlorer에서 뜨지 않는다면 컨테이너가 부착되지 않는 것이다. 컨테이너를 부착하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/all-in-vs_code/avscode_4.png?raw=true&quot; alt=&quot;&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Extension&lt;/code&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;REMOTE EXPLORER&lt;/code&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DETAILS&lt;/code&gt; 탭에서 가운데 설정 모양이 “Open Container Configuration File”이다.&lt;/li&gt;
  &lt;li&gt;옆 에디터 창에 json 형식의 설정이 뜬뜬다. 여기서 관련 extension 등의 설정을 지정할 수 있다. 이렇게 설정을 해두면 해당 컨테이너를 부착할 때 설정한 대로 자동으로 옵션들이 적용된다. 매우 편리하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;extensions&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ikuyadeu.r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;julialang.language-julia&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ms-python.python&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ms-toolsai.jupyter&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;REditorSupport.r-lsp&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;workspaceFolder&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/home/jovyan/github-anari&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;forwardPorts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;43463&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;		
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;settings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;jupyter.alwaysTrustNotebooks&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;r.bracketedPaste&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;r.sessionWatcher&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
		&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;r.rterm.linux&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/opt/conda/bin/radian&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
	&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/all-in-vs_code/avscode_5.png?raw=true&quot; alt=&quot;&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;File&lt;/code&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Perefences&lt;/code&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Settings&lt;/code&gt;에서도 설정에 접근할 수 있다.&lt;/li&gt;
  &lt;li&gt;VS Code의 설정은 두가지 축을 지닌다. 하나는 설정 전체를 담은 json 파일이다. 왼쪽 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; 표시된 항목에서처럼 카테고리 별로 구별되어 쉽게 접근할 수 있다. 이 하나의 설정을 위 그림에서 보듯이 User, Remote(컨네이터), Workspace 별로 별도로 지정해 활용할 수도 있다.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="coding" /><summary type="html">각자 언어에 맞는 IDE가 있다. 예를 들어 R에는 RStudio가 가장 잘 어울린다. 언어 별로 별도의 IDE 보다는 모든 것을 한방에 해결할 수는 없을까? 이러한 목적에 복무하는 코더를 위한 좋은 에디터들이 많이 있다. 하지만 설정이 쉽지 않고 아무래도 나 같은 문송한 존재들에게는 접근성이 떨어진다. 그리고 대체로 유료다! 이 사이에서 타협할 수 있는 제품이 없을까? MS에서 제공하는 무료 (만능) 에디터 VS Code는 여기서 제법 괜찮은 대안이다. 각설하고 파이썬, 줄리아, R을 VS code로 부리는 데 필요한 준비물을 살펴보자. 필요 조건 Windows 10 with WSL 2 VS Code Global extension Remote-Container Container extension Julia Jupyter Python R R LSP Client Radian Docker Desktop jupyter/datascience-notebook:latest (from dockerhub) 개별 소프트웨어를 로컬 머신에 깔아서 쓸 수도 있고, 이 경우 역시 아래를 참고해 설정하는 데 어려움이 없으리라 본다. 여기서는 WSL 2를 통해 docker를 활성화하고 jupyter 개발자가 직접 관리하는 Data Science 노트북을 끌어와 VS Code를 통해 부리는 방법을 소개한다. 위의 적은 두 가지 사항은 이 포스팅을 참고하자. 간단히 결론만 요약하면 다음과 같다. 도커를 쓰면 별도의 인스톨이 필요 없고 뭔가 꼬였을 때 해당 컨테이너만 날려버리면 된다. 위에 소개한 jupyter/datascience 컨테이너는 다소 큰 용량이지만 잘 갖춰진 파이썬, 줄리아, R의 프리셋을 제공한다. 핵심 VS Code의 익스텐션 Remote-Container 컨테이너 접속 기능을 활용해 datascience-notebook 컨테이너 접속한다. 오른쪽 하단에 &amp;gt;&amp;lt; 표시된 부분을 클릭하자. 위의 그림을 볼 수 있다. 여기에서 “Remote-Containers: Attach to Running Container…“를 클릭하면 현재 돌아가고 있는 docker 컨테이너를 볼 수 있다. 미리 docker를 통해 돌려 둔 datascience 컨테이너를 선택하자. 이 컨테이너는 안에 python, jupyterlab, R, Julia를 모두 갖고 있다. 따라서 이 녀석 하나만 도커에 올리면 된다. 각각의 언어에 접근하기 위해서 필요한 VS Code Extension을 설치한다. Julia, Python의 경우 공식 확장이 있어서 그대로 쓰면 된다. R의 경우 비공식 확장이지만 약간의 세팅을 거치면 꽤 근사하게 사용할 수 있다. For Julia 필요한 줄리아 작업 파일을 .jl 확장자로 만든다. VS Code가 알아서 잘 잡아서 줄리아 커널과 연결시켜준다. 간략한 실행 명령 체계를 살펴보자. 자세한 내용은 여기를 참고하자. 코드 셀 구분은 ## CTRL + ENTER 해당 라인 실행 ALT + ENTER 코드 블럭 실행 (드래그앤드롭 선택) SHIFT + ENTER 코드 셀 실행 패키지 설치는 두가지로 가능하다. Julia 콘솔에서 ]를 치면 패키지 관리 모드로 들어간다. &amp;gt;(pkg) add SOMETHING 이렇게 설치할 경우는 별도로 Pkg를 호출하지 않도 설치가 가능하다. 아래와 같이 설치할 수도 있다. using Pkg Pkg.add(&quot;Plots&quot;) Pkg.add(&quot;PyPlot&quot;) Julia는 JIT를 쓰기 때문에 패키지를 설치하고 명령을 구동하는 데 시간이 오래 걸린다. 이 과정을 작업을 시작하기 전에 미래 해두는 것이 좋다. 즉, 필요한 경우 미리 컴파일을 해놓는 것이 좋다. QuantEcon에 소개된 사례를 따르자. For Python 필요한 파일을 .py 확장자로 만든다. 간략한 명령어 Code 블럭의 구별은 #%% Python은 VS Code에서 거의 완벽하고 편리하게 지원이 된다. 별다른 설명이 필요하지 않다. For Rstat 익스텐션 두 개를 깐다. R Support R LSP Client 다음으로 R 내에서 LSP와 연결할 패키지를 설치한다. remotes::install_github(&quot;REditorSupport/languageserver&quot;) 마지막으로 Rstat에서 편리하게 사용할 터미널 앱을 깐다. 파이썬으로 제작되었다. 다행스럽게도 DS 도커는 이미 파이썬을 잘 지원하기 때문에 쉽게 쓸 수 있다. 주의할 것은 Rstat 내부가 아니라 그냥 docker의 bash에 접속한 상태에서 깔아야 한다는 것이다. Radian은 텍스트 상에서 모든 정보를 편리하게 보여주기 때문에 VS Code의 부족함을 잘 메워준다. pip install -U radian 이제 VS Code에서 설정 몇 가지를 바로 잡아 줘야 한다. File &amp;gt; Preferences &amp;gt; Settings로 가자 옆에 탭을 보면 Extensions가 있을 것이다. 여기서 R로 찾아가자. Extension의 설정을 User, Remote, Workspace 등 상황에 맞게 다양하게 정할 수 있다. 여기서는 Workspace에 하도록 하겠다. R: Bracketed Paste 항목 체크 R › Rterm: Linux /opt/conda/bin/radian R: Session Watcher 항목 체크 설정 항목에서 json 파일로 한방에 해결할 수도 있다. Jupyterlab ipynb를 확장자로 해서 노트북을 쓰고 싶다면, 그냥 만들어 쓰면 된다! VS Code에 Jupyter 확장이 있기 때문에 보통의 웹 브라우저에서 쓰는 Jupyter 노트북과 거의 동일한 기능을 제공한다. 웹 노트북에서와 마찬가지로 파이썬, 줄리아, R의 커널을 선택하면 된다 (아래 그림의 오른쪽 상단 박스). jupyterlab과 거의 비슷한 인터페이스를 지니고 있다. 오히려 주피터에 비해 복잡하지 않아서 좋다고 느낄지도 모르겠다. Setting for Remote Connection 앞서 보았듯이 도커를 활용하면 윈도우 혹은 다른 플랫폼의 VS Code를 통해 네트워크로 도커 내의 컨테이너로 접근하게 된다. 따라서 작업을 마치고 나오게 되면 매번 해당 컨테이너에 다시 접속을 해주고 워크 스페이스 등 여러가지 작업을 해줘야 하는 번거로움이 있다. 항상 도커 컨테이너를 거쳐 작업한다면 그냥 한방에 작업하던 환경이 뜨는 편이 좋다면 아래와 같이 하면 된다. 만일 위와 같은 화면에 Remote Exlorer에서 뜨지 않는다면 컨테이너가 부착되지 않는 것이다. 컨테이너를 부착하면 된다. Extension &amp;gt; REMOTE EXPLORER &amp;gt; DETAILS 탭에서 가운데 설정 모양이 “Open Container Configuration File”이다. 옆 에디터 창에 json 형식의 설정이 뜬뜬다. 여기서 관련 extension 등의 설정을 지정할 수 있다. 이렇게 설정을 해두면 해당 컨테이너를 부착할 때 설정한 대로 자동으로 옵션들이 적용된다. 매우 편리하다. { &quot;extensions&quot;: [ &quot;Ikuyadeu.r&quot;, &quot;julialang.language-julia&quot;, &quot;ms-python.python&quot;, &quot;ms-toolsai.jupyter&quot;, &quot;REditorSupport.r-lsp&quot; ], &quot;workspaceFolder&quot;: &quot;/home/jovyan/github-anari&quot;, &quot;forwardPorts&quot;: [ 43463, ], &quot;settings&quot;: { &quot;jupyter.alwaysTrustNotebooks&quot;: true, &quot;r.bracketedPaste&quot;: true, &quot;r.sessionWatcher&quot;: true, &quot;r.rterm.linux&quot;: &quot;/opt/conda/bin/radian&quot; } } File &amp;gt; Perefences &amp;gt; Settings에서도 설정에 접근할 수 있다. VS Code의 설정은 두가지 축을 지닌다. 하나는 설정 전체를 담은 json 파일이다. 왼쪽 &amp;gt; 표시된 항목에서처럼 카테고리 별로 구별되어 쉽게 접근할 수 있다. 이 하나의 설정을 위 그림에서 보듯이 User, Remote(컨네이터), Workspace 별로 별도로 지정해 활용할 수도 있다.</summary></entry><entry><title type="html">Eigenspace, part 2</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/18/Eigenspace-2.html" rel="alternate" type="text/html" title="Eigenspace, part 2" /><published>2021-01-18T00:00:00-06:00</published><updated>2021-01-18T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/18/Eigenspace-2</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/18/Eigenspace-2.html">&lt;h2 id=&quot;singluar-value-decomposition&quot;&gt;Singluar Value Decomposition&lt;/h2&gt;

&lt;p&gt;$A \in \mathbb C^{m \times n}$을 생각해보자. 이런 조건에서 아이겐 분해를 어떻게 활용할 수 있을까? 일단 $A$를 정방행렬로 만들어주어야 할 것이고, 이에는 두 가지 방법이 있다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\underbrace{A^T A}_{n \times n}, \overbrace{A A^T}^{m \times m}
$$&lt;/div&gt;

&lt;p&gt;아울러, $(A^T A)^T = A^T A$, $(AA^T)^T = A A^T$가 성립하기 때문에 두 매트릭스 모두 normal 매트릭스다. 따라서 아래와 같은 개념화가 가능하다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = 
\underbrace{
\begin{bmatrix}
\vert &amp; \dotsb &amp; \vert \\
u_1 &amp; \dotsb &amp; u_m \\
\vert &amp; \dotsb &amp; \vert \\
\end{bmatrix}}_{m \times m}
\overbrace{
\begin{bmatrix}
\sigma_1 &amp; 0 &amp; \dotsb \\
0 &amp; \sigma_2 &amp; \dotsb \\
0 &amp; 0 &amp; \dotsb \\
\end{bmatrix}}^{m \times n}
\underbrace{
\begin{bmatrix}
-- &amp; v_1^T &amp; -- \\
-- &amp; \vdots &amp; -- \\
-- &amp; v_n^T &amp; -- \\
\end{bmatrix}}_{n \times n} =
U \Sigma V^T
$$&lt;/div&gt;

&lt;p&gt;$U$를 통해 분해되는 부분을과 $V$를 통해 분해되는 부분을 다음과 같이 나타내보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
A A^T &amp; = U \Lambda_l U^T \\
A^T A &amp; = V \Lambda_r V^T \\
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;소문자의 $l$와 $r$은 각각 left, right를 뜻한다. $U$와 $V^T$를 명시적으로 적어보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
U = 
\begin{bmatrix}
\vert &amp; \dotsb &amp; \vert \\
u_1 &amp; \dotsb &amp; u_m \\
\vert &amp; \dotsb &amp; \vert \\
\end{bmatrix}, \text{~where}
\{ (\lambda_i, u_i) = \text{eigenvects}(A A^T) \}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
V = 
\begin{bmatrix}
-- &amp; v_1^T &amp; -- \\
-- &amp; \vdots &amp; -- \\
-- &amp; v_n^T &amp; -- \\
\end{bmatrix}, \text{~where}
\{ (\lambda_i, v_i) = \text{eigenvects}(A^T A) \}
$$&lt;/div&gt;

&lt;p&gt;이제  유사 대각행렬 $\Sigma$를 보자. 이 행렬은 $m \times n$ 형태다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\sigma_i = \sqrt{\lambda_i}, \text{ where } \lambda_i = \text{eigenvals}(A A^T) = \text{eigenvals}(A^T A)
$$&lt;/div&gt;

&lt;h3 id=&quot;change-of-basis&quot;&gt;Change-of-basis&lt;/h3&gt;

&lt;p&gt;기저를 바꾸는 관점에서 다시 음미해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec y = A \vec x = U \Sigma V^T \vec x
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$V^T \vec x$: $V^T = \phantom{}_{B_{SVD}}[1]_{B_S}$. 즉, $B_{SVD} \leftarrow B_S$를 수행한다.&lt;/li&gt;
  &lt;li&gt;$\Sigma$: 유사 대각행렬을 곱해 각 기저의 크기를 조절한다.&lt;/li&gt;
  &lt;li&gt;$U$: $U = \phantom{}_{B_{S}}[1]_{B_{SVD}}$: $B_{S} \leftarrow B_{SVD}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, $B_S \leftarrow B_{SVD} \leftarrow B_S$를 수행한다.&lt;/p&gt;

&lt;h3 id=&quot;outer-product&quot;&gt;Outer product&lt;/h3&gt;

&lt;p&gt;외적의 관점에서 이해하는 것도 흥미롭다. 일단 외적에 관해서 간단히 살펴보자. 벡터 $u \in \mathbb R^m$, $v \in \mathbb R^n$이 있다고 할 때,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
u v^T \in \mathbb R^{m \times n}
$$&lt;/div&gt;

&lt;p&gt;외적의 rank는 어떻게 될까? 1이다. 직관적으로는 이해가 안될 수 있다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm rank} (AB) \leq \min({\rm rank}(A), {\rm rank} (B)) =  1
$$&lt;/div&gt;

&lt;p&gt;SVD 식을 다시 조립해보자. 일단 $m \geq n$을 가정하자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
U \Sigma = 
\begin{bmatrix}
\vert &amp; \dotsb &amp; \vert \\
u_1 &amp; \dotsb &amp; u_m \\
\vert &amp; \dotsb &amp; \vert \\
\end{bmatrix}
\begin{bmatrix}
\sigma_1 &amp; 0 &amp; \dotsb \\
0 &amp; \sigma_2 &amp; \dotsb \\
0 &amp; 0 &amp; \dotsb \\
\end{bmatrix}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
A &amp; = U \Sigma V^T \\
&amp; =
\underbrace{
\begin{bmatrix}
\sigma_1 u_1, \dotsb,  \sigma_n u_n, \dotsc,  0 u_m
\end{bmatrix}}_{U \Sigma}
\begin{bmatrix}
-- &amp; v_1^T &amp; -- \\
-- &amp; \vdots &amp; -- \\
-- &amp; v_n^T &amp; -- \\
\end{bmatrix} \\
&amp; =  \sigma_1 u_1 v_1^T + \dotsb + \sigma_n u_n v_n^T + \dotsb + 0 u_m v_m^T
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;$u_i v_i^T$는 각각 1의 rank를 지니고 앞에 곱해진 singular value의 기저로 이해할 수 있다. 해당 기저들의 선형 결합으로 매트릭스 $A$를 다시 분해할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;an-application&quot;&gt;An application&lt;/h3&gt;

&lt;p&gt;이렇게 대각화를 할 때 어떤 이득이 있을까? 앞서 유사 행렬을 활용하면 행렬의 곱이 간단해진다는 점을 보았다. SVD에도 비슷한 이점이 있다. 이 외에 SVD를 써서 할 수 있는 중요한 이득이 있다. 계산량을 줄이는 것이다.&lt;/p&gt;

&lt;p&gt;$M \in \mathbb R^{1000 \times 2000}$의 변환이 있다고 하자. 이 변환의 싱귤러 밸류들 많아야 1000개가 나올 것이다. 만일 이 1000개 중에서 3개를 제외하고 나머지 값이 0에 가깝다고 하자.&lt;/p&gt;

&lt;p&gt;$M$이라는 변환의 근사값을 구하고 싶다면, 0에 가까운 값을 모두 0으로 둔다. 이렇게 바꾸면,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
M \approx \hat M = \hat U \hat \Sigma \hat V^T
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$\hat U \in \mathbb R^{1000 \times 3}$&lt;/li&gt;
  &lt;li&gt;$\hat \Sigma \in \mathbb R^{3 \times 3}$&lt;/li&gt;
  &lt;li&gt;$\hat V^T \in \mathbb R^{3 \times 2000}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이렇게 근사값을 구하면 계산량이 많이 줄어들 것이다. $M$와 $\hat M$ 사이의 힐베르트-슈미트 거리를 구하면,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Vert M - \hat M \Vert_{\rm HS} = \sqrt{\sum_{i=4}^{1000} \sigma_i^2}
$$&lt;/div&gt;

&lt;p&gt;이 값이 크지 않다면, $M \approx \hat M$으로 간주할 수 있다. 실제로 이미지 압축 등에서 많이 사용되는 방법이다.&lt;/p&gt;

&lt;h2 id=&quot;lu&quot;&gt;LU&lt;/h2&gt;

&lt;p&gt;$n \times n$ 정방 행렬에 대해서 $A = LU$를 수행할 수 있다. $L$과 $U$는 하방삼각 행렬과 상방삼각 행렬이다. 이렇게 매트릭스를 쪼개는 이유를 각각에 관해서 역행렬을 구하기 힘들기 때문이다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A \vec x = LU \vec x = b \Leftrightarrow U \vec x = L^{-1}b \Leftrightarrow \vec x = U^{-1}L^{-1}b
$$&lt;/div&gt;

&lt;p&gt;우선 LU가 가능하라면 $A$가 RREF으로 열의 교환 없이 환원될 수 있어야 한다.&lt;/p&gt;

&lt;h3 id=&quot;cholesky&quot;&gt;Cholesky&lt;/h3&gt;

&lt;p&gt;$A$가 대칭이고 PSD 매트릭스라면 $LU$ 분해는 더욱 단순해진다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = L L^T \text{ or } A = U^T U
$$&lt;/div&gt;

&lt;h2 id=&quot;qr&quot;&gt;QR&lt;/h2&gt;

&lt;p&gt;$A \in \mathbb R^{n \times n}$일 때 이 매트릭스를 쪼개는 강력한 방법은 G-S 알고리듬을 활용하는 것이다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = O U
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$O$: orthogonal matrix&lt;/li&gt;
  &lt;li&gt;$U$: Upper triangluar matrix&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;보통 $U$를 right-triangular matrix로도 쓰기 때문에, 이를 $QR$로 표기하기도 한다. $R$의 경우 $O$의 컬럼 벡터를 하나씩 추가해가면서 계산하게 된다. 이는 G-S 알고리듬에서 하나씩 벡터를 빼가면서 계산하는 것을 구현할 수 있게 해준다.&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;실제로는 어떻게 하는지 살펴보자. 먼저 $A$의 각 컬럼들을 두고, 두번째 열은 첫번째에 직교하게, 세번째는 첫번째와 두번째에 직교하게 행렬을 변형한다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = 
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} 
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;$O$ 혹은 $Q$를 구해보자. G-S 알고리듬을 활용하자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = [a_1, a_2, a_3]
$$&lt;/div&gt;

&lt;p&gt;$a_1$과 직교하는 $e_2$를 구하면 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
e_2 = a_2 - \dfrac{a_2 \cdot a_1}{\Vert a_1 \Vert^2}
$$&lt;/div&gt;

&lt;p&gt;다음으로 $e_3$는 $a_1$과 직교하고 동시에 $e_2$와 직교해야 한다. 따라서,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
e_3 = a_3 - \dfrac{a_3 \cdot a_1}{\Vert a_1 \Vert^2} - \dfrac{a_3 \cdot e_2}{\Vert e_2 \Vert^2}
$$&lt;/div&gt;

&lt;p&gt;이를 다시 길이 1로 표준화하면 $Q$를 구할 수 있다. 그리고&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Q^T A = Q^T Q R = 1 R
$$&lt;/div&gt;

&lt;p&gt;따라서 $R$은 $Q^T A$로 구할 수 있다. 
&lt;!--stackedit_data:
eyJoaXN0b3J5IjpbNzAzMDc4NjM4LDkyMzM2MDQ1NCwtOTE4OD
UxNjc4LC05NDMxMTE1MDUsMTM1MjA2MDIyLDE0NTI1NTY4MTAs
MTA2MTYwMTEzNywtMjEyNzA0NDgyOSwtMTYyMTY0ODM2OSw1OT
I4MDIwMDIsLTIwMjMzODg3NTAsMF19
--&gt;&lt;/p&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Singluar Value Decomposition</summary></entry><entry><title type="html">Eigenspace, part 1</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/10/Eigenspace-1.html" rel="alternate" type="text/html" title="Eigenspace, part 1" /><published>2021-01-10T00:00:00-06:00</published><updated>2021-01-10T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/10/Eigenspace-1</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/10/Eigenspace-1.html">&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;아이겐밸류, 아이겐벡터는 중요하다. 어디서나 튀어나온다. 그래서 친숙하지만 나는 이걸 제대로 알고 있는 것일까? 이번 포스팅에서는 아이겐 공간의 관점에서 이 문제를 살펴볼 예정이다.&lt;/p&gt;

&lt;p&gt;사실 아이겐 공간은 $n \times n$ 매트릭스의 숨은 구조(뼈대)와 같다. 아이겐밸류를 구하는 식을 떠올려보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
M_T \vec x = \lambda \vec x
$$&lt;/div&gt;

&lt;p&gt;$M_T$라는 $n \times n$ 변환이 특정한 벡터 아래서는 스칼라 곱의 문제로 환원된다. 이런 의미에서 아이겐벡터는 $M_T$라는 매트릭스가 지닌 일종의 축이다. 이 축 위에서 변환이 다시 축으로 환원되기 때문이다. $M_T$의 원래 기저와 상관 없이, 이 아이겐벡터로 다시 기저를 구성한다고 생각해보자. 이때 아이겐밸류는 해당 축의 크기(길이)로 이해할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;eigenspace&quot;&gt;Eigenspace&lt;/h2&gt;

&lt;p&gt;아이겐 공간이란 무엇일까? 특정한 아이겐밸류 $\lambda_i$에 의해 파생되는 아이겐 공간은 다음과 같이 정의될 수 있다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
E_{\lambda_i}  \overset{\rm def}{=} \mathcal N (A - \lambda_i 1) = \{ \vec v | (A - \lambda_i ) \vec v = \vec 0 \}
$$&lt;/div&gt;

&lt;p&gt;즉, $A - \lambda_i 1$의 널 스페이스다. 사실 여기서 아이겐밸류를 구하는 공식도 파생된다. 아이겐벡터가 $A - \lambda_i 1$의 널 스페이스에 있다는 것은 $A - \lambda_i 1$라는 변환이 서로 선형 종속이라는 뜻이다. 즉 $\det (A - \lambda_i 1) = 0$의 의미와 같다. 아이겐밸류를 구하는 특성방정식이 여기서 도출된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
p(\lambda) = \vert A - \lambda 1 \vert = 0
$$&lt;/div&gt;

&lt;h3 id=&quot;all-distinct-eigenvalues&quot;&gt;All distinct eigenvalues&lt;/h3&gt;

&lt;p&gt;실용적으로 접근해보자. 모든 아이겐밸류가 다르다면, 아이겐벡터는 선형독립이다. $n$ 개의 서로 다른 아이겐벡터가 있다면 원래 매트릭스($M_T$)의 컬럼 스페이스를 생성하는 기저가 될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;algebraic-vs-geometric&quot;&gt;Algebraic vs geometric&lt;/h3&gt;

&lt;p&gt;대수적 중복도(algebraic multiplicity: AM)란 특성 방정식에서 특정한 아이겐밸류 $\lambda$가 몇 번 나타나는지를 표시한다. 한편 기하적 중복도(geometric multiplicity: GM)란 $\lambda$의 아이겐벡터가 생성하는 널 공간의 차원을 의미한다. 예를 들어보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = 
\begin{bmatrix}
1 &amp; 2 \\
0 &amp; 1
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;$A$ 특성방정식을 구하면 $p(\lambda) = (1-\lambda)^2$이다. 따라서 아이겐밸류 1의 AM는 2이다. GM은 어떨까?&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A - \lambda I = 
\begin{bmatrix}
0 &amp; 2 \\
0 &amp; 0 \\
\end{bmatrix} x = 0
$$&lt;/div&gt;

&lt;p&gt;이를 만족하는 널 스페이스 $x$는 아래 벡터 하나다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
x = \alpha
\begin{bmatrix}
1 \\
0
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;따라서 $\lambda=1$의 GM은 1이 된다.&lt;/p&gt;

&lt;p&gt;일단 직관적으로 알 수 있는 점은 아이겐밸류 $\lambda$의 기하적 중복도가 대수적 중복도 보다 클 수는 없다는 점이다. 즉, ${\rm GM}(\lambda) \leq {\rm AM}(\lambda)$&lt;/p&gt;

&lt;h3 id=&quot;defective-eigenvalues&quot;&gt;Defective eigenvalues&lt;/h3&gt;

&lt;p&gt;${\rm GM}(\lambda) &amp;lt; {\rm AM}(\lambda)$가 되는 $\lambda$를 defective eigenvalue라고 부른다. 특성 방정식에서 해당 아이겐벡터가 생성하는 널 스페이스의 차원이 AM보다 작다면, 아이겐벡터를 모아서 특성 방정식 생성한 널 스페이스를 생성할 수 없다. 다시 말하면, 이는 아이겐 분해를 통해서 원래 매트릭스의 컬럼 스페이스를 온전하게 생성할 수 없다는 뜻이다.&lt;/p&gt;

&lt;p&gt;모든 아이겐밸류의 값이 다를 경우, 즉 아이겐밸류의 중복이 없을 경우 각각 아이겐밸류의 AM은 1이 된다. 이 경우 아이겐벡터들이 모두 선형 독립이기 때문에 각 아이겐밸류의 기하적 중복도 역시 1이 된다.  따라서 반복되는 아이겐밸류가 없는 경우에는 defective eigenvalue는 없고, 행렬의 아이겐 분해가 가능해진다.&lt;/p&gt;

&lt;h2 id=&quot;diagonalization-as-change-of-basis&quot;&gt;Diagonalization as Change-of-Basis&lt;/h2&gt;

&lt;p&gt;아이겐벡터와 아이겐밸류를 동원해서 매트릭스를 분해하는 것을 대각화라고도 부른다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = Q \Lambda Q^{-1}
$$&lt;/div&gt;

&lt;p&gt;이렇게 분해될 때 가운데 매트릭스 $\Lambda$가 아이겐밸류의 대각 행렬로 구성되기 때문이다. $Q$는 다음과 같이 정의된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Q = 
\begin{bmatrix}
\vert &amp; \vert &amp; \vert \\
\vec e_{\lambda_1} &amp; \dotsc &amp; \vec e_{\lambda_n} \\
\vert &amp; \vert &amp; \vert \\
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;앞서 아이겐벡터가 일종의 축의 역할을 한다고 했다. 즉, 이 아이겐벡터는 매트릭스의 인풋으로 아이겐 스페이스 벡터를 받고 이를 현재의 표준 스페이스로 바꿔준다.$B_S \leftarrow B_\lambda$  역할을 한다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Q = \phantom{}_{B_S}[1]_{B_\lambda}
$$&lt;/div&gt;

&lt;p&gt;$Q$를 기저 변환의 관점에서 보면 아이겐 공간의 좌표를 표준 좌표로 바뀌주는 역할을 한다. $Q^{-1}$은 반대로 $B_{\lambda} \leftarrow B_{S}$의 역할을 한다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Q^{-1} = \phantom{}_{B_\lambda}[1]_{B_S}
$$&lt;/div&gt;

&lt;p&gt;이 관점에서 보면 행렬의 대각화가 새롭게 보인다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
[\vec w]_{B_S} = \phantom{}_{B_S}[A]_{B_S} [\vec v]_{B_S} = Q \Lambda Q^{-1}[\vec v]_{B_S}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Q \Lambda Q^{-1}[\vec v]_{B_S} = \underbrace{\phantom{}_{B_S}[1]_{B_\lambda}}_{Q}\phantom{}_{B_\lambda}[\Lambda]_{B_\lambda}\overbrace{\phantom{}_{B_\lambda}[1]_{B_S}}^{Q^{-1}}[\vec v]_{B_S}
$$&lt;/div&gt;

&lt;p&gt;행렬의 대각화란 일정한 변환 혹은 매트릭스를 아이겐 공간을 통해 다시 해석하는 과정이다. 즉, $B_S \to B_\lambda \to B_S$의 과정을 거친다.&lt;/p&gt;

&lt;p&gt;대각 행렬 $\Lambda$는 아이겐벡터들로 구성된 아이겐 공간의 기저(아이겐벡터)의 크기를 나타낸다.&lt;/p&gt;

&lt;h3 id=&quot;eat-this&quot;&gt;Eat this!&lt;/h3&gt;

&lt;p&gt;대각화를 통해서 아이겐밸류의 중요한 특성 두 가지를 다시 음미해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm det}(A) = \vert A \vert = \prod_{i} \lambda_i
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm Tr}(A) = \sum_{i} a_{ii} = \sum_{i} \lambda_i
$$&lt;/div&gt;

&lt;p&gt;논리는 아래와 같이 간단하다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vert A \vert = \vert Q \Lambda Q^{-1} \vert =  \vert Q \vert \vert \Lambda \vert \vert Q^{-1} \vert = \vert Q \vert \vert Q^{-1} \vert \vert \Lambda \vert = \dfrac{\vert Q \vert}{\vert Q^{} \vert} \vert \Lambda \vert = \vert \Lambda \vert
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm Tr}(Q \Lambda Q^{-1}) = {\rm Tr}(\Lambda Q Q^{-1}) = {\rm Tr}(\Lambda 1) = {\rm Tr}(\Lambda) = \sum_{i} \lambda_i
$$&lt;/div&gt;

&lt;p&gt;두 가지 속성은 ${\rm det}(A) = \vert A \vert = \prod_{i} \lambda_i$는 대각화가 가능한 경우 뿐 아니라 일반적으로 성립한다. 첫번째 속성만 살펴보자. 특성방정식을 생각해보면, $\vert A - \lambda I \vert = 0$이다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
p(\lambda)  = &amp; {\rm det} (A - \lambda I) \\
&amp; (-1)^n (\lambda - \lambda_1) \dotsb  (\lambda - \lambda_n) \\
&amp; (\lambda_1 - \lambda)\dotsb(\lambda_n - \lambda)
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;따라서, $\det (A) = \lambda_1 \dotsb \lambda_n$.&lt;/p&gt;

&lt;h3 id=&quot;normal-matrix&quot;&gt;Normal matrix&lt;/h3&gt;

&lt;p&gt;매트릭스 $A$가 노멀이라면, 이는 $A^T A = A A^T$를 만족하는 경우를 뜻한다. 모든 노멀 매트릭스는 대각화가 가능하고 아울러 $Q$를 직교 행렬(orthgonal matrix or orthonormal matrix) $O$ 로 택할 수 있다. 이는 $Q^{-1}$의 계산이 간단해진다는 뜻이다.  즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
OO^T = I = O^T O
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
O^TO = 
\begin{bmatrix}
-- &amp; \hat e_1 &amp; -- \\
   &amp; \vdots &amp; \\
-- &amp; \hat e_n &amp; -- \\
\end{bmatrix}
\begin{bmatrix}
\vert &amp;  &amp; \vert \\
\hat e_1 &amp; \dotsc &amp; \hat e_n \\
\vert &amp; &amp; \vert \\
\end{bmatrix} = I
$$&lt;/div&gt;

&lt;h2 id=&quot;gram-schmidt-orthogonalization&quot;&gt;Gram-Schmidt Orthogonalization&lt;/h2&gt;

&lt;p&gt;orthnormal, orthogonal, generic 세 가지 기저의 품질을 따져보자. 당연히 orthonormal 기저가 가장 작업하기 쉽다. 위에서 보듯이, $Q^T = Q^{-1}$라는 좋은 특징도 지니고 있다. 만일 통상적인 벡터를 orthonormal 기저로 변형할 수 있다면, 작업이 훨씬 쉬울 것이다.&lt;/p&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$V$: $n$ 차원 벡터&lt;/li&gt;
  &lt;li&gt;${ v_1, \dotsc, v_n }$: $V$의 generic 기저&lt;/li&gt;
  &lt;li&gt;${ e_1, \dotsc, e_n }$: $V$의 orthogonal 기저. $e_i \cdot e_j = 0$ for $i \neq j$&lt;/li&gt;
  &lt;li&gt;${\hat e_1, \dotsc, \hat e_n }$ V의 orthonormal 기저.&lt;/li&gt;
  &lt;li&gt;Inner production operation: $\langle \cdot, \cdot \rangle: V \times V \to \mathbb R$&lt;/li&gt;
  &lt;li&gt;Length: $\Vert v \Vert = \langle v, v \rangle$&lt;/li&gt;
  &lt;li&gt;Projection operation: Projection of $u$ onto $e$:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_e(u) = \dfrac{\langle u, e \rangle}{\Vert e \Vert^2}e
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The projection complement of projection $\Pi_e(u)$ is $w$&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_e(u) + w = u ~~~\Rightarrow~~~w = u - \Pi_e(u)
$$&lt;/div&gt;

&lt;h3 id=&quot;orthonormal-basis-is-nice&quot;&gt;Orthonormal basis is nice&lt;/h3&gt;

&lt;p&gt;어떤 벡터 $v$든 orthonormal 기저를 통해 간편하게 나타낼 수 있다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
v = \langle v, \hat e_1 \rangle  \hat e_1 + \dotsb + \langle v, \hat e_n \rangle \hat e_n
$$&lt;/div&gt;

&lt;h3 id=&quot;orthogonalization&quot;&gt;Orthogonalization&lt;/h3&gt;

&lt;p&gt;일단 기억해야 할 것은 generic 기저 ${v_i}$가 생성하는 벡터 공간과 ${ \hat e_i }$가 생성하는 벡터 공간이 동일하다는 것이다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\text{span}(v_1, \dotsc, v_n) = V = \text{span}(\hat e_1, \dotsc, \hat e_n)
$$&lt;/div&gt;

&lt;p&gt;그람-슈미트 알고리듬은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;일단 orthogonal 기저를 만든다.&lt;/li&gt;
  &lt;li&gt;해당 벡터를 표준화한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Orthogonal 기저는 어떻게 만들까? 먼저 과정을 살펴보자.   .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$e_1 = v_1$&lt;/li&gt;
  &lt;li&gt;$e_2 = v_2 - \Pi_{e_1}(v_2)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Gram%E2%80%93Schmidt_process.svg/450px-Gram%E2%80%93Schmidt_process.svg.png&quot; alt=&quot;gram-schmidt 1&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림에서 보듯이 $v_2$는 $v_1$ 프로젝션된 벡터와 이와 직교하는 $e_2$와의 합으로 계산할 수 있다. 따라서 $e_2 = v_2 - \Pi_{e_1}(v_2)$가 성립한다. 벡터의 빼기 관점에서 생각해보면 어떨까? $v_2$와 $\Pi_{e_1}(v_2)$의 차이가 $e_2$다. 벡터는 방향과 크기로 정의된다는 점을 다시 기억하자. 같은 방식으로 아래 그림에서 보듯이 더 많은 축과 직교하는 벡터들을 구성할 수 있다.&lt;/p&gt;

&lt;p&gt;이후 $\hat e_i = \dfrac{e_i}{\Vert e_i \Vert}$로 $e_i$를 표준화하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/2334554B591BF2B720&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
v = {\rm proj}_{u_1} (v) + {\rm proj}_{u_2}(v) + w
$$&lt;/div&gt;

&lt;p&gt;즉, 원래 벡터($v$)에서 이미 확립된 직교 벡터에서 $v$로 쏜 프로젝션 벡터를 빼주면 원하는 새로운 직교 벡터를 얻을 수 있다. 따라서&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$e_3 = v_3 - \Pi_{e_1}(v_3) - \Pi_{e_2}(v_3)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;   $\vdots$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$e_n= v_n - \sum_{i=1}^{n-1} \Pi_{e_i}(v_n)$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wrap-it-up&quot;&gt;Wrap-it-up&lt;/h2&gt;

&lt;p&gt;행렬 대각화에 관해서 다시 한번 정리해보자. 때로는 혼동될 사안이라서 정리한다. 증명은 일단 생략한다.&lt;/p&gt;

&lt;h3 id=&quot;diagonalization-theorem&quot;&gt;Diagonalization Theorem&lt;/h3&gt;

&lt;p&gt;매트릭스 $A \in \mathbb C^{n \times n}$가 대각화가 가능하다는 것은 $n$의 선형 독립인 아이겐벡터를 지니고 있다는 뜻이다. 즉, $A =Q \Lambda Q^{-1}$ 형태로 분해될 수 있음을 뜻한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;착각하지 말아야 할 것! 대각가능 행렬과 역행렬이 존재하는 행렬은 아무 관계가 없다. 둘은 서로 다른 이야기다.&lt;/li&gt;
  &lt;li&gt;아이겐분해(eigendecomposition)이란 닮음 행렬을 통해서 기저를 바꾸는 과정인데, 이때 기저를 바꾸는 매트릭스로 동원되는 것이 아이겐벡터 매트릭스다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;eat-this-1&quot;&gt;Eat this!&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;$A$ positive semidefinite $\Rightarrow$ 아이겐밸류는 비음이다.&lt;/li&gt;
  &lt;li&gt;$A$ symmetric $\Rightarrow$ 아이겐밸류는 실수&lt;/li&gt;
  &lt;li&gt;$A$ normal ($A^T A = A A^T$) $\Rightarrow$ $Q$를 직교 행렬 $O$로 고를 수 있다($O^T O = I$).&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Why?</summary></entry><entry><title type="html">Basis</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/08/basis.html" rel="alternate" type="text/html" title="Basis" /><published>2021-01-08T00:00:00-06:00</published><updated>2021-01-08T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/08/basis</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/08/basis.html">&lt;h2 id=&quot;concepts--definition&quot;&gt;Concepts &amp;amp; Definition&lt;/h2&gt;

&lt;p&gt;벡터 공간 $V$의 기저&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
B = \{ \vec{e_1}, \dotsc, \vec{e_n} \}  
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;는 다음의 두 특성을 만족한다.&lt;/p&gt;

&lt;h3 id=&quot;spanning-property&quot;&gt;Spanning property&lt;/h3&gt;

&lt;p&gt;모든 $v \in V$는 다음과 같이 기저의 선형 결합으로 표현된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
v = v_1 \vec{e_1} + \dotsc + v_n \vec{e_n}
$$&lt;/div&gt;

&lt;h3 id=&quot;linear-independence-property&quot;&gt;Linear Independence property&lt;/h3&gt;

&lt;p&gt;즉, 기저를 구성하는 벡터 $\vec{e_i}$에 불필요한 것이 없어야 한다. 즉, $\vec{e}$를 구성하는 어떤 $e_i$도 다른 $e_j$($j \neq i$)의 선형 결합으로 표현될 수 없다.&lt;/p&gt;

&lt;h3 id=&quot;orthonomal-basis&quot;&gt;Orthonomal basis&lt;/h3&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
B_{\hat{e}} = \{ \hat{e_1}, \dotsc, \hat{e_n} \} \text{~with}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{cases}
\hat{e_i} \cdot \hat{e_j} = 1 &amp; \text{if $i = j$} \\
\hat{e_i} \cdot \hat{e_j} = 0 &amp; \text{if $i \neq j$} 
\end{cases}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
(a_1, \dotsc, a_n)_{B_{\hat{e}}} = \underbrace{(\vec{a} \cdot \hat{e_i})}_{a_1} \hat{e_i} + \dotsb + (\vec{a} \cdot \hat{e_n}) \hat{e_n}
$$&lt;/div&gt;

&lt;h2 id=&quot;orthogonal-basis&quot;&gt;Orthogonal basis&lt;/h2&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
B_{e} = \{ e_1, \dotsc, e_n \} \text{~with}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{cases}
e_i \cdot e_j \neq 0 &amp; \text{if $i = j$} \\
e_i \cdot e_j = 0 &amp; \text{if $i \neq j$} 
\end{cases}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
(b_1, \dotsc, b_n)_{B_e} = \underbrace{(\vec b \cdot \dfrac{e_i}{\Vert e_i \Vert})}_{b_1} e_1 + \dotsb + (\vec b \cdot \dfrac{e_i}{\Vert e_i \Vert}) e_n
$$&lt;/div&gt;

&lt;p&gt;$b_i$의 값을 제대로 반영하기 위해서는 정규화된 orthorgonal basis가 필요하고, $\frac{e_i}{\Vert e_i \Vert}$가 내적 계산에 들어간다.&lt;/p&gt;

&lt;h2 id=&quot;take-this&quot;&gt;Take This!&lt;/h2&gt;

&lt;h3 id=&quot;generic-basis&quot;&gt;Generic basis&lt;/h3&gt;

&lt;p&gt;서로 직교하지 않는 기저,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\{ \vec f_1, \dotsc, \vec f_n \}
$$&lt;/div&gt;

&lt;p&gt;가 있다고 하자. $\vec c$를 이 기저로 어떻게 표현할 수 있을까?&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
c_1 f_1 + \dotsb + c_n f_n = \vec c
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;$f_i$가 직교행렬이 아니기 때문에, $c_i$ 역시 하나씩 결정될 수 없고 동시에 결정되어야 한다. 즉, 이는 연립방정식을 푸는 문제와 같다. 즉 $n$ 개의 미지수와 $n$ 개의 방정식을 푸는 문제다.&lt;/p&gt;

&lt;h3 id=&quot;example&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;$T: \mathbb R^2 \to \mathbb R^2$의 변환을 생각해보자. 어떤 이유에서인가 $T$를 기본 기저가 아닌 다른 기저로 표현해야 한다고 하자. 두 개의 기저를 아래와 같이 두자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\\{ \vec v_1 = (v_{1x}, v_{1y})^T, \vec v_2 = (v_{2x}, v_{2y})^T \\}
$$&lt;/div&gt;

&lt;p&gt;이 기저는 $T$에 의해서 다음과 같이 변형된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
T(\vec v_1) = 
\begin{bmatrix}
t_{1x} \\
t_{1y}
\end{bmatrix},~ 
T(\vec v_2) = 
\begin{bmatrix}
t_{2x} \\
t_{2y}
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;이걸 매트릭스로 표현하면 어떻게 될까? $2 \times 2$로 이 변형이 표현될 수 있기 때문에,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
M_T = 
\begin{bmatrix}
m_{11} &amp; m_{12} \\
m_{21} &amp; m_{22} 
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;앞서 변형을 그대로 적어보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
m_{11} v_{1x} + m_{12} v_{1y} &amp; = t_{1x} \\
m_{21} v_{1x} + m_{22} v_{1y} &amp; = t_{1y} \\
m_{11} v_{2x} + m_{12} v_{2y} &amp; = t_{2x} \\
m_{21} v_{2x} + m_{22} v_{2y} &amp; = t_{2y} \\
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;여기서 미지수는 $m_{\cdot}$이다. 즉 4개의 미지수를 지니는 연립방정식이 된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A \vec m = \vec t ~\Leftrightarrow~
\begin{bmatrix}
v_{1x} &amp; v_{1y} &amp; 0 &amp; 0 \\
v_{1x} &amp; v_{1y} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; v_{2x} &amp; v_{2y} \\
0 &amp; 0 &amp; v_{2x} &amp; v_{2y} \\
\end{bmatrix} 
\begin{bmatrix}
m_{11} \\
m_{12} \\
m_{21} \\
m_{22} \\
\end{bmatrix} = 
\begin{bmatrix}
t_{1x} \\
t_{1y} \\
t_{2x} \\
t_{2y}
\end{bmatrix}
$$&lt;/div&gt;

&lt;h2 id=&quot;change-of-basis&quot;&gt;Change of Basis&lt;/h2&gt;

&lt;p&gt;한 벡터의 기저를 다른 기저로 바꾸는 것을 생각해보자. 일반적으로는 $T: V \to W$ 역시 $B_V$에서 $B_W$로 기저를 바꾸는 것이다. 차원이 바뀐다면 기저 역시 바뀐다.&lt;/p&gt;

&lt;p&gt;기저 변환(change-of-basis)은 매트릭스를 이해하는 매우 중요한 방식이다. 이를 통해 역시 매트릭스 표현에 도달할 수 있다. 선형 번환 $T: V \to W$가 있다고 하자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
B_V &amp; = \{ \hat e_1, \dotsc, \hat e_n \} \\
B_W &amp; =  \{ \hat b_1, \dotsc, \hat b_m \} 
\end{aligned}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
[M_T]_{B_V} \vec v_{B_V} &amp; =
\begin{bmatrix}
\vert &amp; \vert &amp; \vert \\
T(\hat e_1) &amp; \dotsc &amp; T(\hat e_n) \\
\vert &amp; \vert &amp; \vert \\
\end{bmatrix}_{B_V} 
\begin{bmatrix}
v_1 \\
\vdots \\
v_n \\
\end{bmatrix}_{B_V} \\
&amp; = T(\hat e_1) v_1 + \dotsb + T(\hat e_n) v_n \\
&amp; = T(v_1 \hat e_1 + \dotsb + v_n \hat e_n) \\
&amp;  = T(\vec v) \\
&amp; = \vec w_{B_W}
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;이제 $T(\hat e_1)$ 하나만 구체적으로 풀어보자. $T$를 통해서 기저는 $B_V$에서 $B_W$로 바뀐다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
T(\hat e_1) = 
\begin{bmatrix}
c_{11} \\
\vdots \\
c_{m1}
\end{bmatrix}_{B_W} = c_{11} \hat b_1 + \dotsb + c_{m1} \hat b_m
$$&lt;/div&gt;

&lt;p&gt;이를 모든 행에 대해서 적용하면, 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\phantom{}_{B_W}M_{B_V} = 
\vphantom{
\begin{bmatrix}
\\
\\
\\
\end{bmatrix}
}_{B_W}
\begin{bmatrix}
c_{11} &amp; \cdots &amp; c_{1n} \\
&amp;\cdots&amp;\\
c_{m1}  &amp; \cdots &amp; c_{mn} \\
\end{bmatrix}_{B_V}
$$&lt;/div&gt;

&lt;p&gt;정리하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
[T(\vec v)]_{B_W} = \phantom{}_{B_W} [M_T]_{B_V} [\vec v]_{B_V}
$$&lt;/div&gt;

&lt;h3 id=&quot;change-of-basis-1&quot;&gt;Change-of-basis&lt;/h3&gt;

&lt;p&gt;이제 하나의 같은 벡터의 기저를 $B_v \to B_{v^{\prime}}$으로 바꾸는 것을 살펴보자. 즉 $T: V \to V$의 경우에 해당한다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec v = (v_1, v_2, v_3)_B = v_1 \hat e_1 + v_2 \hat e_2 + v_3 \hat e_3
$$&lt;/div&gt;

&lt;p&gt;이제 기저를 $B \to B^\prime$으로 바꾸는 어떤 변환이 있다고 하자. 이 변환을 $\phantom{}_{B^{\prime}}[1]_B$라고 표기하자. 이 표기의 뜻은 매트릭스의 인풋(오른쪽)이 원래의 기저 $B$이고 변환을 통해 산출되는 기저를 $B^{\prime}$으로 나타낸 것이다. $1$의 의미는 벡터의 기저만 바뀌었을 뿐 동일한 벡터의 변환이라는 의미를 지닌다 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
(v^{\prime}_1, v^{\prime}_2, v^{\prime}_3) = v^{\prime}_1 \hat e^{\prime}_1 +  + v_2 \hat e^{\prime}_2 + v_3 \hat e^{\prime}_3 = \vec v = v_1 \hat e_1 + v_2 \hat e_2 + v_3 \hat e_3
$$&lt;/div&gt;

&lt;p&gt;$_{B^{\prime}}1_B$을 찾기 위해서 $\hat e_1$을 $B^{\prime}$ 기저로 표현해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\hat e_1 = (\hat e^{\prime}_1 \cdot \hat e_1) e^{\prime}_1 + (\hat e^{\prime}_2 \cdot \hat e_1) e^{\prime}_2 + (\hat e^{\prime}_3 \cdot \hat e_1) e^{\prime}_3 = ( \hat e^{\prime}_1 \cdot \hat e_1,  \hat e^{\prime}_2 \cdot \hat e_1 ,  \hat e^{\prime}_3 \cdot \hat e_1  )_{B^{\prime}}
$$&lt;/div&gt;

&lt;p&gt;따라서 기저 변환을 위한 매트릭스는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{bmatrix}
e^{\prime}_1 &amp; e^{\prime}_2 &amp; e^{\prime}_3 
\end{bmatrix} = 
\begin{bmatrix}
e^{\prime}_1 \cdot \hat e_1  &amp; e^{\prime}_1 \cdot \hat e_2 &amp; e^{\prime}_1 \cdot \hat e_3 \\
e^{\prime}_2 \cdot \hat e_1  &amp; e^{\prime}_2 \cdot \hat e_2 &amp; e^{\prime}_2 \cdot \hat e_3 \\
e^{\prime}_3 \cdot \hat e_1  &amp; e^{\prime}_3 \cdot \hat e_2 &amp; e^{\prime}_3 \cdot \hat e_3 \\ 
\end{bmatrix} = 
\phantom{}_{B^{\prime}}[1]_B
$$&lt;/div&gt;

&lt;h3 id=&quot;back-to-generic-bases&quot;&gt;Back to generic bases&lt;/h3&gt;

&lt;p&gt;다시 위에서 살펴보았던 날 기저(generic basis) $f$ 의 문제로 돌아와보자. $\phantom{}_S1_f$는 어떻게 구할 수 있을까? 위의 식을 참고하면 된다. $B_S = {(1,0,0), (0,1,0), (0,0,1)}$이라고 하자. 표기의 편의상 $B_S$ 각각을 $i$, $j$, $\hat i$, $\hat j$, $\hat k$ 라고 하자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\phantom{}_S[1]_f = 
\begin{bmatrix}
\vec f_1 \cdot \hat i  &amp; \vec f_2 \cdot \hat i  &amp; \vec f_3 \cdot \hat i \\
\vec f_1 \cdot \hat j  &amp; \vec f_2 \cdot \hat j  &amp; \vec f_3 \cdot \hat j \\
\vec f_1 \cdot \hat k &amp; \vec f_2 \cdot \hat k &amp; \vec f_3 \cdot \hat k \\ 
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;$\phantom{}_S[1]_f$ 매트릭스는 $B_f$ 기저의 벡터를 $B_S$ 기저의 벡터로 바꿔주는 매트릭스다. 이를 적용하면 $B_S$ 기저의 매트리스 벡터가 나온다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\underbrace{\phantom{}_S[1]_f}_{B_S \leftarrow B_f} \vec v_{B_f} = \vec v_{B_S}
$$&lt;/div&gt;

&lt;h3 id=&quot;transformation-with-change-of-basis&quot;&gt;Transformation with change-of-basis&lt;/h3&gt;

&lt;p&gt;이제 $\phantom{}_B[M_T]_B$ 가 주어져 있다고 하자. 이를 $\phantom{}_{B^{\prime}}[M^T]_{B^{\prime}}\phantom{}$로 어떻게 교체할 수 있을까? 개념적으로는 이럴 것이다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\phantom{}_{B^{\prime}}{[M^T]}_{B^{\prime}} = \underbrace{\phantom{}_{B^{\prime}}{[1]}_{B}}_{B^\prime \leftarrow B}\phantom{}_{B}{[M^T]}_{B}\overbrace{\phantom{}_{B}{[1]}_{B^{\prime}}}^{B \leftarrow B^\prime}
$$&lt;/div&gt;

&lt;p&gt;$\phantom{}_{B^{\prime}}{[1]}_{B}$과 $\phantom{}_{B}{[1]}_{B^{\prime}}$이 서로 역행렬의 관계이 있음을 기억해두자.&lt;/p&gt;

&lt;h2 id=&quot;similar-matrix&quot;&gt;Similar Matrix&lt;/h2&gt;

&lt;p&gt;$B \in \mathbb R^{n \times n}$과 역행렬이 존재하는 매트릭스 $C \in \mathbb R^{n \times n}$가 있다고 하자. A은 다음과 같이 정의된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A = C B C^{-1}
$$&lt;/div&gt;

&lt;p&gt;$A$과 $B$은 서로 닮은 꼴의 매트릭스다. 위 식을 만족하는 매트릭스 $A$과 $B$을 similar matrix라고 정의한다. 우선 두 매트릭스가 서로 닮은 꼴일 때에는 $n \geq 1$에 대해서 $A^n = C B^n C^{-1}$이 성립한다. 이는 아이겐 분해에서 보듯이 $B$가 어떤 매트릭스냐에 따라서 계산 상 편리함을 줄 수 있다. 만일 $B$가 대각 행렬이라면 행렬의 $n$은 대각 원소의 $n$ 승만 수행하면 된다.&lt;/p&gt;

&lt;p&gt;$C$가 역행렬을 지니기 때문에 $C$의 컬럼은 $\mathbb R^n$의 기저가 된다. 이렇게 보면 $C$는 change-of-basis와 같은 맥락에서 이해할 수 있다. 즉, 앞서 살펴 본 기저를 바꾸는 행렬과 동일한 행렬이다. $A x$라는 변환을 이 맥락에서 다시 이해해보자. 여기서 $\mathcal B$는 표준 기저($\hat e_i$)로 이해하면 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$C^{-1} x$는 $[x]_{u}$의 기저를 $[x]_{\mathcal B}$로 바꾸는 것이다. 즉, $\phantom{}_{\mathcal B}1_u$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;이 바뀐 기저에서 $B$이라는 변환을 수행한다. 즉, $\phantom{}_{\mathcal B}B_\mathcal B$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$C$는 곱해 다시 통상적인 기저로 돌아오게 된다. 즉,  $\phantom{}_{u}1_\mathcal B$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;아래 그림에서 보듯이, $Ax$라는 변환과 $B[x]_{\mathcal B}$라는 변환은 좌표계만 다를 뿐 동일한 변환이다. $C B C^{-1}$의 기저의 변화를 살펴보면,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
\underbrace{u \rightarrow \mathcal B}_{C^{-1}} &amp; \cdots B \cdots \underbrace{\mathcal B \rightarrow u}_{C} \\
&amp; \cdots A \cdots
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;로 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/basis/coord.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similar matrix인 $A$과 $B$ 사이에서 다음과 같은 관계가 성립한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;${\rm Tr}(A) = {\rm Tr}(B)$&lt;/li&gt;
  &lt;li&gt;${\rm det}(A) = {\rm det}(B)$&lt;/li&gt;
  &lt;li&gt;${\rm rank}(A) = {\rm rank}(B)$&lt;/li&gt;
  &lt;li&gt;${\rm eig}(A) = {\rm eig}(B)$&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Concepts &amp;amp; Definition</summary></entry><entry><title type="html">Matrix as Linear Transformation</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/07/linear-transform-matrix.html" rel="alternate" type="text/html" title="Matrix as Linear Transformation" /><published>2021-01-07T00:00:00-06:00</published><updated>2021-01-07T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/07/linear-transform-matrix</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/07/linear-transform-matrix.html">&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/linear-transform/matrix_func.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 내용을 이해할 수 있다면, 더 할 것이 없다. 일단 잘 봐두도록 하자. 나중에 돌아와서 음미하면 의미가 와 닿을 것이다.&lt;/p&gt;

&lt;p&gt;선형 변환은 특별한 형태의 함수로 이해할 수 있다. 다만 투입과 산출이 다양한 차원(벡터)을 취할 수 있다. 그리고 이 선형 변환이 매트릭스로 표현될 수 있다. 때문에 매트릭스 표현이 강력하다. 추상적인 선형 변환 함수를 구체적으로 표현하고 쉽게 계산할 수 있게 만드는 것이 매트릭스다.&lt;/p&gt;

&lt;h2 id=&quot;linear-transformation&quot;&gt;Linear Transformation&lt;/h2&gt;

&lt;h3 id=&quot;concepts&quot;&gt;Concepts&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$V$: $T$의 인풋&lt;/li&gt;
  &lt;li&gt;$W$: $T$의 아웃풋&lt;/li&gt;
  &lt;li&gt;$T: V \to W$: $V$에서 $W$로의 선형 변환
    &lt;ul&gt;
      &lt;li&gt;$T(\vec v) = \vec w$. 즉, $\vec v \in V$를 $\vec w \in W$로 변환하는 것을 나타낸다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/linear-transform/matrix_func_fig.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;함수와 마찬가지로 위의 선형 변환에서 치역(Im($T$))와 커널(스칼라 함수에서는 $f(x) = 0$의 해)이 정의된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm Im}(T) \overset{\rm def}{=} \{ \vec w \in W | \vec w = T(\vec v) \text{ for some } \vec v \} \subseteq W
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm Ker}(T) \overset{\rm def}{=} \{ \vec v \in V | T(\vec v) = \vec 0 \} \subseteq V
$$&lt;/div&gt;

&lt;h2 id=&quot;matrix-representation&quot;&gt;Matrix Representation&lt;/h2&gt;

&lt;p&gt;인풋, 아웃풋의 기저 벡터를 다음과 같이 두자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
B_V = \{ \vec e_1, \dotsc, \vec e_n \}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
B_W = \{ \vec b_1, \dotsc, \vec b_m \}
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$M_T \in \mathbb R^{m \times n}$은 선형 변환 $T$의 매트릭스 표현이다.&lt;/li&gt;
  &lt;li&gt;보다 정확하게 표현해보자.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\phantom{}_{B_W}[M]_{B_V}
$$&lt;/div&gt;

&lt;p&gt;즉, $V$의 기저로 표현되는 인풋을 $W$의 기저로 표현되는 아웃풋으로 바꿔준다.&lt;/p&gt;

&lt;h2 id=&quot;linearity&quot;&gt;Linearity&lt;/h2&gt;

&lt;p&gt;선형의 의미는 무엇일까? 기하적으로 선을 다룬다는 뜻이 아니다. 선형의 의미는 함수적인 의미다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/linear-transform/linearity.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
T(\alpha_1 \vec v_1 + \alpha_2 \vec v_2) = \alpha_1 T(\vec v_1) + \alpha_2 T(\vec v_2) = \alpha_1 \vec w_1 + \alpha_2 \vec w_2
$$&lt;/div&gt;

&lt;h2 id=&quot;matrix-as-linear-transformation&quot;&gt;Matrix as Linear Transformation&lt;/h2&gt;

&lt;p&gt;왜 매트릭스가 선형 변환을 나타낼 수 있는지를 좀 더 들여다보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
T(\vec v)  &amp;=  T(v_1{\hat e_1} + \dotsb + v_n{\hat e_n} ) \\
&amp; =  v_1 T(\hat e_1) + \dotsb+ v_n T(\hat e_n)  \\
&amp; = 
\begin{bmatrix}
\vert &amp; \vert &amp; \vert \\
T(\hat e_1) &amp; \cdots &amp; T(\hat e_n) \\
\vert &amp; \vert &amp; \vert
\end{bmatrix} \vec v \\
&amp; = M_T \vec v
\end{aligned}
$$&lt;/div&gt;

&lt;h2 id=&quot;eat-this&quot;&gt;Eat This!&lt;/h2&gt;

&lt;h3 id=&quot;mapping-spaces&quot;&gt;Mapping Spaces&lt;/h3&gt;

&lt;p&gt;선형 변환을 다시 적어보자. $T: V \to W$ where $V \in \mathbb R^n$, $W \in \mathbb R^m$. 즉 이 변환은 $n$ 차원의 벡터를 $m$ 차원으로 바꿔주는 것이다. 이 변환의 투입이 지니는 차원을 생각해보자. $n$ 차원은 로우 공간이 생성하는 $\mathcal R (M_T)$와 널 공간으로 가는 $\mathcal N(M_T)$으로 나뉘게 된다. 그리고 이 공간은 서로 직합(direct sum) 관계다. 이를 요약하면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
T: \mathcal R(M_T) \to \mathcal C(M_T),~  
T: \mathcal N(M_T) \to \{ \vec 0 \}
$$&lt;/div&gt;

&lt;p&gt;즉 함수처럼 인풋 $\vec v \in \mathcal R(M_T)$이 $\vec w \in \mathcal C(M_T)$로 대응된다. 한편, $\vec v \in \mathcal N(M_T)$는 $\vec 0 \in W$으로 대응된다.&lt;/p&gt;

&lt;h3 id=&quot;surjective-and-injective&quot;&gt;Surjective and Injective&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/linear-transform/sur_inj.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;함수에서 전사 함수와 단사 함수의 개념을 그대로 적용할 수 있다. 선형 변환 혹은 행렬도 함수다.&lt;/p&gt;

&lt;p&gt;만일, $\vec v_1 \neq \vec v_2$이고 $\vec v_1, \vec v_2 \in \mathcal R(M_T)$라면 이는 선형 변환의 정의에 따라서 서로 다른 $\vec w$로 매핑된다. 따라서 만일 단사 변환이 되려면, $\mathcal N(M_T) = { \vec 0 }$만 성립하면 된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
Ay - Az = A(y-z)
$$&lt;/div&gt;

&lt;p&gt;단사 변환이란 오직 $x = y$일 때만 $Ax = Ay$가 성립한다는 뜻이다. 즉 위의 식에서 $A(x-y) = 0$이 $x=y$일 때만 성립하면 된다. 즉, $A x = 0$이 $x=0$일 때만 성립하면 된다. 전사 변환의 정의는 통상적인 정의와 같다; ${\rm Im} (T) = \mathbb R^m$.&lt;/p&gt;

&lt;p&gt;매트릭스의 맥락에서 다시 음미해보자. 만일 전사(surjective) 변환이 되려면 $n \geq m$이 성립해야 한다. 로우 스페이스의 차원이 컬럼 스페이스보다 커야 컬럼 스페이스 전체를 생성할 수 있다. 반면 단사(injective) 변환이 되려면 $n \leq m$이 되어야 한다. 1-1 대응이 가능하려면 컬럼 스페이스의 크기가 로우 스페이스보다 커여 한다.&lt;/p&gt;

&lt;p&gt;따라서 전단사 변환이 되기 위한 조건은 $m=n$이다. 함수에서 역함수가 존재하려면 전단사 함수여야 한다. 선형 변환도 마찬가지다. 역행렬이 존재하기 위한 필요 조건은 정방 행렬,  $m=n$이다.&lt;/p&gt;

&lt;p&gt;보다 상세한 내용은 &lt;a href=&quot;https://textbooks.math.gatech.edu/ila/one-to-one-onto.html&quot;&gt;여기&lt;/a&gt;를 참고하자.&lt;/p&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html"></summary></entry><entry><title type="html">Vector Geometry, part 2</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/06/vector-geometry-2.html" rel="alternate" type="text/html" title="Vector Geometry, part 2" /><published>2021-01-06T00:00:00-06:00</published><updated>2021-01-06T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/06/vector-geometry-2</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/06/vector-geometry-2.html">&lt;h2 id=&quot;vector-space&quot;&gt;Vector Space&lt;/h2&gt;

&lt;p&gt;먼저 벡터 스페이스를 살펴보기 위해서 몇 가지 정의부터 보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$V$ is vector space&lt;/li&gt;
  &lt;li&gt;$\vec v \in V$&lt;/li&gt;
  &lt;li&gt;$W$ is vector subspace $W \subseteq V$&lt;/li&gt;
  &lt;li&gt;span: 벡터의 선형 결합을 통해 생성되는 벡터 집합 
&lt;span class=&quot;kdmath&quot;&gt;$\text{span}(\vec v_1, \dotsc, \vec v_n) = \{\vec v | \vec v = \alpha_1 \vec v_1 + \dotsb + \alpha_n \vec v_n, \alpha_i \in \mathbb R \}$&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;행렬 $M \in \mathbb R^{m \times n}$이 있다고 할 때&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathcal R(M) \subseteq \mathbb R^n$: $M$의 로우 스페이스, 즉 $M$의 행들의 모든 가능한 선형 결합이 나타내는 벡터 공간&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal R (M) \overset{\rm def}{=} \{ \vec v \in \mathbb R^n | \vec v = \vec w^T M \text{ for some } \vec w \in \mathbb R^m \}
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathcal C(M) \subseteq \mathbb R^m$: $M$의 컬럼 스페이스, 즉 $M$의 열들의 모든 가능한 선형 결합이 나타내는 벡터 공간&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal C (M) \overset{\rm def}{=} \{ \vec w \in \mathbb R^m | \vec w = M v \text{ for some } \vec v \in \mathbb R^n \}
$$&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathcal N(M) \subseteq \mathbb R^n$: $M$의 널 스페이스. 즉, 오른쪽에 곱해졌을 때 $\vec 0_{m}$이 되는 벡터의 집합 
&lt;span class=&quot;kdmath&quot;&gt;$\mathcal N(M) \overset{\rm def}{=} \{ \vec v \in \mathbb R^n | M \vec v = \vec 0 \}$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal N(M^T) \subseteq \mathbb R^n$: $M$의 좌 널 스페이스. 즉, 왼쪽에 곱해졌을 때 $\vec 0_{n}$이 되는 벡터의 집합&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal N(M^T) \overset{\rm def}{=} \{ \vec w \in \mathbb R^m | \vec w^T M  = \vec 0^T \}
$$&lt;/div&gt;

&lt;p&gt;혹은&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal N(M^T) \overset{\rm def}{=} \{ \vec w \in \mathbb R^m | M^T \vec w = \vec 0 \}
$$&lt;/div&gt;

&lt;p&gt;$M$의 랭크는 컬럼 스페이스의 차원 그리고 로우 스페이스의 차원과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
r(M) = \dim(\mathcal R (M)) = \dim(\mathcal C(M))
$$&lt;/div&gt;

&lt;h2 id=&quot;checklist&quot;&gt;Checklist&lt;/h2&gt;

&lt;h3 id=&quot;zero-vector&quot;&gt;Zero vector&lt;/h3&gt;

&lt;p&gt;벡터 스페이스가 되려면 $\vec 0$를 집합 내에 지니고 있어야 한다. 간단한 내용 같지만 참 중요하다. 일단 벡터 스페이스의 정의에서 $\alpha \vec v$가 들어가기 때문에 $\alpha=0$의 조건에 따라서 $\vec 0$는 포함되어야 한다. 생각해 볼만한 대목. 벡터 서브스페이스 역시 마찬가지로 $\vec 0$을 포함해야 한다. 2차원 벡터로 이야기한다면, $y = 2x + 1$ 같은 형태의 선은 벡터 (서브) 스페이스가 될 수 없다.&lt;/p&gt;

&lt;h3 id=&quot;subset-vs-subspace&quot;&gt;Subset vs subspace&lt;/h3&gt;

&lt;p&gt;부분집합은 원래 집합에 일정한 조건을 부여한 것이고, 이 점에서는 서브 스페이스 역시 부분 집합에 속한다. 다만 모든 부분 집합에 서브 스페이스가 되진 않는다. 이 점에서 서브 스페이스의 조건이 보다 제약적이다.&lt;/p&gt;

&lt;p&gt;다음과 같은 두 방정식의 해를 비교해보자. $A \vec x = \vec b$, $A \vec x = \vec 0$&lt;/p&gt;

&lt;p&gt;$A \vec v_1 = \vec b$와 $A \vec v_2 = \vec b$를 생각해보자. $A(\vec v_1 + \vec v_2)$는 원래의 벡터 스페이스에 들어 있는가?&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A(\vec v_1 + \vec v_2) = 2 \vec b
$$&lt;/div&gt;

&lt;p&gt;$\vec b = \vec 0$가 아니라면, $\vec v_1 + \vec v_2$는 해가 될 수 없다. 조금 더 자세하게 표현해보자.&lt;/p&gt;

&lt;p&gt;$A \vec v = \vec b$의 해 공간은 다음과 같은 완전해의 집합이다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\{ \vec c + \vec v_n\} \text{ where }\vec v_n \in \mathcal N (A)
$$&lt;/div&gt;

&lt;p&gt;즉, 해 공간은 특수해(particular solution) $\vec c$와 널 스페이스에 속한 벡터의 합으로 구성된다. 해 공간의 원소 하나를 $\vec x_1 = \vec c + \vec v_1$이라고 하고 다른 하나를 $\vec x_2 = \vec c + \vec v_2$라고 하자. 이 둘을 더하면, $2 \vec c + \vec v_1 + \vec v_2$가 된다. 이것이 해 공간 안에 있어야 하는데, $\vec c \neq 0$이면 성립하지 않는다.&lt;/p&gt;

&lt;p&gt;한편 $\mathcal N(M)$은 자연스럽게 벡터 공간을 이룬다. 더한 것도 $\vec 0$에 있고, 스칼라 곱 역시 마찬가지다. 참고로 아래에서 보겠지만, $\mathcal R(M)$, $\mathcal C(M)$, $\mathcal N(M)$, $\mathcal N(M^T)$를 네 개의 근본 서브 스페이스라고 부른다. 네 개는 밀접한 연관을 지니고 있다.&lt;/p&gt;

&lt;h3 id=&quot;solutions&quot;&gt;Solutions&lt;/h3&gt;

&lt;p&gt;이 기회에 해의 종류를 한번 살펴보고 넘어가자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Particular solution (특수해): RREF에서 free variables를 모두 0으로 두고 찾은 해를 뜻한다.&lt;/li&gt;
  &lt;li&gt;Homogenous soution (일반해): 일반해는 널 스페이스에 속하는 해를 뜻한다. 즉, $A \vec x = 0$을 만족시키는 $\vec x$를 의미한다.&lt;/li&gt;
  &lt;li&gt;Complete solution (완전해): 완전해란 특수해와 일반해를 더한 형태이다. 즉,  $x_c = x_p + c_h$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이제 각각의 의미를 음미해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
A \vec x = A x_c = A(x_p + x_h) = A x_p + A x_h = A x_p + \vec 0_{m}
$$&lt;/div&gt;

&lt;p&gt;보다 자세한 사례는 &lt;a href=&quot;https://m.blog.naver.com/crm06217/221674223212&quot;&gt;여기&lt;/a&gt;를 참고하라.&lt;/p&gt;

&lt;h2 id=&quot;system-of-linear-equations&quot;&gt;System of Linear Equations&lt;/h2&gt;

&lt;p&gt;다음 방정식의 해 공간을 생각해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
M \vec x = \vec b
$$&lt;/div&gt;

&lt;p&gt;먼저 $M$의 널 스페이스를 생각해보자. 여기 속한 $\vec x$는 $\mathbb R^n$ 집합에 속한다. 정의상 $M \vec x = 0$이므로 $\vec x$를 어떤 해에 더하면 이 값 역시 해가 된다. 따라서, $M \vec x = \vec b$를 만족하는 $\vec x = \vec c$라고 하자. 이를 특수 해라고 부른다. 여기에 널 스페이스에 속한 임의의 원소를 더하면 완전해가 된다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec x = \vec c + \text{span}(\vec v_1, \dotsc, \vec v_k) \text{ where } \text{span}(\vec v_1, \dotsc, \vec v_k) = \mathcal N(M)
$$&lt;/div&gt;

&lt;h3 id=&quot;rref-method&quot;&gt;RREF method&lt;/h3&gt;

&lt;p&gt;연립방정식의 해를 구하는 가장 기초적인 방법, 즉 RREF, 기약 행사다리꼴 행렬을 만드는 과정이 이에 부합한다. 즉, $[M \lvert \vec b]$를 RREF로 만들면, $[\text{rref}(M) \vert \vec c]$의 형태가 된다. 이때, $\text{rref}(M)$는 $k$ 개의 자유 변수를 $n-k$ 개의 특수 해를 지니게 된다. 자유 변수에서 널 스페이스에 속하는 일반해를 얻을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;two-transformation&quot;&gt;Two Transformation&lt;/h2&gt;

&lt;p&gt;$M \in \mathbb R^{m \times n}$이 편리한 이유는 $M$이 $\vec x \in \mathbb R^n$을 $\vec y \in \mathbb R^m$으로 바꾸는 선형 변환 모두를 표현할 수 있기 때문이다. 그렇다면, $M^T$는 어떨까? 이는 같은 맥락에서  $\vec a \in \mathbb R^m$을 $\vec b \in \mathbb R^n$으로 바꾸는 선형 변환을 표현한다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec a^T M = \vec b
$$&lt;/div&gt;

&lt;p&gt;이는 $M^T \vec a= \vec b$가 된다. 즉, $M^T$는 $M$의 좌 벡터 공간의 집합을 표현한다. 마찬가지로 $M$은 $M$의 우 벡터 공간의 집합을 표현한다. $M$의 좌 벡터 공간이 바로 로우 공간이고 우 벡터 공간이 컬럼 공간이다.&lt;/p&gt;

&lt;p&gt;이제, $\mathcal N(M)$의 원소는 $\mathbb R^n$에 속한다. 이와 직교하는 공간은 어떤 공간일까? 좌 벡터 공간일까? 우 벡터 공간일까? 쉽게 생각하자. 직교 하기 위해서는 서로 차원이 같아야 한다. 우 벡터 공간은 $\mathbb R^m$에 속한다. 따라서 직교한다면 좌 벡터 공간과 한다. 확인해보자. $M \vec v_n = \vec 0_m$ where $\vec v_n \in \mathcal N(M)$ 가 성립한다. 이제 양번에 $\vec a^T$를 곱해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\underbrace{(M^T \vec a)^T}_{\text{left space}} \vec v_n = \vec a^T M\vec v_n = \vec a^T \vec 0_m = 0
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal R(M) \oplus \mathcal N(M) = \mathbb R^n
$$&lt;/div&gt;

&lt;p&gt;한편 같은 논리로&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\mathcal C(M) \oplus \mathcal N(M^T) = \mathbb R^m
$$&lt;/div&gt;

&lt;p&gt;앞서 보았듯이 ${\rm rank} (M) =\dim(\mathcal R(M)) = \dim(\mathcal C(M))$다. $\dim(\mathcal N(M)) = {\rm nullity} (M)$이라고 하면,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
{\rm rank} (M) + {\rm nullity} (M) = n = \dim(\mathbb R^n)
$$&lt;/div&gt;

&lt;p&gt;이 모든 걸 그림 하나로 정리하면 다음과 같다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.cs.utexas.edu/users/flame/laff/alaff-beta/images/Chapter04/FundamentalSpaces.png&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;
&lt;!--stackedit_data:
eyJoaXN0b3J5IjpbMTU3NzQ3MzMwMiwtMzU1MjA2NzgzLDE5Nz
UwMzk5MzUsLTYzNjI0MTEyNywtMTI5MDAyNTUyMiwyMDg4MDQz
Mzc0LC0yMDUyMjAxNzQwLC04MzgyODM3MDddfQ==
--&gt;&lt;/p&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Vector Space</summary></entry><entry><title type="html">Vector Geometry, part 1</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/05/vector-geometry-1.html" rel="alternate" type="text/html" title="Vector Geometry, part 1" /><published>2021-01-05T00:00:00-06:00</published><updated>2021-01-05T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/05/vector-geometry-1</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2021/01/05/vector-geometry-1.html">&lt;h2 id=&quot;concepts&quot;&gt;Concepts&lt;/h2&gt;

&lt;p&gt;편의상 3차원 공간을 예시하도록 한다. $n$ 차원으로 확장은 쉽게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$p = (p_x, p_y, p_z)$: $p \in \mathbb R^3$&lt;/li&gt;
  &lt;li&gt;$\vec v = (v_x, v_y, v_z)$: $\vec v \in \mathbb R^3$&lt;/li&gt;
  &lt;li&gt;$\hat v = \dfrac{v}{\lVert v \lVert}$: Unit vector&lt;/li&gt;
  &lt;li&gt;$p_0$를 지나는 무한의 1차원 선은 다음과 같이 정의된다.
    &lt;ul&gt;
      &lt;li&gt;Parametric equation
  &lt;span class=&quot;kdmath&quot;&gt;$l : \{ (x, y, z) \in \mathbb R^3 | p_0 + t \vec v, t \in \mathbb R\}$&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;Symetric equation
  &lt;span class=&quot;kdmath&quot;&gt;$l: \{  (x, y, z) \in \mathbb R^3 | \dfrac{x - p_{0x}}{v_x} = \dfrac{y - p_{0y}}{v_y}  =  \dfrac{z - p_{0z}}{v_z} \}$&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$p_0$를 지나는 무한 평면 P는 다음과 같이 정의된다.
    &lt;ul&gt;
      &lt;li&gt;General equation
  &lt;span class=&quot;kdmath&quot;&gt;$P: \{ (x, y, z) \in \mathbb R^3 | Ax + By + Cz = D \}$&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;Parametric equation
  &lt;span class=&quot;kdmath&quot;&gt;$P : \{ (x, y, z) \in \mathbb R^3 | p_0 + s \vec v + t \vec w, s, t \in \mathbb R \}$&lt;/span&gt;&lt;/li&gt;
      &lt;li&gt;Geometric equation
  &lt;span class=&quot;kdmath&quot;&gt;$P : \{ (x, y, z) \in \mathbb R^3 | \vec n \cdot [(x,y,z) - p_0] = 0 \} \text{with normal vector $\vec n$}$&lt;/span&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;point-vs-vector&quot;&gt;Point vs vector&lt;/h3&gt;

&lt;p&gt;점과 벡터의 차이는 무엇일까? 점은 좌표계에서 원점을 기준으로 한 위치를 나타낸다. 벡터는 크기와 방향을 모두 나타내며, 벡터는 $(0,0)$ 같은 기준이 없다.&lt;/p&gt;

&lt;p&gt;벡터를 점으로 나타내려면 어떻게 해야 할까? 두 점의 차이를 구하면 이는 벡터가 된다. 쉽게 생각해보자. 2차원 데카르트 좌표계에서 $(3,3)$이라는 위치를 생각해보자. $(3,3)$을 그냥 나타내면 점이다. 반면, $(3,3) - (0,0)$을 의미하면 $(3,3)$은 벡터가 된다. 그래프 상에서 $(0,0)$에서 $(3,3)$으로 화살표를 그리면 벡터가 된다. 아래 그림을 참고 하자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://notesclasses.com/wp-content/uploads/2020/04/Displacement-in-Physics-Explanation.png&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;normal-vector&quot;&gt;Normal vector&lt;/h3&gt;

&lt;p&gt;노멀 벡터는 아래 플레인 혹은 벡터과 직교하는 성분의 벡터다. 이 녀석을 어떻게 구할까? 플레인의 파라메트릭 식을 보자. 2차원 플레인을 구성하는 두 벡터 $\vec v$, $\vec w$가 있다. 노멀 벡터는 이 두 성분 모두와 직교하는 성분의 벡터이다. 이는 바로 크로스 프로덕트의 정의를 그대로 따른다. 평면위의 어떤 세 점 $p, q, r$이 있다고 하자. 평면 위에 존재하는 벡터 두 개를 $\vec v = q - p$, $\vec w = r - p$과 같이 만들자. 이 벡터와 직교하는 벡터 $\vec n$은 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec n  = \vec v \times \vec w = (q-p) \times (r-p)
$$&lt;/div&gt;

&lt;h2 id=&quot;distance&quot;&gt;Distance&lt;/h2&gt;

&lt;p&gt;거리 역시 세 가지로 나누어 이해해보자. 점과 점 사이의 유클리드 거리는 생략하겠다. 벡터와 점의 거리는 어떻게 구할까? 플레인과 점의 거리는 어떻게 구할까? &lt;a href=&quot;https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/12/16/Projection.html&quot;&gt;이 포스팅&lt;/a&gt;을 참고하라. 
&lt;!--stackedit_data:
eyJoaXN0b3J5IjpbODczNTYwMTQzLC0yMTIxMjAzNTM5LC0xND
A5NDUwNzgsMTkwNjcxMDg2MCw2NDI2ODMyMTVdfQ==
--&gt;&lt;/p&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Concepts</summary></entry><entry><title type="html">Projection and Distance</title><link href="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/12/16/Projection.html" rel="alternate" type="text/html" title="Projection and Distance" /><published>2020-12-16T00:00:00-06:00</published><updated>2020-12-16T00:00:00-06:00</updated><id>https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/12/16/Projection</id><content type="html" xml:base="https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/12/16/Projection.html">&lt;h2 id=&quot;projection-scalar-and-vector&quot;&gt;Projection: Scalar and Vector&lt;/h2&gt;

&lt;h3 id=&quot;definition&quot;&gt;Definition&lt;/h3&gt;

&lt;p&gt;다음과 같은 두 개의 벡터, $\vec a$, $\vec b$를 일단 떠올려보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/9/98/Projection_and_rejection.png&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;스칼라 프로젝션 $a_1$의 정의는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
a_1 ={\cos \theta}{\lVert \vec a \lVert} = \lVert \vec a_1 \lVert
$$&lt;/div&gt;

&lt;p&gt;각 $\theta$에 관해서 다음과 같이 정의할 수 있다. 혹시 리마인드가 필요하면 포스팅 &lt;a href=&quot;https://anarinsk.github.io/lostineconomics-v2-1/math/2019/07/18/dot-product.html&quot;&gt;dot product&lt;/a&gt;를 참고하라.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\cos \theta = \dfrac{\vec a \cdot \vec b}{\lVert\vec a\lVert \lVert\vec b\lVert}
$$&lt;/div&gt;

&lt;p&gt;따라서, $\hat\vec b = \dfrac{\vec b}{\lVert\vec b\lVert}$라고 할 때&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
a_1 = {\cos \theta}{\lVert \vec a \lVert} = \dfrac{\vec a \cdot \vec b}{\lVert\vec b\lVert} = \vec a \cdot \hat\vec b
$$&lt;/div&gt;

&lt;p&gt;쉽게 말해서, $\vec a$ 벡터와 정규화된 $\vec b$의 닷프로덕트라고 생각하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;vector-projection&quot;&gt;Vector projection&lt;/h3&gt;

&lt;p&gt;스칼라 프로젝션의 크기로 $\vec b$의 벡터를 만든 것이 벡터 프로젝션이다. $\vec a$를 $\vec b$ 위로 프로젝션한 벡터 $\Pi_{\vec b} (\vec a)$는 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec a_1 = \Pi_{\vec b} (\vec a) = a_1 \hat\vec b = \dfrac{\vec a \cdot \vec b}{\lVert\vec b\lVert} \dfrac{\vec b}{\lVert\vec b\lVert}
$$&lt;/div&gt;

&lt;p&gt;말로 풀어보자. $\vec b$와 같은 방향성을 지니는 벡터를 $\vec a$와 $\vec b$ 간의 스칼라 프로젝션의 크기로 만들어주는 것이 벡터 프로젝션이다.&lt;/p&gt;

&lt;h3 id=&quot;projection-as-outer-product&quot;&gt;Projection as outer product&lt;/h3&gt;

&lt;p&gt;$x$축 위로 프로젝션을 생각해보자. 이 역시 선형 변환이다. 기저를 생각하면 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_{x}(
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}) = 
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}, ~
\Pi_{x}(
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}) = 
\begin{bmatrix}
0 \\
0 \\
\end{bmatrix}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
M_{\Pi_x} = 
\begin{bmatrix}
\Pi_{x}(
\begin{bmatrix}
1 \\
0 \\
\end{bmatrix}),~
\Pi_{x}(
\begin{bmatrix}
0 \\
1 \\
\end{bmatrix}) 
\end{bmatrix} = 
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{bmatrix}
$$&lt;/div&gt;

&lt;p&gt;$M_{\Pi_x}$를 외적의 관점에서 표현해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\begin{aligned}
\Pi_{x}(\vec v) &amp; = (\hat i \cdot \vec v) \hat i = \hat i (\hat i \cdot \vec v) = \hat i (\hat i^T \vec v) = (\hat i \hat i^T)\vec v \\
&amp; = 
\begin{bmatrix}
1 &amp; 0 \\
0 &amp; 0
\end{bmatrix} \vec v = M_{\Pi_x} \vec v
\end{aligned}
$$&lt;/div&gt;

&lt;p&gt;이 결과를 일반적인 벡터 $\vec b$에도 확장할 수 있다. $\vec b$로의 프로젝션을 구현하기 위해서는 표준화된 벡터 $\hat b$를 먼저 구해야 한다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\hat b = \dfrac{\vec b}{\Vert b \Vert},~M_{\Pi_{\hat b}} \vec a = \hat b \hat b^T \vec a
$$&lt;/div&gt;

&lt;p&gt;$\vec a$를 인풋 벡터로 이해하면 이를 $\vec b$의 프로젝션 위치로 옮기는 선형 변환이 $\Pi_{\hat b}$에 해당한다.&lt;/p&gt;

&lt;h2 id=&quot;more-definition&quot;&gt;More Definition&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$S \subseteq \mathbb R^n$: $S$는 벡터 부분공간이라고 하자.&lt;/li&gt;
  &lt;li&gt;$S^\perp$: $S$와 직교 벡터들의 집합이고 이 역시 벡터 부분공간이다. 
&lt;span class=&quot;kdmath&quot;&gt;$S^\perp = \{ \vec w \in \mathbb R^n : \vec w \cdot S = 0 \}$&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;$\Pi_S$: 부분공간 $S$ 위로 프로젝션&lt;/li&gt;
  &lt;li&gt;$\Pi_{S^\perp}$: 부분공간 $S^\perp$ 위로 프로젝션&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\Pi_S$는 일종의 함수로서 다음과 같이 정의된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_S: \mathbb R^n \to S
$$&lt;/div&gt;

&lt;p&gt;$\vec x \in \mathbb R^n$의 어떤 벡터가 $\Pi_S$를 거치면 $S$에 속하지 않는 나머지 부분은 사라지게 된다. 즉, $S$라는 화면 위로 자신의 이미지를 투영한다고 보면 된다. 그래서 프로젝션이다.&lt;/p&gt;

&lt;p&gt;몇가지 특징은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If ${\vec v} \in S$, then $\Pi_S(\vec v) = \vec v$&lt;/li&gt;
  &lt;li&gt;If $\vec w \in S^\perp$, then $\Pi_S(\vec w) = \vec 0$&lt;/li&gt;
  &lt;li&gt;if $\vec u = \alpha \vec v + \beta \vec w$ where $\vec v \in S$, $\vec w \in S^{\perp}$, then $\Pi_S(\vec u) = \alpha \vec v$&lt;/li&gt;
  &lt;li&gt;$\Pi_S(\vec v) (\Pi_S(\vec v) ) = \Pi_S(\vec v)$ (idempotent)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projection-onto-line&quot;&gt;Projection onto line&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/projection/vector_proj.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 $\vec u$의 $\vec v$로의 프로젝션은 앞서 살펴본 벡터 프로젝션이다. $\vec v$ 대신 $l$로 표기된 점에 유의하자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
l: \{ \vec v' \in \mathbb R^n \lvert \vec v' = \vec 0 + t \vec v, t \in \mathbb R \}
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_l(\vec u) = \dfrac{\vec u \cdot \vec v}{\lVert\vec v\lVert^2} \vec v
$$&lt;/div&gt;

&lt;p&gt;이제 여기서 $\Pi_{l^{\perp}}$를 구해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec u = \Pi_{l} (\vec u) + \Pi_{l^{\perp}} (\vec u)
$$&lt;/div&gt;

&lt;p&gt;잠시 생각해볼 것이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\Pi_{l^{\perp}}(\vec u)$가 의미하는 것은 무엇일까? $\vec u$에서 $\Pi_{l}(\vec u)$와 직교하는 벡터 프로젝션이다. 그런데, 둘은 $\Pi_{l^{\perp}}(\vec u)$ 만난다. 따라서, 프로젝션의 결과는 $\Pi_{l^{\perp}}(\vec u)$ 위에 있게 된다.위 그림은 이를 나타낸다.&lt;/li&gt;
  &lt;li&gt;$\vec u$라는 벡터가 두 개의 직교하는 성분의 결합을 통해 표현될 수 있다는 것을 알 수 있다. $\vec u = \Pi_{l}(\vec u) + \Pi_{l^\perp}(\vec u)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;projection-onto-plane&quot;&gt;Projection onto plane&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/projection/vector_proj_2.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;평면 위로의 프로젝션을 생각해보자. 기본적인 원리는 동일하다. 이에 앞서 평면을 벡터로 표현하는 방법에 대해 알아보자. 가장 편리한 방법은 노멀 벡터를 활용하는 것이다. 즉, 평면 위의 점 $\vec {p_0}$를 지나는 $P$는 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
P = \{ \vec p \in \mathbb R^n : \vec n \cdot(\vec p - \vec p_0) = 0 \}
$$&lt;/div&gt;

&lt;p&gt;여기서 노멀 벡터 $\vec n$은 프로젝션된 지점에서 평면과 직교하는 성분을 나타낸다. 3차원의 경우 노멜 벡터는 3차원 공간에서 해당 평면이 놓인 모습을 결정한다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\vec u = \Pi_P(\vec u) + \Pi_{P^\perp}(\vec u)
$$&lt;/div&gt;

&lt;p&gt;그림에서 보듯이 $\vec u$에서 $\vec n$으로의 프로젝션은 앞서 보았던 벡터에서 벡터로의 프로젝션이다. 따라서 앞서 구한 공식을 그대로 활용하자.  즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_{P^\perp}(\vec u) = \dfrac{\vec u \cdot \vec n}{\lVert\vec n\lVert}\dfrac{\vec n}{\lVert \vec n \lVert}
$$&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/projection/vector_proj_2a.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;벡터는 점과 달리 상대적인 위치로 정의된다. 벡터 연산은 그런 맥락에서 이루어진다. $\Pi_{P^\perp}$를 위 그림처럼 이동해 보자. 이렇게 보면 벡터 프로젝션이 자연스럽다. P에서의 위치는 변하지 않고 노멀 벡터 쪽으로만 변화하므로 벡터 자체는 이동 전 벡터와 이동 후 벡터가 동일하다. 즉,  $\Pi_{P^\perp}$를 왼쪽으로 이동시킨 벡터에 대한 프로젝션과 $\Pi_{P^\perp}$이 같다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_{P^\perp} = \dfrac{\vec u \cdot \vec n}{\lVert\vec n\lVert^2}{\vec n}
$$&lt;/div&gt;

&lt;p&gt;이제 $\Pi_{P^\perp}$ 대입하면 $\Pi_{P}$를 쉽게 구할 수 있다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_P(\vec u) + \Pi_{P^\perp} = \vec u
$$&lt;/div&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
\Pi_P(\vec u)  = \vec u - \dfrac{\vec u \cdot \vec n}{\lVert\vec n\lVert^2}{\vec n}
$$&lt;/div&gt;

&lt;h2 id=&quot;distance-in-vector-space&quot;&gt;Distance in Vector Space&lt;/h2&gt;

&lt;p&gt;프로젝션은 거리를 측정할 때 유용하다. 먼저 원점(굳이 원점일 필요는 없다)과 해당 벡터 공간을 지나는 선 사이의 거리를 구해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
l: \{ \vec p \in \mathbb R^n | \vec p = \mathbf {p}_0 + t \vec v, t \in \mathbb R \}
$$&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/projection/line.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 원점과 $l$의 거리는 어떻게 나타낼 수 있을까? $l$이 지나가는 $\vec p_0$를 그대로 두고, $l$이 원점을 지나가도록 이동해보자. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
l_0 = \{  \vec p \in \mathbb R^n | \vec p = \mathbf {0} + t \vec v, t \in \mathbb R \}
$$&lt;/div&gt;

&lt;p&gt;이제 이 직선과 $\vec p_0$ 사이의 거리를 구하면 된다 .이는 앞서 제시한 벡터 프로젝션의 수직 벡터의 길이와 같다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
d(l, \vec 0) = d(\vec p_0, l_0) = \lVert \mathbf p_0 - \dfrac{\mathbf p_0 \cdot \vec v}{\lVert \vec v\lVert^2}\vec v \lVert
$$&lt;/div&gt;

&lt;p&gt;이제 아래와 같은 평면과 원점의 거리를 측정해보자.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
P = \{ \vec p \in \mathbb R^n : \vec n \cdot(\vec p - \vec {p}_0) = 0 \}
$$&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lostineconomics-v2-1/blob/master/images/projection/plane.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;원점을 지나도록 평면을 이동시키고 평면의 노멀 벡터가 정의된 $p_0$와 원점을 지나는 평면 $P_0$ 사이의 거리를 측정하면 된다.&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
P_0 = \{ \vec p \in \mathbb R^n : \vec n \cdot(\vec p - \vec {0}) = 0 \}
$$&lt;/div&gt;

&lt;p&gt;이 둘 사이의 거리는 $\Pi_{P^\perp}$의 거리와 같다. 즉,&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
d(P, \vec 0) = d(P_0, \vec p_0) = \lVert  \dfrac{\vec p_0 \cdot \vec n}{\lVert\vec n\lVert}\dfrac{\vec n}{\lVert \vec n \lVert}\lVert = \dfrac{|\vec p_0 \cdot \vec n |}{\lVert\vec n\lVert}
$$&lt;/div&gt;

&lt;p&gt;다시 확인해보자. 노멀 벡터 $\vec n$은 $\vec p_0$와 같은 같은 방향에 있는 벡터다. 따라서 위의 식이 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;application-regression-coefficient&quot;&gt;Application: Regression Coefficient&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/anarinsk/lie-regression/blob/master/assets/imgs/reg-in-vectorspace.png?raw=true&quot; alt=&quot;enter image description here&quot; style=&quot;margin: auto; display: block; border:1.5px solid #021a40;&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 소개했던 회귀 분석에 관한 포스팅 &lt;a href=&quot;https://anarinsk.github.io/lostineconomics-v2-1/math/econometrics/regression/2019/10/25/understanding-regression.html&quot;&gt;Understanding Regression&lt;/a&gt;을 다시 보자. Origin–Observed Y–Fitted $\hat Y$이 만드는 삼각형을 보자. 직각 삼각형이다. 여기서 잔차에 해당하는 $e$는 $X$의 컬럼 스페이스와 항상 직교한다. 즉, $X’ e = 0$이 성립한다. 그리고 $\hat Y = X \hat\beta$이므로&lt;/p&gt;

&lt;div class=&quot;kdmath&quot;&gt;$$
X'(Y - \hat Y)  = X'(Y - X \hat\beta) = 0
$$&lt;/div&gt;

&lt;p&gt;이를 노멀 방정식이라고 부른다. 앞서 평면과 직교하는 벡터를 노멀 벡터라고 불렀는데, 둘은 일맥상통한다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Ivan Savov, &lt;em&gt;No bullshit guide to linear algebra 2nd Edition&lt;/em&gt;, Minireference, 2017&lt;/li&gt;
&lt;/ul&gt;

&lt;!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1ODc3NjU5MzMsMTIwNzg4MzkwNywxND
kxNTUxMzkxLC00MTI5NDc1ODksLTE4MDM2OTE2OTEsMTgwMzM1
NTM5NCwxMzk3MDg3NTcxLC0xMDc5MjIxMjU3LDIyODU3OTg1OS
wtMTU1MzE1Mzc2NSwyMzgzODczNjksLTMwODM5NTIyNywtMTY3
NjAyMzQzLDExNjIyNDgyMDcsMTUyMzEwMTc5NCwtNDY1MDM1ND
Y3LC0xMDkyODY4NTM2LDczODI1MzY4NSwtMjkyMzEyNTQ2LC0x
MjEyMzEwM119
--&gt;</content><author><name></name></author><category term="math" /><category term="matrix-theory" /><summary type="html">Projection: Scalar and Vector</summary></entry></feed>