{
  
    
        "post0": {
            "title": "Following Gilbert Strang 4",
            "content": "Vector Spaces . Spaces is “bunch of something,” in this case, vector | Adding between members and multiplying scalar also produces a member in the space. | . 벡터 공간 $V$에 대해서, $w, v in V$일 때, $ alpha, beta in mathbb R$, 일 때 $ alpha w + beta v in V$을 만족하면 $V$는 벡터 공간이다. . 벡터 공간 안의 두 원소를 더한 것이 벡터 공간에 속해야 한다. | 원소에 스칼라 곱을 한 것이 벡터 공간에 속해야 한다. | 부분 공간이란 어떤 벡터 공간 안에 들어가 있는 또다른 벡터 공간을 의미한다. 부분 공간 자체로 벡터 공간이 되어야 하기 때문에 ${ 0 }$를 반드시 지니고 있어야 한다. | 어떤 유한한 공간은 벡터 공간이 될 수 없다. | . | . Subspaces . A vector space inside $ mathbb R^2$. $ mathbb R^2$ 평면 안에 들어간 수직선을 생각하자. | . | 부분 공간은 그 자체로서 vector spaces로서 성립해야 한다. | . Three question to be checked . 출처를 확인해도 된다. | . x1=[013], x2=[240]x_1 = begin{bmatrix} 0 1 3 end{bmatrix},~ x_2 = begin{bmatrix} 2 4 0 end{bmatrix}x1​=⎣⎢⎡​013​⎦⎥⎤​, x2​=⎣⎢⎡​240​⎦⎥⎤​ . $x_1$과 $x_2$가 생성하는 부분공간 $V_1$, $V_2$를 찾고, $V_1 cap V_2$를 찾아보자. | ${ x_1, x_2 }$가 생성하는 부분공간 $V_3$를 찾아보자. 이는 $V_1 cup V_2$과 일치하는가? | $V_3$의 어떤 부분공간을 $S_3$라고 할 때 $x_1 notin S_3$ and $x_2 notin S_3$인 $S_3$가 존재하는가? | $V_3$와 $x-y$ 평면의 교점은 존재하는가? | Q1 . . V1∩V2={0}V_1 cap V_2 = {0 }V1​∩V2​={0} . Q2 . . $V_3$는 평면이다. 하지만 $V_1 cup V_2$는 두 벡터 부분공간의 합집합일 뿐이다. | . x3=[253]∉V1∪V2x_3 = begin{bmatrix} 2 5 3 end{bmatrix} notin V_1 cup V_2x3​=⎣⎢⎡​253​⎦⎥⎤​∈/​V1​∪V2​ . Q3 . . 단순하게 $x_1 + x_2$가 생성하는 부분공간을 생각해보자. | 그림에서 보듯이 이는 $V_1$ , $V_2$ 어디에도 속하지 않는다. | . Q4 . 역시 그림에서 분명히 확인할 수 있다. | . $V_3 cap$ ($x-y$ plane$) = V_2$ . Column Space and Null Space . https://www.youtube.com/watch?v=8o5Cmfpeo6g . A⏟m×nx={0} underbrace{ mathbf A}_{m times n} x = { 0 }m×n . A​​x={0} . $C mathbf A)$ is the vector space generated by columns of $ mathbf A$ $C( mathbf A) in mathbb R^m$ | . | $ mathcal N( mathbf A)$ is the vector space generated by solution $x$ of $ mathbf A x = {0}$ where $x neq {0}$ $ mathcal N( mathbf A) in mathbb R^n$ | . | . Ax=c, where c≠{0}{ mathbf A} x = c, text{~where } c neq {0 }Ax=c, where c​={0} . 위 식의 해 $x$는 벡터 공간일까? 당연히 아니다! 왜냐하면, $x =0$를 해로 지니지 못하기 때문이다. | 매트릭스 형태로 표현된 연립방정식의 해는 벡터 공간이 될 수 없다. | .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/gilbert-strang/2020/03/19/vector-space.html",
            "relUrl": "/math/matrix-theory/gilbert-strang/2020/03/19/vector-space.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "WSL + Cmder + Python",
            "content": "Assumptions . 아주 친절한 가이드는 아니다. 이 내용을 모두 까먹을 미래의 나를 위한 포스팅이다. | 아래에서 |Something-anyting|으로 표현된 것은 각자의 환경에 따라서 바뀌는 부분이다. | . WSL . 윈도우 10의 가장 뛰어난 무기라면 Windows System for Linux (WSL)이 아닐까 싶다. 가상 머신을 돌리는 차원을 넘어서 거의 네이티브에 가깝게 리눅스를 지원한다.1 아래 화면에서 보듯이 Windows Store에 Ubuntu가 올라와 있어서 설치에도 크게 신경 쓸 것이 없다. . . 다운 받아서 설치하고 한번 실행해줘야 한다. 그래야 계정 설정이 된다. 화면의 안내대로 아이디와 패스워드를 생성하면 된다. 여기까지 마치면 WSL을 쓸 준비 완료다. . Cmder for WSL . 이제 좀 더 편리하고 미감도 좋은 윈도용 코맨드라인 툴 Cmder에서 WSL을 설정해보자. 의외로 간단하다. . . settings → Startup → Tasks | . 여기 항목 중에서 {Bash::bash}를 하나 클론한 후 이름을 적당한 것으로 바꾸도록 하자. 내 경우는 {Bash::WSL}로 설정했다. 커맨드 필드 아래와 같이 붙여 넣는다. . %windir% system32 bash.exe -new_console ~ . WSL을 통해 bash가 설치되었으므로 얘를 쓰겠다는 이야기다. bash를 불러오면 Ubuntu에 접근할 수 있다. 끝이다. . Ubuntu, first thing first . 우분투의 경우 설치를 한 후 몇 가지 업데이트 작업을 해줘야 한다. . sudo apt-get update # 업데이트 목록 갱신 sudo apt-get upgrade # 현재 패키지 업그레이드 sudo apt-get dist-upgrade # 신규 업데이트 설치 . 몇가지 알아두면 좋은 설정 항목이 있다. . sudo dpkg-reconfigure tzdata # 타임존 맞추기 . 업데이트하는 패키지 기본 주소가 해외다. 국내로 맞추면 다운로드 등에서 조금 유리할 수 있다. 자세한 내용은 여기를 참고하시라. . Installing python . 이제 python(이하 파이썬으로 쓰기도 병행하겠다)을 설치할 차례다. 다소 의아할지 모르겠지만, macos, 윈도에 비해서 파이썬을 설치하고 운용하는 것이 쉽지 않다. 리눅스의 경우 하나하나 설정해야 하는 것이 많다. 내용을 자세히 안다면 세밀하고 명확한 통제가 가능한 것이겠지만, 주류 OS의 편안함에 익숙하다면 당황스러운 경우를 접하게 된다. . 우선, 우분투18.04에는 파이썬이 탑재되어 있다. 2.7, 3.6x 두가지 버전이나 이미 깔려 있다. 그럼, 최신 python은 어떻게 써야 할까? . sudo apt update sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt update sudo apt install python3.7 . 이렇게 설치한 후 . ~$ python3.7 --version Python 3.7.7 . 이렇게 잘 출력되어야 한다. 하지만 매번 python3.7이라고 칠 수는 없는 노릇이니, 얘가 내가 쓰고 싶은 파이썬이란 점은 bash에게 알려줘야 한다. . cd ~ # 홈으로 이동 ~$ sudo nano /.bash_aliases . 우분투 18.04에는 nano 에디터가 들어 있다. 없다면, vim을 써도 된다. 이제 이 파일에 아래과 같은 내용을 적고 저장한다. sudo를 쓰는 이유는 파일을 생성하게 되기 때문이다. . alias python=python3.7 alias pip=pip3 . 보통 어떤 포스트의 경우 여기에서 sudo apt install python3-pip로 pip를 업데이트할 수 있다고 가이드하고 있다. 하지만, 18.04버전에서는 9.0 이상으로 pip가 올라가지 않는다. 아래와 같이 하면 최신 판본(이 글을 쓰는 시점에서는 20.0.2)으로 업그레이드가 가능하다. . sudo apt-get purge python-pip wget https://bootstrap.pypa.io/get-pip.py sudo python get-pip.py pip --version rm get-pip.py . 아래와 같이 나와주면 준비완료다. . ~$ pip --version pip 20.0.2 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7) . 만일 가상환경을 쓰고 싶다면, 아래와 같이 venv를 설치해서 쓰면 되겠다. venv는 python 3.3 이후에 파이썬에 내장된 자체 가상 환경이다. conda나 virtualenv 같은 걸 써도 무방하겠다. . sudo apt install python3.7-venv python -m venv |your-venv| source |your-venv|/bin/activate . 정신건강을 위해서 python 개발에 필요한 필수 패키지도 설치해두도록 하자. . sudo apt-get install python3.7-dev . Connect to VS Code . 이제 우분투 18.04를 설치하고 파이썬까지 잘 설치했다. 하지만 코딩은 어떻게 하지? 우분투에 VS Code를 깔아야 할까? 그런데 현재 설치한 우분투는 GUI가 없는 버전이니까 X-Windows 같은 것을 깔아야 할까? 걱정할 필요 없다! 윈도에 깔린 VS Code를 그대로 쓰면 된다. . VS Code의 extension 중에서 Remote - WSL이란 것이 있다. 이 녀석을 깔면 윈도 시스템에 설치된 WSL에 VS Code가 접근할 수 있게 된다. . . VS Code 창 아래 왼쪽에 녹색 바를 클릭하면 WSL에 연결할 수 있게 된다. . . 일단 WSL에 접속하면 별도의 창이 뜬다. 이는 별도의 환경으로 운용되므로 알맞은 세팅을 쓰면 된다. . 가상 환경을 쓰는 경우 가상 환경에 접근할 수 있어야 한다. . VS Code의 메뉴에서 File → Preference → settings | “python:Pythonpath”로 검색한 후 아래를 브라우징하다보면, “Venv path”라는 항목이 나온다. 만일 conda와 같은 다른 가상 환경을 쓴다면 해당 항목에 적당한 디렉토리를 넣으면 되겠다. 가상 환경이 이름이 .pyvenv라면 적당한 디렉토리와 함께 아래와 같이 심어주면 되겠다. | . | . . 여기까지 마쳤으면 WSL에 접근된 상태에서 WSL에 깔린 파이썬 가상 환경으로도 접근이 가능하다. 해당 python 커널을 인식해주자. 아래 그림에서 왼쪽의 보라색을 선택하자. Venv가 제대로 인식되었다면 시스템에 설치된 python 버전 뿐 아니라 Venv 상의 버전들도 보일 것이다. | VS Code가 자신의 환경에서 파이썬을 구동하는 데 필요한 프로그램을 자동으로 깐다. 그냥 깔게 두면 된다. | . . 이후 VS Code를 이용해 필요한 파일을 불러와도 되고, 아니면 Cmder 창에서 아래와 같이 실행하자. . code |your-working-python-file.py| . 실행하면 윈도에 구동되는 VS Code 위에서 WSL 내의 파일들이 뜬다. 편하게 작업할 수 있다. . Why… . 사실 그냥 윈도에서 파이썬 깔아서 써도 문송한 내 수준에서는 충분하다. Pandas 패키지를 몇 배가 빠르게 돌릴 수 있는 modin 프로젝트의 ray가 리눅스와 macos에서만 돌아간다. 이것 때문에 맥북을 살 수는 없는 노릇 아닌가… 마. . References . 파이썬 설치 | pip 설치 issue | WSL 연동 | . V2의 경우 올해 4월 쯤 정식으로 서비스될 예정이다. Windows 10 Preview 프로그램에 가입하면 미리 써볼 수 있다. 상세한 내용은 여기를 참고하라. 윈도 코맨드 라인에서 WSL의 버전을 확인하고 싶다면 wsl -l -v을 치면 된다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/coding-tool/python/wsl/2020/03/19/WSL_Cmder.html",
            "relUrl": "/coding-tool/python/wsl/2020/03/19/WSL_Cmder.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Following Gilbert Strang 3",
            "content": "LU Decomposition . 큰 이슈는 없다. | 다만 이 녀석을 G-J 프로세스로 이해하면 흥미로운 구석이 있다. | 무선, 열 바꿈은 없다고 가정하자. | 이 상태에서 적당한 매트릭스 $ mathbf A$를 G-J 프로세스에 따라서 U 매트릭스(상삼각 행렬), 즉 Upper Triangular Matrix로 바꾼다고 해보자. | . E1⋯EkA=UE_1 dotsb E_k { mathbf A} = { mathbf U}E1​⋯Ek​A=U . 이제 $ mathbf A$ 앞에 붙은 오퍼레이터들의 역행렬을 왼쪽으로 곱해보자. . A=Ek−1⋯E1−1⏟∗U{ mathbf A} = underbrace{E_k^{-1} dotsb E_1^{-1}}_{ ast} { mathbf U}A=∗ . Ek−1​⋯E1−1​​​U . $ ast$ 부분이 바로 $ mathbf L$ 매트릭스가 된다! 즉, | . A=LU=LDU′{ mathbf A} = { mathbf L} { mathbf U} = { mathbf L} { mathbf D}{ mathbf U^ prime}A=LU=LDU′ . LU 분해란 매트릭스 $ mathbf A$를 $ mathbf U$로 만드는 과정에서 자연스럽게 등장한다. | . Permutation . 정의상 열바꿈을 수행하는 매트릭스를 뜻한다. | 왜 Permutation이 필요할까? 만일 pivot (대각선에 위치한 행렬)이 너무 작다면, 계산의 관점에서 효율적이지 않다. | 따라서 얘네들은 뒤로 미뤄주는 것이 좋다. 0으로 간주. 여기서는 pivot을 내림차순으로 배치해주는 매트릭스라고 생각해도 좋겠다. | . | . | for Invertible $ mathbf A$, | . PA=LU mathbf{PA = LU}PA=LU . P is identity matrix which reordered rows. 어차피 열만 바꾸는 것이므로 0, 1을 한 열에 하나씩만 갖게 된다! | . | . P−1=PTPTP=I begin{aligned} mathbf P^{-1} &amp; = mathbf P^ mathrm T mathbf P^ mathrm T mathbf P &amp; = mathbf I end{aligned}P−1PTP​=PT=I​ . Transpose . 생략한다. | . Symmetric . AT=A mathbf A^ mathrm T = mathbf AAT=A . $ mathbf R^ mathrm T mathbf R$ is always symmetric. 회귀 계수 구하는 식에서 $( mathbf X^ mathrm T mathbf X)$이 대목이 symmetric! | $( mathbf R^ mathrm T mathbf R)^ mathrm T = mathbf R^ mathrm T mathbf R$ by rules of tranpose. | . | .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/gilbert-strang/2020/03/19/LU-permut.html",
            "relUrl": "/math/matrix-theory/gilbert-strang/2020/03/19/LU-permut.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Following Gilbert Strang 2",
            "content": "Source . 3. Multiplication and Inverse Matrices . 4 Views of Matrix Multiplication . Standard . A⏟m×nB⏟n×p=C underbrace{ mathbf A }_{m times n} underbrace{ mathbf B}_{n times p} = { mathbf C}m×n . A​​n×p . B​​=C . $c_{ij}$는 뭘까? $ mathbf A$의 $i$ 행 벡터와 $ mathbf B$의 $j$ 열 벡터의 곱이다. 즉, | . cij=∑knaikbkjc_{ij} = sum_{k}^n a_{ik} b_{kj}cij​=k∑n​aik​bkj​ . . Combination of Columns of A . [A1…An][b11⋮bn1]=b11A1+…+bn1An=C1[A_1 dotsc A_n] begin{bmatrix} b_{11} vdots b_{n1} end{bmatrix} = b_{11} A_1 + dotsc + b_{n1} A_n = C_1[A1​…An​]⎣⎢⎢⎡​b11​⋮bn1​​⎦⎥⎥⎤​=b11​A1​+…+bn1​An​=C1​ . 즉, 매트릭스 $A$의 열 벡터 $A_i$가 있을 때 이를 B의 각 열 벡터로 선형결합한 것이 $ mathbf C$의 각 열을 구성하게 된다. | . [A1…An][b1i⋮bni]=b1iA1+…+bniAn=col Ci,for i=1,…,p.[A_1 dotsc A_n] begin{bmatrix} b_{1i} vdots b_{ni} end{bmatrix} = b_{1i} A_1 + dotsc + b_{ni} A_n = text{col } C_i, text{for } i = 1, dotsc, p.[A1​…An​]⎣⎢⎢⎡​b1i​⋮bni​​⎦⎥⎥⎤​=b1i​A1​+…+bni​An​=col Ci​,for i=1,…,p. . C=[col C1…col Cp]{ mathbf C} = [ text{col }C_1 dotsc text{col } C_p]C=[col C1​…col Cp​] . . Combination of Rows of B . 위의 해석과 거의 같다. 다만, $A$와 $B$의 역할을 바꾼 형태다. 즉, | . [ai1…ain][B1⋮Bn]=a11B1+…+ainBn=row Ci,for i=1,…,m[a_{i1} dotsc a_{in}] begin{bmatrix} B_{1} vdots B_{n} end{bmatrix} = a_{11} B_1 + dotsc + a_{in} B_n = text{row } C_i, text{for } i = 1, dotsc, m[ai1​…ain​]⎣⎢⎢⎡​B1​⋮Bn​​⎦⎥⎥⎤​=a11​B1​+…+ain​Bn​=row Ci​,for i=1,…,m . C=[row C1⋮row Cm]{ mathbf C} = begin{bmatrix} text{row } C_1 vdots text{row } C_m end{bmatrix}C=⎣⎢⎢⎡​row C1​⋮row Cm​​⎦⎥⎥⎤​ . . Summation of Matices . 가장 급진적인 형태? 흥미롭게 생각해볼 수 있는 것은 각 개별 행렬의 rank다. 각각은 모두 1이다. | . [A1…An][B1⋮Bn]=A1B1⏟(m×1)×(1×p)+⋯+AnBn[A_1 dotsc A_n] begin{bmatrix} B_{1} vdots B_{n} end{bmatrix} = underbrace{A_1 B_1}_{(m times 1) times (1 times p)} + dotsb + A_n B_n[A1​…An​]⎣⎢⎢⎡​B1​⋮Bn​​⎦⎥⎥⎤​=(m×1)×(1×p) . A1​B1​​​+⋯+An​Bn​ . . Block Multiplication . . Inverse of Matrix . 정의상 보면, 정방행렬 $A$에 대해서 역행렬 $A^{-1}$은 | . AA−1=I, also A−1A=I{ mathbf A} { mathbf A}^{-1} = { mathbf I}, text{ also } { mathbf A}^{-1} { mathbf A} = { mathbf I}AA−1=I, also A−1A=I . 역행렬이 존재하는 정방행렬을 non-singluar or invertible matrices라고 부른다. singluar or non-invertible | . | 역행렬을 구하는 과정을 따져보자. | . A[A1−1,…An−1]=I{ mathbf A} [A^{-1}_1, dotsc A^{-1}_n] = { mathbf I}A[A1−1​,…An−1​]=I . $A^{-1}_i$는 역행렬 ${ mathbf A}^{-1}$의 컬럼 벡터 | 역행렬을 구하는 문제는 사실 $n$개의 연립방정식을 푸는 문제와 구조상 동일하다. | . Another definition . ${ mathbf A} x = 0$를 만족하는 $0$ 벡터가 아닌 벡터 $x$가 존재하면 singular matrix. | 증명은 간단하다. $ mathbf A$의 역행렬이 존재하고, $x neq 0$라고 하자. | ${ mathbf A}^{-1} { mathbf A} x = { mathbf A}^{-1} 0 = 0$ | 따라서 ${ mathbf I}x = {0}$이 되고, $x neq 0$와 전제와 모순이다. | . | . Singularity . 만일 $ mathbf A$의 한 열이 모두 0이면 singular - 왜냐하면, 나머지 열을 조합하는 $x$의 원소를 0으로 놓고 해당 열을 조합하는 $x$는 0이 아닌 다른 숫자를 넣으면 $x neq 0$인 ${ mathbf A} x = 0$를 얻을 수 있다. | 만일 $ mathbf A$의 한 열과 다른 열이 스칼라 값을 곱해 구해진다면 singular - 비슷한 논리로 이해할 수 있다. 두 열을 제외한 다른 $x$의 원소를 0으로 두고 해당 두 열을 적절한 수로 곱하면, $x neq 0$인 ${ mathbf A} x = 0$를 얻을 수 있다. | . Using Gauss Jordan For Inverse Matices . $[{ mathbf A} vert { mathbf I}]$와 같은 형태의 augmented matrix를 만든 후, $ mathbf A$를 $ mathbf I$로 만드는 가우스-조르단 프로세스를 반복하면, $ mathbf I$ 자리에 ${ mathbf A}^{-1}$을 얻게 된다. . Example . [A∣I]=[3−24∣100102∣010010∣001][{ mathbf A}|{ mathbf I}] = begin{bmatrix} 3 &amp;-2 &amp; 4 &amp; vert &amp; 1 &amp; 0 &amp; 0 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 end{bmatrix}[A∣I]=⎣⎢⎡​310​−201​420​∣∣∣​100​010​001​⎦⎥⎤​ . 첫번째 행과 두번째 행을 바꾼다. | . [A∣I]∼E~12[A∣I]=[102∣0103−24∣100010∣001][{ mathbf A}|{ mathbf I}] sim { tilde E}_{12} [{ mathbf A}|{ mathbf I}] = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 3 &amp;-2 &amp; 4 &amp; vert &amp; 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 end{bmatrix}[A∣I]∼E~12​[A∣I]=⎣⎢⎡​130​0−21​240​∣∣∣​010​100​001​⎦⎥⎤​ . 두번째 행과 세번째 행을 바꾼다. | . [A∣I]∼E~23[A∣I]=[102∣010010∣0013−24∣100][{ mathbf A}|{ mathbf I}] sim { tilde E_{23}} [{ mathbf A}|{ mathbf I}] = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 3 &amp;-2 &amp; 4 &amp; vert &amp; 1 &amp; 0 &amp; 0 end{bmatrix}[A∣I]∼E~23​[A∣I]=⎣⎢⎡​103​01−2​204​∣∣∣​001​100​010​⎦⎥⎤​ . 첫번째 행에 $-3$을 곱하고 이를 세번째 행과 더한 후 세번째 행에 둔다. | . [A∣I]∼E13[A∣I]=[102∣010010∣0013+(−3)−2+04+(−6)∣1+00+(−3)0+0]=[102∣010010∣0010−2−2∣1−30] begin{aligned} [{ mathbf A}|{ mathbf I}] sim E_{13} [{ mathbf A}|{ mathbf I}] &amp; = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 3+(-3) &amp;-2 + 0 &amp; 4 + (-6)&amp; vert &amp; 1 + 0 &amp; 0 + (-3) &amp; 0 + 0 end{bmatrix} &amp; = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp;-2 &amp; -2 &amp; vert &amp; 1 &amp; -3 &amp; 0 end{bmatrix} end{aligned}[A∣I]∼E13​[A∣I]​=⎣⎢⎡​103+(−3)​01−2+0​204+(−6)​∣∣∣​001+0​100+(−3)​010+0​⎦⎥⎤​=⎣⎢⎡​100​01−2​20−2​∣∣∣​001​10−3​010​⎦⎥⎤​​ . 두번째 행에 2를 곱한 후 이를 세번째 행에 더하여 세번째 행에 둔다. | . [A∣I]∼E23[A∣I]=[102∣010010∣0010+0−2+2−2+0∣1+0−3+00+2]=[102∣010010∣00100−2∣1−32] begin{aligned} [{ mathbf A}|{ mathbf I}] sim E_{23} [{ mathbf A}|{ mathbf I}] &amp; = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 + 0 &amp;-2 + 2 &amp; -2 + 0&amp; vert &amp; 1 + 0 &amp; -3 + 0 &amp; 0 + 2 end{bmatrix} &amp; = begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; vert &amp; 0 &amp; 1 &amp; 0 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp; 0 &amp; -2 &amp; vert &amp; 1 &amp; -3 &amp; 2 end{bmatrix} end{aligned}[A∣I]∼E23​[A∣I]​=⎣⎢⎡​100+0​01−2+2​20−2+0​∣∣∣​001+0​10−3+0​010+2​⎦⎥⎤​=⎣⎢⎡​100​010​20−2​∣∣∣​001​10−3​012​⎦⎥⎤​​ . 첫번째 행과 세번째 행을 더한 후 이를 첫번째 행에 둔다. | . [A∣I]∼E31[A∣I]=[1+00+02+(−2)∣0+11+(−3)0+2010∣00100−2∣1−32]=[100∣1−22010∣00100−2∣1−32] begin{aligned} [{ mathbf A}|{ mathbf I}] sim E_{31} [{ mathbf A}|{ mathbf I}] &amp; = begin{bmatrix} 1 + 0 &amp; 0 + 0 &amp; 2 + (-2) &amp; vert &amp; 0 + 1 &amp; 1 + (-3) &amp; 0 + 2 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp; 0 &amp; -2 &amp; vert &amp; 1 &amp; -3 &amp; 2 end{bmatrix} &amp; = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; vert &amp; 1 &amp; -2 &amp; 2 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp; 0 &amp; -2 &amp; vert &amp; 1 &amp; -3 &amp; 2 end{bmatrix} end{aligned}[A∣I]∼E31​[A∣I]​=⎣⎢⎡​1+000​0+010​2+(−2)0−2​∣∣∣​0+101​1+(−3)0−3​0+212​⎦⎥⎤​=⎣⎢⎡​100​010​00−2​∣∣∣​101​−20−3​212​⎦⎥⎤​​ . 세번째 행에 $-1/2$을 곱한다. | . [A∣I]∼E3[A∣I]=[100∣1−22010∣00100−2∗(−12)∣12−3∗(−12)−22]=[100∣1−22010∣001001∣−12321]=[I∣A−1] begin{aligned} [{ mathbf A}|{ mathbf I}] sim E_{3} [{ mathbf A}|{ mathbf I}] &amp; = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; vert &amp; 1 &amp; -2 &amp; 2 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp; 0 &amp; -2 * (- frac{1}{2}) &amp; vert &amp; frac{1}{2} &amp; -3*(- frac{1}{2}) &amp; - frac{2}{2} end{bmatrix} &amp; = begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; vert &amp; 1 &amp; -2 &amp; 2 0 &amp; 1 &amp; 0 &amp; vert &amp; 0 &amp; 0 &amp; 1 0 &amp; 0 &amp; 1 &amp; vert &amp; - frac{1}{2} &amp; frac{3}{2} &amp; 1 end{bmatrix} &amp; = [{ mathbf I} vert { mathbf A}^{-1}] end{aligned}[A∣I]∼E3​[A∣I]​=⎣⎢⎡​100​010​00−2∗(−21​)​∣∣∣​1021​​−20−3∗(−21​)​21−22​​⎦⎥⎤​=⎣⎢⎡​100​010​001​∣∣∣​10−21​​−2023​​211​⎦⎥⎤​=[I∣A−1]​ . 이 식을 얻기 위한 가우스-조르단 프로세스는 $E_3 E_{31} E_{23} E_{13} { tilde E_{23}} { tilde E}_{12}$으로 나타낼 수 있다. | 이 프로세스의 곱, 즉 위에 적은 것이 역행렬 ${ mathbf A}^{-1}$이다. | .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/gilbert-strang/2020/03/18/multiplication-inverse.html",
            "relUrl": "/math/matrix-theory/gilbert-strang/2020/03/18/multiplication-inverse.html",
            "date": " • Mar 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Vanilla Python with Virtual ENV",
            "content": "Why . 아나콘다 사가 개발한 conda는 무척 훌륭한 도구다. 하지만 바닐라, 이른바 순정에 로망이 있지 않을까? 단지 로망만은 아니다. 때로는 여러가지 미묘한 의존성 때문에 순정을 이용해야 할 때가 있다. 이번 포스트에서는 python을 수정으로 설치한 후 가상 환경을 이용하는 방법을 간단하게 소개하겠다.1 . Assumption . 당신은 윈도 이용자다. | |this-is-desc|와 같이 표현된 것에서 “this-is-desc”는 각자의 컴퓨팅 환경을 뜻한다. 적당하게 이해하시라. | git bash와 같이 bash 환경을 쓸 경우 슬래시(/)를, 그냥 윈도 커맨드를 쓸 경우 역슬래시( )를 써야 한다. 아래에서는 git bash 환경을 기본으로 두겠다. | . Install . https://www.python.org/ . 취향대로 필요대로 깔도록 하자. 통상적인 윈도 환경이라면 64비트 설치하자. 보통 디폴트로 깔면 32비트가 깔린다. | . Virtual Environment . 가상 환경을 설정해보자. 보통 conda, virtualenv 같은 패키지를 써왔을 것이다. | python 3.3 이후 버전부터 venv가 python 안에 포함되었다. | python 가상 환경을 만들기 전에 일단 작업 중인 가상 환경을 모아둘 특정 디렉토리(폴더)를 지정하자. | . How to make venv . 내 경우 적당한 디렉토리에 .pyenv 라는 디렉토리를 만들었다. | 해당 디렉토리 안에서 가상 환경을 생성한다. | . $ |your-directory|/python.exe -m venv |your-environmet| . 가상 환경으로 진입한다. | . $ |your-python-directory|/Scripts/activate . Powershell을 쓴다면, Activate.ps1을 실행하면 된다. | 이제 실행창 앞에 (|name-of-your-environmet|)이 떠 있는 것을 볼 수 있다. | . Update your venv . 가상 환경을 만드는 이유는 무엇인가? 특정한 환경에서 안정적인 작업환경을 만들기 위해서다. 또한 뭔가 문제가 생겼을 때 가상 환경만 지우면 그만이다. | 가상 환경은 이제 막 설치된 상태이므로 반드시 필요한 업데이트들을 해줘야 한다. pip | 사용할 패키지 | . | pip 업데이트다. python 이용자라면 pip 업데이트를 굳이 설명할 필요가 없겠지! | . | . $ python -m pip install --upgrade pip . 반드시 깔 필요는 없지만 conda에 준하게 패키지 관리를 하고 싶다면, 아래 패키지를 깔자. | . $ pip3 install pip-review # pip-review 설치 $ py -3 -m pip_review --local --interactive # 설치된 패키지 업데이트 체크 . How to choose python version . 가상 환경에서 python 자체를 선택하고 싶다면 어떻게 해야 할까? | 아래 명령처럼 하면 된다. 원하는 버전에 python을 직접 호출해서 가상 환경을 만든다. | . $ |your-directory|&gt; C: Python34 python.exe -m venv |your-venv| . With VS Code . VS code에서 해당 가상 환경을 어떻게 인식시킬 수 있을까? | VS Code의 메뉴에서 File → Preference → settings | “python:Pythonpath”로 검색한 후 아래를 브라우징하다보면, “Venv path”라는 항목이 나온다. 만일 conda와 같은 다른 가상 환경을 쓴다면 해당 항목에 적당한 디렉토리를 넣으면 되겠다. | 가상 환경이 이름이 .pyvenv라면 적당한 디렉토리와 함께 아래와 같이 심어주면 되겠다. | . | . . Trouble Shooting . For git bash . 원도우 환경에 git bash를 깔아서 쓰고 있다면 몇가지 곤란한 점이 있다. 우선 python 명령어가 먹지 않는다. | 가상 환경 활성화를 할 수 없다. | . | 이 문제는 쉽게 해결이 가능하다. | . $ winpty python.exe Python 3.7.6 (default, Oct 2 2018, 09:18:58) Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; . 원하는 버전의 python을 실행하려면 이를 응용하면 되겠다. | . $ winpty |your-python-directory|/python.exe . 매번 이렇게 실행할 수 없으니 git bash 환경에 넣어두자. | . $ echo &quot;alias python=&#39;winpty python.exe&#39;&quot; &gt;&gt; ~/.bashrc . Directory changed . 디렉토리를 만들면 경로나 이름을 바꾸지 않는 게 좋다. 바꾸느냐 차라리 그냥 새로 만드는 게 낫다. 꼭 바꿔야 한다면, 아래를 바꾸면 된다. activate.bat, activate.ps1 파일을 열고 path 부분을 바꿔주어야 한다. | . | . conda가 여러모로 편하다. 하지만 conda를 써야겠다면, conda 전체를 깔지 말고 minconda를 써보시기를 권한다. conda는 쓸 데 없는 것을 너무 많이 깔아버리거든… &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/coding-tool/python/2020/03/17/vanilla-python.html",
            "relUrl": "/coding-tool/python/2020/03/17/vanilla-python.html",
            "date": " • Mar 17, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Following Gilbert Strang 1",
            "content": "What . 선형 대수의 대 마왕 길버트 스트랭 님의 강의를 틈틈이 다시 듣고 있다. | 개인적으로 정리하는 내용이므로 많이 불친절하니 알아서들 보시라. 마. 스트랭 선생님의 원래 강의 취지와 다른 경우도 종종 있을 것이다… | . | . Source . 2. Elimination with Matrices . How To View Matrix . 일단 매트릭스를 이해하는 방법부터 뜯어 고치자. | 계속 강조하듯이, 매트릭스는 일종의 함수다. 그런데 인풋을 왼쪽에도 넣을 수 있고, 오른쪽에도 넣을 수 있다. | . | . Right Operation . $A in { mathbb R}^{m times n}$, $x in { mathbb R}^{n times 1}$ . Ax=[c1…cn][x1⋮xn]=c1x1+⋯+cnxnA x = begin{bmatrix} c_1 &amp; dotsc &amp; c_n end{bmatrix} begin{bmatrix} x_1 vdots x_n end{bmatrix} = c_1 x_1 + dotsb + c_n x_nAx=[c1​​…​cn​​]⎣⎢⎢⎡​x1​⋮xn​​⎦⎥⎥⎤​=c1​x1​+⋯+cn​xn​ . 이 표기식은 $A$의 컬럼 벡터가 $x$의 원소들에 의해 선형결합되는 것임을 잘 보여준다. . Left Operation . $A in { mathbb R}^{m times n}$, $x in { mathbb R}^{1 times m}$ . xA=[x1…xm][r1⋮rm]=x1r1+⋯+xmrmx A = begin{bmatrix} x_1 &amp; dotsc &amp; x_m end{bmatrix} begin{bmatrix} r_1 vdots r_m end{bmatrix} = x_1 r_1 + dotsb + x_m r_mxA=[x1​​…​xm​​]⎣⎢⎢⎡​r1​⋮rm​​⎦⎥⎥⎤​=x1​r1​+⋯+xm​rm​ . 이 표기식은 $A$의 로우 벡터가 $x$의 원소들에 의해 선형결합된다는 것을 보여준다. . Matrix Operation . 일단 이해를 돕기 위해서 $3 times 3$ 매트릭스를 예로 들겠다. . T[121381041]⏟A=[12102−2041]⏟BT underbrace{ begin{bmatrix} 1 &amp; 2 &amp; 1 3 &amp; 8 &amp; 1 0 &amp; 4 &amp; 1 end{bmatrix}}_{A} = underbrace{ begin{bmatrix} 1 &amp; 2 &amp; 1 0 &amp; 2 &amp; -2 0 &amp; 4 &amp; 1 end{bmatrix}}_{B}TA . ⎣⎢⎡​130​284​111​⎦⎥⎤​​​=B . ⎣⎢⎡​100​224​1−21​⎦⎥⎤​​​ . 이때 변형 매트릭스 $T$를 어떻게 찾을 수 있을까? 먼저 $T$는 매트릭스 $A$의 왼쪽에 있다. 따라서 매트릭스 $T$의 로우 벡터들은 각기 $A$의 로우 오퍼레이션을 수행한다. 즉, 매트릭스 $B$의 첫번째 로우는 $A$와 같다. 따라서, $A$의 첫번째 행에는 $[1~0~0]$이 들어간다. 같은 논리로 $T$ 를 찾으면 다음과 같다. . T=[100−310001]T = begin{bmatrix} 1 &amp; 0 &amp; 0 -3 &amp; 1 &amp; 0 0 &amp; 0 &amp; 1 end{bmatrix}T=⎣⎢⎡​1−30​010​001​⎦⎥⎤​ . $T$는 무엇을 의미하는가? $B$의 경우 첫번째 로우와 세번째 로우는 $A$와 동일하다. 그리고 두번째 로우의 경우 첫번째 로우에 $-3$을 곱한 후 이를 두 번째 로우와 더한 것이다. 식은 이를 그대로 나타낸다. $T$의 경우 1열과 2열만 결합된다는 의미에서 $E_{12}$로 표기하기도 한다. . 그리고 이는 사실 사다리꼴 행렬 혹은 기약 사다리꼴 행렬을 만드는 과정이다. 자세한 것은 이 포스트를 참고하라. . Permutation matrix . 만일 $2 times 2$ 매트릭스에서 1행과 2행을 바꾸는 작업은 어떻게 수행할까? . [0110][abcd]=[cdab] begin{bmatrix} 0 &amp; 1 1 &amp; 0 end{bmatrix} begin{bmatrix} a &amp; b c &amp; d end{bmatrix} = begin{bmatrix} c &amp; d a &amp; b end{bmatrix}[01​10​][ac​bd​]=[ca​db​] . 이렇게 하면 된다. 만일 1열과 2열을 바꾸고 싶다면? 열을 조작하는 작업이니 이번에는 매트릭스가 $A$의 오른쪽에 와야 한다. . [abcd][0110]=[badb] begin{bmatrix} a &amp; b c &amp; d end{bmatrix} begin{bmatrix} 0 &amp; 1 1 &amp; 0 end{bmatrix} = begin{bmatrix} b &amp; a d &amp; b end{bmatrix}[ac​bd​][01​10​]=[bd​ab​] .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/gilbert-strang/2020/03/16/GS-Matrix-Elimination.html",
            "relUrl": "/math/matrix-theory/gilbert-strang/2020/03/16/GS-Matrix-Elimination.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Streamlit + Heroku",
            "content": "들어가며 . 구슬이 서 말이라도 꿰어야 보배다. 당연하다. 데이터 분석은 . 스마트한 노가다 | 갈고 닦은 모델링 | 환상적인 시각화 | . 의 삼위 일체다. 물론 이 모든 과정에 화룡점정은 “전달”이다. 이 세 개를 어떻게 묶어서 전달하느냐에 따라서 분석을 받아들이는 그 사람의 판단이 달라진다. 어떻게 하면 잘 전달하는 것일까? . 지구상의 많은 인구가 손 안에 컴퓨터를 하나씩 들고 다니는 시대다. 안타깝게도 PC에서 많이 쓰던 PDF는 모바일에 적절하지 않다. 문서의 확대, 축소가 동적이지 않기 때문이다. 아울러 새로운 정보를 계속 주고 받을 수 있다는 점을 생각하면 결국은 ‘웹’, html이 데이터 분석을 전달하는 새로운 합의점이 되지 않을까 싶다. 아니 이미 그런 시대다. . Streamlit . Python은 이제 자타가 공인하는 데이터 사이언스의 주요 도구다. Python에는 이미 좋은 웹 개발 도구들이 있다. Django, Flask가 그렇다. 하지만 역시 나 같은 문송한 자들이 쓰기는 쉽지 않다. 아마도 네트워크 관련 지식이나 웹 관련 지식에 익숙하지 않은 탓이다. 그냥 pandas, numpy 정도만 배울 용이가 있는 사람들에게는 넘기 쉽지 않은 장벽이다. 어쩌면 닭잡고 싶은데 소잡는 칼을 휘두르는 법을 배워야 해서 그럴지도 모르겠다. . 이런 웹 개발 프레임워크들은 “데이터 혹은 그 시각화를 인터랙티브하게 공유한다,”라는 기본 목적에 비추어보면 살짝 과한 면도 있다. 비슷한 필요를 느낀 사람들이 많았는지 작년에 Streamlit라는 반가운 프로젝트가 등장했다. . R에는 이미 Shiny라는 비슷한 프로젝트가 있다. R의 기능을 활용해 데이터를 처리하고 별도의 웹 개발 없이 R의 기능을 그대로 시각화로 구현할 수 있는 툴이다. Python에도 비슷한 기능을 구현하는 시도는 많았으나 어딘가 불편했다. Streamlit는 이런 2%의 부족함을 꽤 잘 해결했다. 어떤 면에서는 Shiny보다도 쓰기가 편리한 듯도 싶다. 다만 잃는 것도 있다. Shiny의 ‘미감’에는 미치지 못하고 기능도 살짝 부족하다. 하지만 통상적인 데이터 탐색 및 시각화의 용도로는 충분하다. . Workflow . 여기서 Streamlit를 자세히 설명하지는 않겠다. Python을 써보면 사람이면 예제를 쉽게 따라해보면 된다. 페이지의 설명대로 아래 명령만 실행해봐도 느낌을 한번에 받을 수 있다. 아래와 같이 설치하고 hello를 띄울 수 있다. . $ pip install streamlit $ steamlit hello . 대략의 워크플로우는 아래와 같다. . Python으로 작업한다. | Streamlit로 interactive하게 처리될 부분을 간단히 설계한다. | 터미널에서 streamlit run [앱이름].py | 각자의 컴퓨터, 즉 로컬에서 실행하면 알아서 브라우저 앱이 뜬다. 디자인이 다소 허접해보일 수 있지만, 미니멀을 좋아하는 사람이라면 ‘본질’에 집중할 수 있는 점을 칭찬할 것이다. R의 Shiny와 마찬가지로 어지간한 Python 비주얼 라이브러리(matplotlib, Vega, Plotly 등등)는 모두 구현이 가능하다. . How to deploy to web… . 로컬에서 Steamlit를 이용해 그럴 듯한 앱을 만들었다. 이제 이 녀석을 어떻게 배포할까? 즉 이걸 남들에게 어떻게 보여줄까? . 일단 배포를 위해서는 전용 웹 서비스가 있어야 한다. AWS의 무료 티어를 활용해서 배포할 수도 있지만 세팅이 간단하지는 않다. 솔직히 말하면 이 마저 귀찮고 살짝 겁난다. 이런 사람들이 쉽게 쓸 수 있는 서비스가 salesforce에서 제공하는 Heroku다. 원래 Ruby로 제작된 웹 앱만 서비스 했으나, 최근 쓰임새가 늘어나면서 다양한 랭귀지를 지원하고 있다. 당연히 Python도 포함된다. 작업의 전체적인 개념은 다음과 같다. . Heroku 계정 생성 | CLI 설치 | Streamlit 앱 제작 | Heroku 빌드를 위한 파일 생성 | Streamlit 작업 디렉토리를 Heroku로 push | Push와 동시에 웹 빌드 및 자동 배포 | 필요한 과정을 간략하게 살펴보자. . Heroku 계정 생성 및 CLI 설치 . 홈페이지에 들어가서 가입하면 된다. CLI, 즉 터미널 툴은 왜 필요할까? 터미널에서 작업을 하면 좀 더 편하다. CLI를 설치해야 제대로 Heroku를 사용할 수 있다. 아래와 같이 터미널에서 로그인을 한다. . $ heroku login . 앱 디렉토리에 깃 생성 . git을 통해 heroku를 관리하는 게 편하다. git을 쓸 줄 모른다면, 야매로라도 얼른 배우고 오시라. git으로 작업을 브랜치에서 commit 한 후 heroku로 푸시하는 것으로 일종의 작은 CI/CD가 달성된다. . 빌드를 위한 파일 생성 . Heroku로 제작한 Streamlit 앱을 푸시하면 자동으로 빌드가 진행된다. 이를 위해서 제작한 앱의 최상위 디렉토리에 아래의 파일을 넣어주자. . project ├── app.py (제작한 앱) ├── requirements.txt ├── setup.sh └── Procfile . requirements.txt . 설치해야 하는 Python 패키지를 지정해준다. 아래의 예와 같다. . numpy==1.16.4 streamlit==0.52.1 seaborn==0.9.0 pandas==0.25.1 matplotlib==3.1.1 scikit_learn==0.22 . 이 파일을 자동으로 생성하는 pipreqs를 이용해도 된다. . setup.sh . mkdir -p ~/.streamlit/ echo &quot; [general] n email = &quot;your-email@domain.com &quot; n &quot; &gt; ~/.streamlit/credentials.toml echo &quot; [server] n headless = true n enableCORS=false n port = $PORT n &quot; &gt; ~/.streamlit/config.toml . sh 명령을 실행할 내용을 담고 있다. . Procfile . web: sh setup.sh &amp;&amp; streamlit run app.py . sh 명령을 실행하고 streamlit를 띄우는 명령어를 담고 있다. . runtime.txt . 특정 Python 버전이 필요하다면 이 파일을 추가할 수 있다. 파일 안에 담긴 내용은 아래와 같다. . python-3.7.3 . Create and Just push . 이제 heroku로 나갈 앱을 만들고 보내면 끝이다. master에서 작업했다면 아래와 같이 명령어를 실행하면 된다. . $ heroku create $ git push heroku master . Protips . 한글이 안나와요~ . streamlit는 웹에서 즉 브라우저 위에서 돌아가기 때문에 한글이 잘 나오는 것이 정상이다. 문제는 Python 내에서 그래프에 한글을 렌더링할 경우 이것이 제대로 표현되지 않는다는 것이다. 보통 matplotlib과 같은 패키지들을 써본 사람이라면 해당 내용을 한번 검색해본 기억이 있을 것이다. 대략 로컬에서는 폰트를 찾은 후 이를 matplotlib이 쓸 수 있도록 인식시켜주는 것으로 간단히 끝난다. . 왜 Heroku에서는 이것이 안될까? 간단하다. Heroku는 외부의 웹서버이고 그 웹 서버에 한글 폰트가 없는 것이다. 웹 서버에 한글 폰트를 심어주면 될 것이다. 하지만 어떻게 심을까? 간단하다. 작업 중인 앱의 루트 디렉토리에 다음과 같은 시스템 디렉토리를 하나 생성한다. . /.fonts . 이 디렉토리 안에 필요한 한글 폰트를 넣으면 된다. 그러면 해당 앱이 Heroku로 푸시되었을 때 시스템 폰트로 디렉토리 안의 폰트들이 잘 등록된다. . 예제 . https://morning-depths-10545.herokuapp.com/ . 앱 안에 비교적 자세한 설명을 달아 두었으니 별도의 과정은 생략하도록 하겠다. | . 데이터 정리 등의 지저분한 코드는 생략하고 위 Heroku 앱을 구현하는 데 동원된 코드는 다음과 같다. . https://github.com/anarinsk/adp-st-kap_1 . 참고자료 . https://github.com/Taxuspt/heroku_streamlit_nginx . https://towardsdatascience.com/quickly-build-and-deploy-an-application-with-streamlit-988ca08c7e83 . https://blog.jcharistech.com/2019/10/24/how-to-deploy-your-streamlit-apps-to-heroku/ .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/coding-tool/python/web-tool/2020/03/09/Streamlit-Heroku.html",
            "relUrl": "/coding-tool/python/web-tool/2020/03/09/Streamlit-Heroku.html",
            "date": " • Mar 9, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Free Your Blog with Github",
            "content": "tl;dr . 깃헙에서 가장 쉽게 블로그를 빌드할 수 있는 방법을 알아보자. | 디자인은 포기하시라, 마. (지금으로도 사실 훌륭하니까!) | . Blogging with Github . 깃헙을 통해 static web 기반의 블로그 / 홈페이지를 운영할 수 있다. 많은 분들이 이미 그렇게 하고 있다. Hugo, Jekyll의 도구를 잘 사용하면 상용 수준의 블로그 운영도 가능하다. 물론 이 도구들을 쓸 경우 관리를 위해서는 약간의 번거로움을 감수해야 한다. Hugo의 경우 R의 BlogDown 패키지를 활용할 수 있다. Jekyll의 경우 Ruby 기반으로 제작되어서 블로그 관리를 위해서는 local에서 해줘야 하는 작업이 있다.1 . 이상적인 형태의 블로그 툴이란 무엇일까? 내 생각은 이렇다. . 최초의 설정 및 초기 디자인 요소를 제외하면 코드를 수정할 필요가 없어야 한다. | html을 되도록 쓰지 않고, md 형태로 작성한 후 바로 반영이 되어야 한다. | 무료로 제한 없이 호스팅이 가능해야 한다. | 3은 깃헙으로 가볍게 해결된다. 1, 2는 사실 좀 어려운 부분이었다. 깃헙을 블로그 툴로 쓰기 위한 일종의 트레이드오프랄까… 2 . 이 글에서 소개할 fastpages는 깃헙의 기본 웹툴인 Jekyll을 기반으로 1,2를 구현해주는 이상적인 서비스다. 게다가 fastpages는 ipynb 형태의 노트북, word 파일도 알아서 블로그 페이지로 바꿔준다. 문송한 나로서는 참으로 반가운 서비스다. . Before we go . 이하의 글에서 [your-x-id]라고 표기된 부분은 x라는 서비스의 각자 계정명 등의 정보를 바꿔서 설정해야 하는 부분이다. 그냥 복붙하지 마시라는 말씀. | [repo root]는 해당 리퍼지토리의 최상위 디렉토리를 나타낸다. | . fastpages by fast.ai . 일단 별도의 인스톨 과정이 필요 없다! 사전에 준비해야 할 것은 깃헙 계정 뿐. . 페이지 생성 . https://github.com/fastai/fastpages/generate . . 깃헙에 로그인한 상태에서 이 곳에 접속해 페이지를 생성한다. 페이지를 생성한 후 내 깃헙 계정에서 조금 기다리면 “Pull Request”(PR) 메시지가 날아온다. . . PR . PR을 하기 전에 SSH 키를 생성하는 작업을 해줘야 한다. 메시지에 친절하게 설명이 되어 있으니 그대로 따라하면 된다. . . 링크를 눌러 웹 페이지에서 private key와 public 키를 생성한다. RSA, 4069를 선택하도록 하다. | 두번째의 링크를 눌러 새 sceret을 생성하고 private key를 넣어준다. 이름은 반드시 SSH_DEPLOY_KEY로 해야 한다. | 세번째의 링크를 눌러 deploy key를 생성하고 여기에 public key를 넣어주면 된다. 이름은 임의로 지정하면 된다. | 마지막으로 PR를 수락하고 이를 아래 화면 같이 merge하면 준비가 완료된다. 깃헙이 작업을 하는 동안 잠시 기다리면 된다. 작업 상태가 궁금하면 github actions 탭을 확인하면 된다. 이후 몇 가지를 더 묻는데 대충 진행해도 문제 없다. . . 설치가 끝나면 아래와 같이 생성된 페이지를 확인할 수 있다. 아래는 위의 절차에 따라서 테스트계정(test-fastpages)에 페이지를 생성한 결과다. . https://anarinsk.github.io/test-fastpages/ . Customize yours . 이제 각자의 취향에 맞게 몇 가지 커스터마이즈를 거치면 된다. 대체로 md를 고치면 되기 때문에 크게 어려운 대목은 없다. . home repo root의 index.md를 적당히 수정하면 된다. | . | about [repo root]/_pages/about.md 를 수정하면 된다. | . | comments utterances 앱을 깔면, github의 issue 기능을 활용해 코멘트를 관리할 수 있다. | 인스톨 페이지의 안내대로 하면 된다. | 별도로 뭘 까는 것은 아니고 github 계정의 해당 repo를 설정하면 끝. | 다만 코멘트를 달기 위해서는 github 계정이 필요하다. | . | favicon [repo root]/images의 favicon.ico를 고치면 된다. | . | 그림 관리 [repo root]/images에 적절한 디렉토리를 만들어 관리하면 된다. | 그림 링크는 ![](/lostineconomics-v2-1/images/git-cmder/fig_3.png) 식으로 /lostineconomics-v2-1로 시작하면 된다. | . | css css를 다룰 줄 안다면 블로그의 많은 것은 커스터마이즈할 수 있다. | [repo root]/_sass/minima안에 보면 custom-styles.scss와 fastpages-styles.scss 두 개가 있다. custom-styles.scss에 부가 내용을 수정하면 스타일을 바꿀 수 있다. 필요한 폰트를 로딩해 설정한다든가 하는 디자인 수정 사항을 여기에 넣으면 된다. | 예를 들어 기본 텍스트의 크기를 바꾸고 싶다면, | . | . .post-content p, .post-content li { font-size: 15px; //원래값은20px color: #515151; } . google analytics 먼저 google analytics id가 필요하다. 알아서 발급 받으시라. | 구글 페이지에도 안내가 되어 있지만, gs 모듈을 활용하기 위해서 [repo root]/_include에 google_analytics.html과 같은 파일을 만들어 아래의 내용을 넣는다. 물론 자신의 analytics id로 바꿔 넣어야 한다. | . | . &lt;!-- Global site tag (gtag.js) - Google Analytics --&gt; &lt;script async src=&quot;https://www.googletagmanager.com/gtag/js?id=[your-ga-id]&quot;&gt;&lt;/script&gt; &lt;script&gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(&#39;js&#39;, new Date()); gtag(&#39;config&#39;, &#39;[your-ga-id]&#39;); gtag(&#39;set&#39;, {&#39;user_id&#39;: &#39;USER_ID&#39;}); // 로그인한 User-ID를 사용하여 User-ID를 설정합니다. &lt;/script&gt; . [repo root]/_include의 custom_head.html의 적당한 줄에 아래 코드를 넣어준다. | google analytics의 ‘실시간’ 항목에서 작동 여부를 확인할 수 있다. | . 활용 . Post 작성 . 통상적인 md 파일로 작성하면 된다. 두 가지만 주의하면 된다. 우선 파일 이름을 년도-월-일-이름.md 형태로 넣어야 한다. 그래야 fastpages가 컴파일을 할 수 있다. 예를 들어 이 포스팅의 파일 이름은 2020-03-07-blogging-with-fastpages.md다. . 둘째, 포스팅 만 앞에 yml을 넣어 해당 포스트에 관한 정보를 지정해야 한다. . layout: post # 글의 레이아웃, 보통이라면 post로 두면 된다. toc: false # 목차 출력 여부 comments: true # 코멘트 기능 사용 여부 title: Free Your Blog with Github # 제목 description: github에서 무료 블로그를! # 제목 아래 부제목 categories: [coding-tool, web-tool] # tag 혹은 카테고리 . 으로 감싼 후 안에 정보를 넣으면 된다. . fastpage의 몇가지 장점 . 보통 github 기반의 블로그를 만들면, [your-git-id].github.io만을 주소로 가지게 된다. fastpages를 쓰면 repo 수준의 홈페이지를 운영할 수 있다. 예를 들어, 이 블로그의 주소는 anarinsk.github.io/lostineconomics-v2-1다. | 강력한 장점은 ipynb 확장자의 노트북 파일을 그대로 포스팅으로 바꿔준다는 것이다. [repo root]/_notebooks에 파일을 넣어주면 된다. | 디자인은 포기하라. css나 html을 잘 안다면 커스터마이즈할 여지가 있지만, 그럴 수 있는 사람이라면 Hugo나 Jekyll을 직접 쓰는 편이 나을 수도 있겠다. fastpages가 사용하는 Jekyll 테마는 minima다. | . | 매번 markdown을 에디터에 올려쓰는 것이 불편하다면 웹 에디터를 활용할 수 있다. stackedit의 경우 github 저장을 지원하기 때문에 해당 repo의 _posts/ 아래의 md 문서를 동기화해두면 웹에서 수정 후 동기화하는 것만으로도 포스트의 수정을 쉽게 할 수 있다. stackedit의 활용에 관해서는 이 포스팅을 참고하면 좋겠다. 사실 웹에서 이렇게 수준 높은 마크다운 에디터를 제공한다는 사실이 고맙고 놀랍다. (그래서 나는 이 회사에 매년 기부한다!) | fastpages의 경우 commit이 발생하면 자동으로 블로그의 빌드에 들어간다. | . | 따라서 웹 에디터에서 글을 수정한 후 적절한 주소를 지정해주고 동기화를 하면, 즉 커밋을 하면 바뀐 내용을 반영해 블로그가 다시 빌드 된다. | . | . . Update는 어떻게 하지? . 블로그 자체의 Jekyll 기반 코드들도 업데이트될텐데, 이건 어쩌나 생각하셨다면 걱정할 필요가 없다. 깃헙에서 제공하는 PR 기능을 활용해서 내 리포지토리까지 거의 자동으로 업데이트해준다. 다만 커스터마이즈를 많이 했다면 지원이 안될 수도 있다는 점에 유념하시라. - issues &gt; New issue &gt; Get started를 누르면 아래의 그림을 볼 수 있다. .   . 여기서 아무 것도 바꾸지 말고 Submit new issue를 하면 업데이트 요청이 들어가고 업데이트 사항이 없을 경우는 이슈가 그낭 종결되고 있을 경우는 그대로 따라가면 된다. 처음 페이지를 깔 때처럼 PR을 해주는 형태로 진행된다. 꽤나 영리한 업데이트 방법이다. . 바뀌는 파일들 . 아직까지는 사소한 업데이트만 진행되는 듯 싶다. PR을 완료한 이후 커밋 메시지에 “upgrade fastpages”라고 적혀 있는 부분을 확인해서 커스터마이즈한 부분을 적당히 바꾸면 된다. _pages/about.md, |includes/custom_head.html 와 같이 커스터마이즈해둔 파일들도 바뀌게 되는 경우가 종종 있으므로 항상 백업본을 어딘가 로컬에 유지해두는 편이 좋겠다. . 자세한 내용은 여기을 참고하라. 미리 말하면 fastpages는 이런 로컬의 작업을 깃헙 서버에게 대신 시키고 완성된 html은 바로 깃헙 페이지(깃헙의 무료 웹호스팅 서비스)로 보여주는 방식으로 돌아간다. 참으로 은혜로운 서비스다. &#8617; . | 이 블로그의 v1을 html로 만든 이유이기도 하다. v1은 html까지 완성한 후 이를 깃헙이 호스팅하도록 하는 원시적인 방법을 썼다. 이에 관해서는 이 포스팅을 참고하라. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/coding-tool/web-tool/2020/03/07/blogging-with-fastpages.html",
            "relUrl": "/coding-tool/web-tool/2020/03/07/blogging-with-fastpages.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Using Git ‘Smoothly’ in Windows 10",
            "content": "Why . 단언컨대 git은 곧 문과생도 알아야 할 필수 도구가 될 것이다. 버전 관리가 절실해지는 그리고 코딩이 학제를 넘어선 필수처럼 인정되는, 그런 세상에서 컴퓨터를 끼고 있으면서 git의 사용을 피하기는 힘들 듯 싶다. git에 관한 포스트는 아니니 여기까지 하자. . macos나 linux 계열을 쓸 때는 git에 관해서 크게 고민하지 않아도 된다. 터미널 켜고, CLI(Command Line Interface)로 쓰면 그만이다. 이렇게 쓰는 것이 윈도에서는 쉽지 않다. 물론 윈도에서 WSL로 리눅스를 지원하기는 한다. 하지만 그냥 메인 os 위에 얹어 쓰는 게 편할 때가 있다. . What . 우리는 아래 두 개의 소프트웨어를 쓸 것이다. . Git for Windows | Cmder | . Git for Windows . 웬만하면 64비트로 깔자. 그리고 중간에 여러 가지 선택지에 관해서 질문한다. 커맨드라인은 어떤 것으로 쓸 것인지 등등. 원래 git이 linux에 뿌리를 두고 있는 만큼 윈도의 디폴트 명령창보다는 Bash를 쓰는 편이 좋다. ls 등의 명령어를 자유롭게 쓸 수 있어서 꽤 편하다. 윈도용 git은 특화된 git-bash를 제공하니, 이 녀석을 쓰면 좋을 듯 싶다. . Cmder . “커맨더”라고 읽으면 된다. git은 이미 깔았으므로 쓰면 된다. 다만 좀 더 편리한 환경에서 CLI를 쓰기 위해서 Cmder를 깔아보도록 하자. 이 녀석은 별도의 인스톨 없이 쓸 수 있다. git을 포함한 무거운 버전과 git이 빠진 미니 버전이 있다. 어차피 우리는 별도로 깃을 깔았으니, 미니 버전을 다운받으면 되겠다. 다운 받은 후 아무 위치에나 두어도 된다. 어차피 exe 파일만 사용할 것이기 때문이다. 다만, 관리가 편하도록 나는 아래의 위치에 두었다. . Users [your id] cmder_mini . 최초 실행하면 윈도우가 경고를 날린다. 이래 그림에서 형광색이 칠해진 부분을 클릭하고 프로그램을 허용해준다. 이후 몇 번의 경고성 질문이 더 날아온다. 전부 “허용”해주면 되겠다. . . 그리고 cmder 실행 파일 위에서 좌 클릭해서, 속성 &gt; 호환성 &gt; 관리자 권한으로 이 프로그램을 실행을 체크해두는 편을 권한다. 혹시 dock에 cmder를 박아둔 상태라면 Shift 키와 함께 좌 클릭을 하면 속성을 선택할 수 있다. . How . 깃을 편하게 쓰기 위해서 Cmder의 설정을 약간 바꿔주겠다. 우선 아래의 그림처럼 맨 위 메뉴의 삼선을 클릭하면 Cmder의 세팅으로 갈 수 있다. . . 엄청나게 많은 설정들이 있다. 창을 투명으로 만든다든가, 텍스트의 폰트를 바꾼다든가, 하는 것들이 가능하다. 여기서는 git bash를 쓰는 설정만 간단하게 설명하도록 하자. . . 그림에서 보듯이 startup &gt; task를 선택한다. 선택 가능한 CLI 중에서 {Bash:: Git bash}를 선택한다. 이 녀석을 디폴트 CLI로 설정한다. 시작 디렉토리를 지정하고 싶다면 아래 Startup dir...을 선택해 바꿔주면 된다. . Finally? . 아래 그림에서 보듯이 윈도 명령 창에서는 먹지 않는 ls 등의 명령어가 잘 먹고, git도 편안하게 사용할 수 있다. . . Seriously! . 여기까지 좋은데, git-bash에서 python을 쓰시는 분들이라면 python을 제대로 띄울 수 없다. 해결책은 간단하다. 아래와 같이 실행하면 된다.1 . $ winpty python.exe Python 3.7.6 (default, Oct 2 2018, 09:18:58) Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; . 매번 이럴게 winpty로 띄울 수는 없으니 .bashrc에 넣도록 하자. . $ echo &quot;alias python=&#39;winpty python.exe&#39;&quot; &gt;&gt; ~/.bashrc . 넣은 뒤에 터미널을 재실행하거나 source ~/.bashrc를 실행해주면 된다. . 출처는 여기다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/coding-tool/git/2020/03/05/using-git-w10.html",
            "relUrl": "/coding-tool/git/2020/03/05/using-git-w10.html",
            "date": " • Mar 5, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "The College Wealth Divide",
            "content": "Source . The College Wealth Divide: Education and Inequality in America, 1956-2016 download Alina K. Bartscher, Moritz Kuhn, and Moritz Schularick . Addressing Issues . 새로운 설문 데이터의 발굴을 통해 대졸자와 비대졸자 사이의 소득과 부의 장기 추세를 살펴본다. | 대학 졸업에 따른 소득 프리미엄과 부 프리미엄의 패턴 차이가 확연히 존재하는데, 이러한 결과가 생긴 원인을 추적한다. | . Data . 미국 연준에서 다운로드 받을 수 있는 Survey of Consumer Finances(SCF)라는 데이터가 있다. 이 데이터는 3년 간격으로 1983년부터 설문 자료를 수집하고 있는데, 여타 센서스와 같은 설문 항목과 비교해서 금용 관련된 상세한 내용을 담고 있다. | 그런데 SCF에 선배라고 할만한 자료가 1947년에서 1971년 사이 그리고 1977년에 대해서 미시건 대의 서베이 리서치 센터에 존재한다. Kuhn, Schularick, Steins가 이 두 자료를 코드 북에 기반해서 자료를 최대한 비슷하게 맞춰 결합했고, 이렇게 1949년~2016년의 전후 기간을 망라한 소득, 부 그리고 관련된 인구 통계적 정보를 담고 있는 데이터 셋이 만들어졌다. 저자들은 이를 SCF+라고 부른다.[^1] [^1]: 매칭과 데이터 결합에 필요한 절차 및 기법은 논문에 비교적 상세하게 소개되어 있으니 관심 있는 분은 참조하시라. | . Issues . Income premium vs wealth premium . 저자들의 관심사는 대졸자와 비대졸자(앞으로 혼란의 여지가 없을 때 각각 c(college), nc(noncollege)로 표기하겠다) 사이에 소득과 부의 차이가 역사적으로 어떻게 진화했는지를 파악하는 것이다. 우선 그림 1을 보자. | . 그림 1. 소득과 부의 역사적 변화 . 그림1에서 $y$축은 1971년도 수준을 1로 두었을 때 각 연도의 소득과 부의 수준을 나타낸다. 시간에 흐름에 따라서 소득의 차이가 상대적으로 적게 벌어졌음에 반해서 부의 차이는 크게 벌어지고 있다. 특히 1980년대 이후 차이가 크게 벌어지고 있다. . | 이는 소득과 부의 비율을 c와 nc로 나누어 비교한 그림2에서 보다 분명하게 드러난다. . | . 그림 2. c와 nc의 부/소득의 역사적 변화 . Decomposing wealth premium . W‾e,tW‾e,71=∑i=13se,i,tW‾e,i,tW‾e,71=∑i=13se,i,tW‾e,i,71W‾e,71W‾e,i,tW‾e,i,71 dfrac{ overline{W}_{e,t}}{ overline{W}_{e,71}} = dfrac{ sum^{3}_{i=1} s_{e,i,t} overline{W}_{e,i,t}}{ overline{W}_{e,71}} = sum^{3}_{i=1} s_{e,i,t} dfrac{ overline{W}_{e,i,71}}{ overline{W}_{e,71}} dfrac{ overline{W}_{e,i,t}}{ overline{W}_{e,i,71}}We,71​We,t​​=We,71​∑i=13​se,i,t​We,i,t​​=∑i=13​se,i,t​We,71​We,i,71​​We,i,71​We,i,t​​ . 변수 $ overline {W}_ cdot$: 해당 집단이 보유한 평균 부 | $s_ cdot$: 해당 부 계층의 비율. $s_{c, 1, 81}$이라면 1981년도 c에 속한 사람들 중에서 전체 부의 계층 1에 속하는 비율을 나타낸다. | . | . | 인덱스 $e = { rm c, nc}$ | $t =$ 56, 61, 66, 71, 76, 81, 86, 91, 96, 01, 06,11, 16 | $i = 1, 2, 3$ | . | $e$는 c와 nc의 구분을 $t$는 연도, 그리고 $i$는 세 가지로 구분한 부 계층을 구분하기 위한 인덱스다. 1은 하위 50%, 2는 중위 50%~90%, 그리고 3은 상위 10%를 각각 나타낸다. | 분해 식을 통해서 크게 세가지 변동을 구분할 수 있다. $s$의 변화 | W‾e,i,71W‾e,71 dfrac{ overline{W}_{e,i,71}}{ overline{W}_{e,71}}We,71​We,i,71​​: 기준 연도의 부 계층의 차이 | W‾e,i,tW‾e,i,71 dfrac{ overline{W}_{e,i,t}}{ overline{W}_{e,i,71}}We,i,71​We,i,t​​: 기준 연도 대비 특정 부 계층의 연도별 변화 | . | 이 세 가지에 분해 요소 중에서 시간을 따라가지 않은 두 번째를 제외하고 나머지에 대해서 반사실(counterfactual)을 적용해보자. . | Counterfactual 1 W‾x,i,tW‾x,i,71={W‾nc,i,tW‾nc,i,71if x=cW‾c,i,tW‾c,i,71if x=nc dfrac{ overline{W}_{x,i,t}}{ overline{W}_{x,i,71}} = begin{cases} dfrac{ overline{W}_{nc,i,t}}{ overline{W}_{nc,i,71}} &amp; text{if $x = rm{c}$} dfrac{ overline{W}_{c,i,t}}{ overline{W}_{c,i,71}} &amp; text{if $x = rm{nc}$} end{cases}Wx,i,71​Wx,i,t​​=⎩⎪⎪⎪⎨⎪⎪⎪⎧​Wnc,i,71​Wnc,i,t​​Wc,i,71​Wc,i,t​​​if x=cif x=nc​ . | Counterfactual 2 $s_{x,i,t}$ 대신 $s_{x,i,71}$을 적용한다. 즉, 시간이 지나면서 학력별 부 계급의 분포가 1971년도 지표에 고정되어 있었을 상황을 대입해본다. 이는 시간이 지나면서 소득 불평등의 패턴이 n, nc 사이에 차별적으로 타나났는지를 추적한다. | 같은 맥락에서 n와 nc의 $s$를 바꾸는 분석도 해볼 수 있을 듯 한데, 저자들이 시도하지는 않았다. | . | . 그림 3. c와 nc의 변화는 어디에서 왔는가? . 그림 3의 A는 c의 부에 대해서 반사실 1, 2 그리고 1+2를 적용했을 때의 변화를 나타낸다. 반사실 1의 영향이 크고, 반사실 2는 거의 영향을 주지 않았다. 이를 통해 시간이 지나면서 c의 경우 nc에 비해서 더 많은 부를 축적했음을 보여준다. 반면, 코호트(n, nc) 내에서 부의 계층 변화는 거의 영향을 주지 않았다. | 한편, nc의 경우는 반사실 1과 반사실 2가 모두 부의 축적에 악영향을 주었음을 알 수 있다. | 각 코호트별로 부의 계층별 점유율 변화는 그림 4와 같다. | . 그림 4. c와 nc 내의 소득 계층별 비율의 역사적 변화 . 그림에서 보듯이 c의 경우 연도 변화에 따른 계층별 변화가 크지 않다. nc의 경우 상위 10%에 속하는 비율이 경향적으로 낮아지고 있다. | . 그림 5. 소득 계층별 c/nc 비율의 역사적 변화 . 그림 5는 전체 인구에서 c가 차지하는 비중의 연도별 변화와 중간 계층(50~90%)의 변화가 비슷하게 진행되었음을 알 수 있다. 반면 c의 경우 50% 아래로 떨어지기보다는 상위 10%로 진입한 경향이 많았다는 점도 확인할 수 있다. | . Regression analysis . 대학 졸업 여부가 어느 측면에서 부의 축적에 영향을 주었는지 살펴보기 위해서 계량적 분석을 시도 했다. 즉, c 여부가 부의 증가에 어떻게 영향을 주었는가를 보다 면밀하게 살펴보는 것이 분석의 목표다. | . 그림 6. c/nc의 소득 비율과 부 비율의 역사적 변화 . 그림 6은 c와 nc의 격차를 소득과 부 각각에 관해 표시한 것이다. 앞서 보았듯이 1980년대 이전까지는 소득과 부 모두 1 주변에 머물다가 1980년대 이후 부의 증가가 점점 격차를 벌이며 증가하는 추세를 보이고 있다. 혹시 이러하는 추세가 c에 속한 상위 10%의 보다 빠른 부의 축적 때문이었을까? 코호트를 중위 소득 계층(50~90%)으로 제한해서 살펴보면 그림 7과 같다. 그림에서 보듯이 c의 자산 증가는 최상 계층의 현상이 아닌 일반적인 추세였다고 볼 수 있겠다. | . 그림 7. 50~90% 부 계층의 소득 및 부 수준의 역사적 변화 (10년 단위) . 질문은 무엇이 c의 빠른 부의 축적을 낳았는가, 하는 대목이다. 일반 기본적으로 살펴본 회귀 식은 다음과 같다. | . Wit=α0+β1cit⏟A+∑t&gt;1956β2,t Iyear=t⋅ct⏟B+∑t&gt;1956β3,t Iyear=t⏟C+Γ′Xit⏟D+ξitW_{it} = alpha_0 + underbrace{ beta_1 c_{it}}_{A} + underbrace{ sum_{t &gt; 1956} beta_{2,t}~ mathbb{I}_{year = t} cdot c_t}_{B} + underbrace{ sum_{t &gt; 1956} beta_{3,t}~ mathbb{I}_{year = t}}_{C} + underbrace{ Gamma&amp;#x27; X_{it}}_{D} + xi_{it}Wit​=α0​+A . β1​cit​​​+B . t&gt;1956∑​β2,t​ Iyear=t​⋅ct​​​+C . t&gt;1956∑​β3,t​ Iyear=t​​​+D . Γ′Xit​​​+ξit​ . 회귀분석의 각 부분을 간략하게 살펴보자. A: 대학 졸업 여부가 전체 기간에 걸쳐 부에 미치는 효과 | B: 연도별로 구분된 대학 졸업 여부가 부에 미치는 효과 | C: 연도별 효과가 부에 미치는 효과 | D: 통제 변수 (소득수준, 결혼 여부, 자녀 유무 등) | . | 부에 c가 미치는 효과는 $ beta_1 + beta_{2,t}$로 연도별로 측정할 수 있다. | . 그림 8. c의 부 관련 회귀 계수의 역사적 변화 . 그림 8에서 보듯이 c의 효과는 분명하다. 다만 소득 계층 기준으로 전체 집단의 효과가 50~90%의 중간 층에 비해 강하게 나타고 있다. 그림 9를 통해 이 내용을 다시 살펴보자. | . 그림 9. c의 부와 소득 회귀 계수의 비교 . 그림 9는 소득 계층별로 나누어 결과를 나타내고 있다. 왼쪽은 좌변의 종속 변수가 부이고 오른쪽은 종속 변수가 소득이다. c에 대해서 부의 효과가 전반적으로 높으며 특히 상위 10%에 두드러진다. | 소득에 대해서는 이러한 효과가 나타나지 않는다. 특히 상위 10%를 제외하면 c의 소득 증가 효과는 거의 없다고 봐도 좋다. | . | 논문에서는 이러한 c의 효과가 어디서 비롯하는지를 조금 더 탐구하고 있다. 자료의 한계 때문에 상세하게 추적하지는 못한다. 다만 c와 nc 사이에 존재하는 중요한 차이로 금융 지식(financial literacy)의 차이를 지목하고 있다. 즉, 소득을 투자로 전환할 수 있는 수단을 알고 있었는지 여부가 중요하다는 것이다. 논문에서 주목한 또 하나의 차이는 사업체의 소유 여부다. 이 역시 부의 차이를 만들어내는 데 일정한 역할을 한다. . | 논문에서 제기한 또 하나의 흥미로운 질문은 금융 지식의 여부가 c와 nc 사이에 세분화되어 있는지 여부다. 다시 말하면, 두 코호트가 같이 금융 상품에 투자했다면, 누가 더 잘 하는가에 관한 것이다. | . 그림 10. 금융 및 사업 관련 부의 수익률 비교 . 그림 10의 A에 따르면 사업체 보유자와 미 보유자 사이에는 수익률 차이가 존재한다. 하지만 n와 nc 사이에 차이는 거의 없다. 나머지 그림에서도 n와 nc 사이에 뚜렷한 차이를 발견하기 힘들다. 즉 c와 nc 사이의 금융 지식의 차이란 투자 대상의 인지 및 실행과 같은 차원일 가능성이 높다는 것이다. | . Extra . 개인적으로 재미있게 본 내용 하나 추가한다. 유유상종 혼(assortative marriage)이 존재하는지 여부에 관한 것이다. 교육 년수를 12년 미만(중퇴), 고졸, 대졸로 구분하면 3X3 테이블이 나온다. | 이제 각각의 경우에 해당하는 부부의 비율을 구한다. | 각각의 인구 비율이 무작위로 매칭되었을 때의 비율을 구한다. | 실제의 유유상종 혼의 비율과 무작위의 비율을 비교하면 유유상종의 정도가 계산된다. | . | 논문에서 밝힌 바에 따르면 대졸자의 유유상종 혼은 줄어드는 추세다. 즉, 1965년에 위 비율은 5.9였는데, 2016년에는 1.8에 불과하다. 즉 유유상종 혼은 감소하는 추세다. . | 다음으로 유유상종 혼이 부의 창출에 있어서 단순 합을 넘어서는 추가적인 이득을 제공하는가 여부에 관한 것이다. | . 그림 11. 유유상종 혼은 부의 형성에 어떻게 영향을 주었는가 . A, B에서 보듯이 소득과 부 모두 유유상종 혼 상태의 가구가 평균보다 두 배 가량 높다. 하지만 C, D에서 보듯이 이를 각 해당 연도의 유유상종 혼 인구가 점유한 부의 비율과 유유상종 혼 인구의 비율로 나눈 수치는 거의 일정한 수준을 유지하고 있다. | .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/economics/paper-summary/2020/01/22/Wealth-divided.html",
            "relUrl": "/economics/paper-summary/2020/01/22/Wealth-divided.html",
            "date": " • Jan 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Matrix Decomposition",
            "content": "Summary in advance . Name $A in$ Restriction Comments . LU | ${ mathbb R}^{m times n}$ |   |   | . Cholesky | ${ mathbb C}^{n times n}$ | PD, Symmetric(Hermite) | fast algorithm | . Eigendecomposition | ${ mathbb C}^{n times n}$ | invertible |   | . QR | ${ mathbb R}^{m times n}$ |   | Gram-Schmidt processing | . SVD | ${ mathbb C}^{m times n}$ |   |   | . $m geq n$ | . LU Decomposition . $A in { mathbb R}^{n times n}$ 기본적으로는 가우스-조르단 소거법을 떠올리면 좋다. 가우스 소거법이란게 뭔가? 우리가 중고등학교 때 배웠던 연립방정식의 해를 얻는 과정이다. LU 방법은 가우스-조르단 소거법은 2단계로 분리한 형태라고 이해하면 된다. | . Row operation . Row exchange 연립방정식을 풀 때 식의 순서를 바꾼다고 해가 달라지지는 않는다. | . | Multiply a row by a nonzero constant 어떤 식의 양변에 0이 아닌 수를 곱한다고해서 해가 달라지지는 않는다. | . | Add one row to another 연립방정식의 해를 구하는 과정이 이러한 식 조작의 연속이다. | . | . Solution for linear equation . $A x = b$와 같은 적당한 선형 연립방정식이 있다고 하자. 이때 . PA=LUP A= LUPA=LU . P는 행과 열을 적당히 재배치하는 데 동원되는 순열 행렬이며, 0과 1만 원소로 지닌다.1 | . PAx=PbLUx=Pb begin{aligned} P A x &amp; = P b LU x &amp; = Pb end{aligned}PAxLUx​=Pb=Pb​ . 이때, $Ux =y$라고 두고 문제를 한번 풀고, $Ly = Pb$라고 두고 문제를 다시 한번 풀면 해를 쉽게 구할 수 있다. . Determinant . $A = P^{-1} L U$ 를 만족하고, . det⁡A=det⁡P−1det⁡Ldet⁡U=det⁡P(∏i=1nlii)(∏i=1nuii)=(−1)S(∏i=1nlii)(∏i=1nuii) begin{aligned} det A &amp; = det P^{-1} det L det U &amp; = det P ( prod _{i=1}^{n} l_{ii}) ( prod _{i=1}^{n} u_{ii}) &amp; = (-1)^S ( prod _{i=1}^{n} l_{ii}) ( prod _{i=1}^{n} u_{ii}) end{aligned}detA​=detP−1detLdetU=detP(i=1∏n​lii​)(i=1∏n​uii​)=(−1)S(i=1∏n​lii​)(i=1∏n​uii​)​ . $S$는 행이 교환된 횟수를 나타낸다. | $P$는 직교행렬이므로, $P^{-1} = P^T$가 성립한다. | UTM(Upper Triangular Matrix, 상삼각 행렬), LTM(Lower Triangular Matrix, 하삼각 행렬)의 경우 행렬식의 값은 대각원소를 곱한 것과 같다. | . Cholesky Decomposition . $A in { mathbb C}^{n times n}$ | 에르미트 행렬(Hermite matrix)이란 $A$가 그 켤레 전치(conjugate transpose) 행렬($A^*$)과 같은 행렬이다. | . A=A∗, where Aij=Aji‾A = A^*,~ text{where}~A_{ij} = overline{A_{ji}}A=A∗, where Aij​=Aji​​ . 이때 에르미트 행렬이고 양정부호 행렬 $A$가 있을 때 숄레스키 분해는 다음과 같다. | . A=LL∗A = L L^*A=LL∗ . $L$은 LTM이며, $L^*$는 UTM이다. 한편 $L$의 대각성분은 모두 양의 실수가 된다. | 만일 $A in { mathbb R}^{n times n}$이라면, | . A=LLTA = L L^TA=LLT . LU 분해의 특별한 경우라고 볼 수 있다. 알고리듬의 관점에서보면, LU에 비해 숄레스키 분해가 2~3 배 빠르다. | . Eigendecomposition . $A in { mathbb R}^{n times n}$ | 자세한 내용은 여기를 참고하면 된다. | 행렬 $A$가 가역일 경우 $A = Q boldsymbol{ lambda}Q^{-1}$로 분해할 수 있다. | 만일 $A$가 가역이고 대칭행렬이라면, $Q^T Q = I$ 이므로 $Q^{-1} = Q^T$가 된다. 따라서 $A = Q boldsymbol{ lambda}Q^{T}$ | . QR decomposition . $A in { mathbb R}^{n times n}$ | 행렬을 직교 행렬과 삼각 형렬(UTM, LTM)로 분해하는 과정을 뜻한다. | . Gram-Schmidt processing . A=[∣ … ∣v1 … vn∣ … ∣]A = begin{bmatrix} vert ~ &amp; dotsc &amp; ~ vert v_1 ~ &amp; dotsc &amp; ~ v_n vert ~ &amp; dotsc &amp; ~ vert end{bmatrix}A=⎣⎢⎡​∣ v1​ ∣ ​………​ ∣ vn​ ∣​⎦⎥⎤​ . u1=v1, e1=u1∥u1∥u2=v2−(v2⋅e1)e1, e2=u2∥u2∥vk+1=vk+1−(vk+1⋅e1)e1−⋯−(vk+1⋅ek)ek, ek+1=uk+1∥uk+1∥ begin{aligned} u_1 = v_1, &amp; ~e_1 = dfrac{u_1}{ Vert u_1 Vert} u_2 = v_2 - (v_2 cdot e_1) e_1, &amp;~e_2 = dfrac{u_2}{ Vert u_2 Vert} v_{k+1} = v_{k+1} - (v_{k+1} cdot e_1) e_1 - dotsb-(v_{k+1} cdot e_k)e_k,&amp;~e_{k+1} = dfrac{u_{k+1}}{ Vert u_{k+1} Vert} end{aligned}u1​=v1​,u2​=v2​−(v2​⋅e1​)e1​,vk+1​=vk+1​−(vk+1​⋅e1​)e1​−⋯−(vk+1​⋅ek​)ek​,​ e1​=∥u1​∥u1​​ e2​=∥u2​∥u2​​ ek+1​=∥uk+1​∥uk+1​​​ . . G-S 과정은 위 그림이 잘 표현해준다. 즉, 그림의 $v_2$(식의 $a_2$)에서 $e_1$으로 프로젝션을 한 벡터를 빼주게 되면 $v_1$과 직교하는 $u_2$를 얻을 수 있는 것이다. . | 과정을 행렬의 곱으로 재배열해준 것이 QR 분해다. 즉, . | . A=[∣ … ∣v1 … vn∣ … ∣]=[e1 … en]⏟Q[v1⋅e1v2⋅e1…vn⋅e10v2⋅e2…vn⋅e2⋮⋮⋱⋮00…vn⋅en]⏟R begin{aligned} A &amp; = begin{bmatrix} vert ~ &amp; dotsc &amp; ~ vert v_1 ~ &amp; dotsc &amp; ~ v_n vert ~ &amp; dotsc &amp; ~ vert end{bmatrix} &amp; = underbrace{ [e_1 ~ dotsc ~ e_n] }_{Q} underbrace{ begin{bmatrix} v_1 cdot e_1 &amp; v_2 cdot e_1 &amp; dotsc &amp; v_n cdot e_1 0 &amp; v_2 cdot e_2 &amp; dotsc &amp; v_n cdot e_2 vdots &amp; vdots &amp; ddots &amp; vdots 0 &amp; 0 &amp; dotsc &amp; v_n cdot e_n end{bmatrix}}_{R} end{aligned}A​=⎣⎢⎡​∣ v1​ ∣ ​………​ ∣ vn​ ∣​⎦⎥⎤​=Q . [e1​ … en​]​​R . ⎣⎢⎢⎢⎢⎡​v1​⋅e1​0⋮0​v2​⋅e1​v2​⋅e2​⋮0​……⋱…​vn​⋅e1​vn​⋅e2​⋮vn​⋅en​​⎦⎥⎥⎥⎥⎤​​​​ . $e_1, dotsc, e_n$만 구하면 쉽게 분해를 달성할 수 있음을 알 수 있다. 그리고 이렇게 분해가 가능한 이유는 G-S 알고리듬 때문이다. | . Least quares solution . QR 분해는 다소 엉뚱하게도 최소자승법의 해를 구할 때 유용하게 활용할 수 있다. | . 먼저 $A in { mathbb R}^{m times n}$ ($m geq n$)이라고 하자. 이때 다음과 같은 QR 분해가 존재하다. . A=Q[R0]A = Q begin{bmatrix} R 0 end{bmatrix}A=Q[R0​] . $Q in { mathbb R}^{n times n}$의 직교행렬(즉, $Q^{-1} =Q^T$), $R in { mathbb R}^{m times m}$의 UTM(upper triangular matrix, 상삼각행렬)이다. | 만일 $m = n$이고 $A$가 가역이면 QR 분해는 유일하다. | 이제 $A$가 full-column rank(즉, ${ rm rank}(A) = n$)라고 하면, A의 QR 분해는 아래와 같이 쓸 수 있다. | . A=[Q1,Q2][R0]=Q1RA = [Q_1, Q_2] begin{bmatrix} R 0 end{bmatrix} = Q_1 RA=[Q1​,Q2​][R0​]=Q1​R . 이제 최소자승법의 목적함수를 다음과 같이 써보자. . ∥Ax−b∥22=∥QT(Ax−b)∥=∥[R0]x−QTb∥=∥Rx−d1∥⏟(∗)+∥d2∥ begin{aligned} Vert A x - b Vert^2_2 &amp; = Vert Q^T(Ax-b) Vert &amp; = Vert begin{bmatrix} R 0 end{bmatrix} x - Q^T b Vert &amp; = underbrace{ Vert Rx - d_1 Vert}_{( ast)} + Vert d_2 Vert end{aligned}∥Ax−b∥22​​=∥QT(Ax−b)∥=∥[R0​]x−QTb∥=(∗) . ∥Rx−d1​∥​​+∥d2​∥​ . 이때 $ Vert d_2 Vert$는 상수이므로, 극소화 문제는 $( ast)$를 최소화하면 된다. 그리고 저 거리를 최소화해주는 것은 당연하게 . x=R−1d1x = R^{-1} d_1x=R−1d1​ . 이 해는 normal equation을 최소화하여 얻은 해와 같음을 알 수 있다. | . Singular Value Decomposition . $M in mathbb{C}^{m times n}$ | . M=UΣV∗M = U boldsymbol{ Sigma} V^*M=UΣV∗ . eigendecomposition를 보다 일반화했다고 보면 좋겠다. | $U in mathbb{C}^{m times m}$: 유니터리 행렬, 즉 $U U^* = I_m$ | $ boldsymbol{ Sigma in mathbb{R}^{m times n}}$: 대각 원소가 음수가 아니며 나머지 원소는 모두 0 | $V^* in mathbb{C}^{n times n}$: 유니터리 행렬, 즉 $V V^* = I_n$ | 만일 $U$, $V$가 모두 실수라면 각각은 $m$ 차원 $n$ 차원의 직교 행렬이다. | . Geometric interpretation . . 기하학적으로 해석해볼 수 있겠다. 대체로 유니터리 혹은 직교 행렬들을 서로 직교하는 축을 회전하는 역할을 한다. 한편, 실수로 구성된 대각 행렬을 벡터의 길이를 늘이고 줄이는 역할을 한다. 아울러 SVD에서는 차원을 조정하는 역할을 한다. 어떤 축은 없애기도 하고, 없던 축을 생성하기도 한다. . 따라서 어떤 $x in { mathbb C}^{n times 1}$가 있을 때 . $V^*$는 이 벡터의 축을 바꿔준다. | $ boldsymbol{ Sigma}$은 각 축의 단위 거리와 차원을 조정한다. - $U$는 다시 벡터의 축을 바꾼다. | . Reduced SVD . SVD를 도식화해서 나타내면 아래와 같다. | . . 그리고 관례상 $ boldsymbol{ Sigma}$은 큰 순서대로 나열하는 것으로 한다. | . Thin SVD . . svd가 존재하지 않은 영역은 사실상 불필요한 셈이니 이들을 잘라낸다. | . Compact SVD . . 만일 svd가 0인 값들이 들어 있다면, 이들을 제외하고 다시 비슷한 축약을 거친다. $r &lt; s$ | . Truncated SVD . . svd 값을 일정한 기준으로 잘라낸 것이다. 즉, 해당 기준 이후의 값들을 쳐냈다고 보면 좋다. | . 이때, raw, thin, compact는 정보의 손실이 없다는 점을 상기하자. Truncated 부터 정보의 손실이 발생한다.[^1] [^1]: 이런 특성 때문에 truncated SVD는 이미지 압축에 많이 활용된다. 여기에서 그 사례를 확인할 수 있다. . Pseudo-inverse . 보통 $A in { mathbb R}^{m times n}$ ($m &gt; n$)인 경우 역행렬을 구할 수 없다. 이때 역행렬과 최대한 비슷한 행렬을 찾는 것이 목표다. 만일 $Ax = b$에서 역행렬이 존재한다면 $x = A^{-1}b$로 쉽게 구할 수 있다. 하지만 역행렬이 존재하지 않는다면 어떤 $x$를 구하는 게 좋을까? $ Vert Ax - b Vert$를 최소화하는 $x$를 구한다고 할 때, 이를 달성해주는 $A^+$를 $A$의 유사역행렬이라고 부른다. 즉, . x=A+bx = A^+ bx=A+b . SVD는 이 유사역행렬을 구하는 과정에서 활용될 수 있다. 즉, $A = U boldsymbol{ Sigma} V^$이라면 $A^+ = V boldsymbol{ Sigma}^+ U^$라고 둘 수 있다. 여기서 . $ boldsymbol{ Sigma}^+$는 $ boldsymbol{ Sigma}$의 대각 원소 중 0이 아닌 것들의 역수를 넣은 후 이를 전치한 행렬을 뜻한다. 즉, | . A=U⏟m×m[σ1 ⋯ 0⋮ ⋱ ⋮0 ⋯ σs0  ⋯  0⋮ ⋱ ⋮0  ⋯  0]⏟m×nVT⏟n×nA = underbrace{U}_{m times m} underbrace{ begin{bmatrix} sigma_1 ~ dotsb~ 0 vdots ~ ddots ~ vdots 0 ~ dotsb ~ sigma_s 0 ~~ dotsb ~~ 0 vdots ~ ddots ~ vdots 0 ~~ dotsb ~~ 0 end{bmatrix} }_{m times n} underbrace{V^T}_{n times n}A=m×m . U​​m×n . ⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎡​σ1​ ⋯ 0⋮ ⋱ ⋮0 ⋯ σs​0  ⋯  0⋮ ⋱ ⋮0  ⋯  0​⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎤​​​n×n . VT​​ . A+=V⏟n×n[1σ1 ⋯ 0 0 ⋯ 0⋮  ⋱  ⋮  0 ⋯ 00 ⋯ 1σs 0 ⋯ 0]⏟n×mUT⏟m×mA^+ = underbrace{V}_{n times n} underbrace{ begin{bmatrix} frac{1}{ sigma_1} ~ dotsb~ 0~0 ~ dotsb ~ 0 vdots ~~ ddots ~~ vdots ~~ 0 ~ dotsb ~ 0 0 ~ dotsb ~ frac{1}{ sigma_s} ~ 0 ~ dotsb ~ 0 end{bmatrix} }_{n times m} underbrace{U^T}_{m times m}A+=n×n . V​​n×m . ⎣⎢⎢⎡​σ1​1​ ⋯ 0 0 ⋯ 0⋮  ⋱  ⋮  0 ⋯ 00 ⋯ σs​1​ 0 ⋯ 0​⎦⎥⎥⎤​​​m×m . UT​​ . 이는 사실 최소자승법과 같은 해를 구해준다. | $A^{-1}$은 행렬의 좌곱과 우곱이 동일한 단위 행렬을 생성한다. 반면, | . A+A=(VΣ+UT)(UΣVT)=VVT=InA^+ A = ( V boldsymbol{ Sigma}^+ U^T) (U boldsymbol{ Sigma} V^T) = V V^T = I_nA+A=(VΣ+UT)(UΣVT)=VVT=In​ . AA+=(UΣVT)(VΣ+UT)=UUT=ImAA^+ = (U boldsymbol{ Sigma} V^T) ( V boldsymbol{ Sigma}^+ U^T) = U U^T = I_mAA+=(UΣVT)(VΣ+UT)=UUT=Im​ . Sources . https://darkpgmr.tistory.com/106 . $P$가 왜 필요한지 생각해본 적이 있는가? 보통 대수적인 관점에서 보면 $A=LU$로 충분하다. 하지만 계산의 관점에서 보자. 피봇 원소가 지나치게 작으면 계산이 성가시다. 얘네들은 뒤로 미뤄놓고 0으로 간주하면 계산이 쉽지 않겠는가? 따라서 계산의 관점에서 보면 $P$를 통해 $A$의 피봇을 큰 순서대로 정렬하는 것이 좋다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/01/07/matrix-decomp.html",
            "relUrl": "/math/matrix-theory/2020/01/07/matrix-decomp.html",
            "date": " • Jan 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Gauss-Jordan Elimination Algorithm",
            "content": "How to Solve Linear System . 1차 연립방정식을 풀기 위한 일종의 알고리듬이다. | 흔히 아는 연립방정식의 형태를 행렬식으로 표시해보자. | . Ax=bA x = bAx=b . $A in { mathbb R}^{m times n}$, $b in { mathbb R}^{m times 1}$ | 이때 $b$를 계수 행렬 $A$의 한 열로 편입한 형태를 augmented form이라고 부른다. 즉, | . [A ∣ b]∈Rm×(n+1) begin{bmatrix} A ~ vert ~ b end{bmatrix} in { mathbb R}^{m times (n +1)}[A ∣ b​]∈Rm×(n+1) . 해가 존재하면 일치성이 있다고 말하고 해가 존재하지 않으면 일치성이 없다고 말한다. | . Row Echelon Form Matrix (REF) . ‘열 사다리꼴’ 행렬이라고 부른다. 정의는 다음과 같다 . . | 각 행에 0이 아닌 첫번째 원소를 “선행 원소” 혹은 피벗이라고 부른다. 선행원소 1을 조건으로 두는 경우가 있다. 이는 행 내에서 스케일링의 문제이므로 큰 문제는 아니다. | . | 컬럼에서 각 선행원소는 앞선 행의 선행 원소보다 오른편에 위치해야 한다. | 모든 행이 0인 행은 0이 아닌 행을 포함한 행보다 뒤쪽에 위치해야 한다. | . . Reduced Row Echelon Form Matrix (RREF) . RREF가 되기 위한 조건은 다음과 같다. REF다. | 각 행의 선행 원소는 해당 열에서 유일한 0이 아닌 원소다. | . | 아래의 예를 확인하자. | . [1∗∗∗∗∗∗∗∗01∗∗∗∗∗∗∗0001∗∗∗∗∗0000001∗∗00000001∗000000000000000000] begin{bmatrix} 1 &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast 0 &amp; 1 &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast 0 &amp; 0 &amp; 0 &amp; 1 &amp; ast &amp; ast &amp; ast &amp; ast &amp; ast 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; ast &amp; ast 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; ast 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 end{bmatrix}⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎡​1000000​∗100000​∗∗00000​∗∗10000​∗∗∗0000​∗∗∗0000​∗∗∗1000​∗∗∗∗100​∗∗∗∗∗00​⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎤​ . Matrix Operation . 연립방정식을 구하는 과정으로 여기면 쉽게 이해할 수 있다. 연립방정식이 주어졌을 때 해당 식의 순서를 바꾼다고 문제가 되지 않는다. | 같은 행에서 일정한 수를 곱해도 무방하다 . | 한 행을 상수배 하여 다른 행에 더한다. | . | . . 사실 우리가 중고등학교 때 배워왔던 1차 방정식의 해를 구하는 알고리듬 그대로다. | . Solution . 이렇게 구했을 때 RREF 행렬의 선행 원소, 즉 피벗은 그대로 맨 왼쪽 열의 가산된 항목을 해를 지닌다. 열로 생각해보자. RREF에서 피벗 열은 1인 한 개를 제외하면 모두 0이다. 따라서 해를 그대로 가중치로 지니게 된다. | . | 피벗이 아닌 경우는 어떤 값이 와도 상관 없다. | . Inverse Matrix . 가우스-조르단 방법으로 역 행렬도 구할 수 있다. | 앞서 매트릭스 연산을 행렬로 나타낸 행렬을 기본 행렬(elementary matrix)라고 한다. | . AX=IAX = IAX=I . 이때 $X = A^{-1}$로 둘 수 있다. 식의 양변에 동시에 기본 행렬 연산 $r$ 번을 전개한다고 하자. . ErEr−1⋯E1⏟(∗)AX=ErEr−1⋯E1I underbrace{E_r E_{r-1} dotsb E_1}_{(*)} A X = E_r E_{r-1} dotsb E_1 I(∗) . Er​Er−1​⋯E1​​​AX=Er​Er−1​⋯E1​I . 이때 이 연산의 결과 식의 좌변이 $IX$로 바뀐다고 하자. 이는 다시 아래와 같다. . ErEr−1⋯E1AX=ErEr−1⋯E1I=IXE_r E_{r-1} dotsb E_1 A X = E_r E_{r-1} dotsb E_1 I = IXEr​Er−1​⋯E1​AX=Er​Er−1​⋯E1​I=IX . $X = E_r E_{r-1} dotsb E_1$이 되고, 이것이 곧 역행렬이다. .   . Reference . https://en.wikipedia.org/wiki/Gaussian_elimination .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2020/01/07/Gauss-Jordan.html",
            "relUrl": "/math/matrix-theory/2020/01/07/Gauss-Jordan.html",
            "date": " • Jan 7, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Four Fundamental Spaces of Linear Algebra",
            "content": "Tales of Two Lines . . 행렬을 행 공간(row space)으로 이해하는 것과 열 공간(column space)으로 이해하는 것은 같은 해를 구하는 문제에서도 전혀 다른 함의를 지닌다. 아래의 연립 방정식을 풀고 싶다고 하자. . 2x+y=3x−2y=−1 begin{aligned} 2 x + y &amp; = 3 x - 2y &amp; = -1 end{aligned}2x+yx−2y​=3=−1​ . 행렬로 나타내면 다음과 같다. . [211−2][xy]=[3−1] begin{bmatrix} 2 &amp; 1 1 &amp; -2 end{bmatrix} begin{bmatrix} x y end{bmatrix} = begin{bmatrix} 3 -1 end{bmatrix}[21​1−2​][xy​]=[3−1​] . Row picture . 행으로 이해해보자. 이게 우리에게 익숙한 방식이다. 이 차원 평면($x$–$y$ 평면)에 직선 두 개를 그리고 교점을 찾으면 되겠다. 이것이 문제를 행으로 보는 관점이다. 아래 그림을 참고하자.1 . . Column picture . 이제 이 문제를 컬럼으로 보자. 행렬을 열로 보면, $(2 times 1)$ 벡터다. 이 벡터를 좌표로 나타나면 이제 $x$, $y$는 식의 방정식 우변의 벡터를 얻는 데 필요한 행렬의 두 행 벡터에 가중치가 된다. 아래 그림을 보자. . . Which of two? . 둘 다 쓸모가 있는 관점이지만 열 공간으로 보는 관점이 몇 가지 점에서 수학적으로 좋다. 우선, 열 공간으로 보게 되면 계산에 동원되는 모든 대상들이 ‘벡터 공간’에 위치하게 된다. 벡터 공간은 반드시 $ boldsymbol{0}$을 포함해야 한다. 열 공간에서는 이게 가능하다. 투입과 산출이 모두 벡터로 표현되고 산출은 행렬을 구성하는 열 벡터의 선형 결합을 통해 표현된다. 이 선형 결합이 일종의 투입이 된다. . 그런데 행 공간의 관점에서는 벡터 공간의 수학적인 표현과 그 결과를 활용하기 힘들다. 2차원 벡터까지는 평면에 도해할 수 있지만 3차원도 보기에 부담스럽다. . . 언감생심 $n( geq 4)$ 차원을 도해하는 것은 불가능하다. . 물론 열 공간의 관점을 취한다고 해도 ‘정확한’ 도해가 가능한 것은 아니다. 하지만 벡터 공간 안에서 정확하게 개념을 표시할 수는 있다. 아래 그림처럼 보통 벡터를 표현할 때 $ boldsymbol{0}$를 중심으로 벡터의 기호를 적는다. 적어도 그림상으로 벡터 스페이스 위에서 더하기와 곱하기를 표기하는 데 무리가 없다. . . 확인 차원에서 열 공간의 관점에서 행렬을 ‘함수’로 이해하는 방식을 다시 살펴보자. . A(m×n)x(n×1)=b(m×1) underset{(m times n)}{A} underset{(n times 1)}{x} = underset{(m times 1)}{b}(m×n)A​(n×1)x​=(m×1)b​ . 이렇게 보면 행렬 $A$는 특정한 방식으로(선형의 방식으로) $x$를 차원이 다른 벡터 $b$로 변환한다.2 이때 $A$의 경우 . A=[∣  …  ∣c1 … cn∣  …  ∣], whereA = begin{bmatrix} vert ~~ dotsc ~~ vert c_1 ~ dotsc~ c_n vert ~~ dotsc ~~ vert end{bmatrix},~ text{where}A=⎣⎢⎡​∣  …  ∣c1​ … cn​∣  …  ∣​⎦⎥⎤​, where . ci=[a1i⋮ami].c_i = begin{bmatrix} a_{1i} vdots a_{mi} end{bmatrix}.ci​=⎣⎢⎢⎡​a1i​⋮ami​​⎦⎥⎥⎤​. 이때, 투입 벡터 $x = [x_1, dots, x_n]$는 열 벡터들, $c_i$를 조합해 $b$를 만들 수 있는 가중치를 찾는 문제가 된다. . Ax=c1x1+c2x2+…+cnxn=bA x = c_1 x_1 + c_2 x_2 + dotsc + c_n x_n = bAx=c1​x1​+c2​x2​+…+cn​xn​=b . Big Picture of Linear Algebra . 다시 강조하지만 기본적으로 행렬은 함수다. $A$는 투입 벡터 $x ( in { mathbb R}^{n})$를 산출 벡터 $b( in { mathbb R}^{m})$로 바꾸는 역할을 한다. $A^T$는 투입벡터 $x^ prime in { mathbb R}^m$을 산출벡터 $b^ prime in { mathbb R}^n$으로 바꾼다. 이들 사이에 어떤 관계가 존재할까? 이를 나타내는 것이 길버트 스트랭(Gibert Strang) 선생이 말한 선형대수의 ‘큰 그림’이다. 아래 그림을 보자. . . 그림 자체로 그냥 이해가 간다. 열 공간으로 이해하는 습관이 들었다면, 그림이 뒤집혀야 하지 않나, 싶겠지만 앞서 보았던 연립방정식처럼 $A x = b$의 형태로 이해하면 좋다. . Row space . $A$의 행 공간은 $ mathbb{R}^n$에 속한다. | . A=[−r1T−−⋮−−rmT−]A = begin{bmatrix} { rm -} &amp; r_1^T &amp; { rm -} { rm -} &amp; vdots &amp; { rm -} &amp; &amp; { rm -} &amp; r_m^T &amp; { rm -} end{bmatrix}A=⎣⎢⎢⎢⎢⎡​−−−​r1T​⋮rmT​​−−−​⎦⎥⎥⎥⎥⎤​ . $r_i$는 $A$의 $i$ 번째 행을 원소로 하며, $r_i in { mathbb R}^{n}$ 벡터다. | 행 공간의 영 공간(null space) 역시 $ mathbb{R}^n$에 속한다. 영 공간이란 $A x = boldsymbol{0}$을 만족하는 $x neq boldsymbol{0}$의 벡터이므로 이 역시 $x in { mathbb R}^{n}$다. | . Orthogonality of row space and null space . 두 벡터는 직교할까? 행 공간 $ mathcal{R}$의 정의는 다음과 같다. . R(A)={xr∈Rn∣xr=∑i=1mαiri, where αi∈R, ri∈Rn} mathcal{R}(A) = { x_r in mathbb{R}^n vert x_r = sum_{i=1}^{m} alpha_i r_i,~ text{where}~ alpha_i in mathbb{R}, ~r_i in mathbb{R}^n }R(A)={xr​∈Rn∣xr​=i=1∑m​αi​ri​, where αi​∈R, ri​∈Rn} . 영공간(nullspace)에 속하는 벡터를 $x_n$라고 할 때(notation에 약간의 교란이 발생하지만 그림과의 일치를 위해 일단 이렇게 표기하도록 하자), 영공간의 정의에 따라서 $r_i^T x_n = 0$. . xrTxn=∑i=1mαi(riTxn)=0{x_r^T} x_n = sum_{i=1}^{m} alpha_i (r_i^T x_n) = 0xrT​xn​=i=1∑m​αi​(riT​xn​)=0 . 그리고 그림에서 보듯이 다음과 같은 관계가 성립한다. . $A x_r = b_r ( in { mathbb R}^m)$ | $A x_n = boldsymbol{0}_m$ | $A (x_r + x_n) = b_r$ | $x_r^T x_n = 0$ | . 위 관계에서 $b_r$, $ boldsymbol{0}$는 모두 열 공간에 존재하는 벡터들이므로 $(m times 1)$의 크기를 지닌다는 점에 유의하자. . Column space . $A$의 열 공간은 $ mathbb{R}^m$에 속한다. | . [∣  …  ∣c1 … cn∣  …  ∣], where ci=[c1i⋮cmi]. begin{bmatrix} vert ~~ dotsc ~~ vert c_1 ~ dotsc~ c_n vert ~~ dotsc ~~ vert end{bmatrix}, ~ text{where}~ c_i = begin{bmatrix} c_{1i} vdots c_{mi} end{bmatrix}.⎣⎢⎡​∣  …  ∣c1​ … cn​∣  …  ∣​⎦⎥⎤​, where ci​=⎣⎢⎢⎡​c1i​⋮cmi​​⎦⎥⎥⎤​. . 열 공간의 영 공간, 좌 영공간(left nullspace) 역시 $ mathbb{R}^m$에 속한다. 이는 $A^T x = 0$에 의해 정의된다. | . Orthogonality of column space and left null space . 나머지 과정은 비슷하게 전개할 수 있다. 열 공간 $ mathcal{C}$의 정의는 다음과 같다. | . C(A)={xc∈Rm∣xc=∑i=1nαici, where αi∈R, ci∈Rm} mathcal{C}(A) = { x_c in mathbb{R}^m vert x_c = sum_{i=1}^{n} alpha_i c_i,~ text{where}~ alpha_i in mathbb{R}, ~c_i in mathbb{R}^m }C(A)={xc​∈Rm∣xc​=i=1∑n​αi​ci​, where αi​∈R, ci​∈Rm} . 좌 영공간의 정의에 따르면, $c_i^T x_n = 0$가 성립한다. 따라서, . xcTxn=∑i=1nαi(riTxn)=0{x_c^T} x_n = sum_{i=1}^{n} alpha_i (r_i^T x_n) = 0xcT​xn​=i=1∑n​αi​(riT​xn​)=0 . $A^T x_c = b_c( in { mathbb R}^n)$ | $A^T x_n = boldsymbol{0}_n$ | $A (x_c + x_n) = b_r$ | $x_c^T x_n = 0$ | . Exchange of row and column . $A^T$의 열 공간이 곧 $A$의 행 공간이 된다. 따라서 $ mathcal{R}(A) = mathcal{C}(A^T)$가 된다. . AT=[r1 ,…, rm]A^T = begin{bmatrix} r_1 ~, dotsc, ~ r_m end{bmatrix}AT=[r1​ ,…, rm​​] . 위의 그림을 컬럼 스페이스로만 다시 표현하면 다음과 같다. 즉, $A$의 행 공간은 $A^T$의 열 공간이다. . . A simple example . 간단한 예 하나를 들어보자. . Ax=[1411].Ax = begin{bmatrix} 1 4 1 1 end{bmatrix}.Ax=⎣⎢⎢⎢⎡​1411​⎦⎥⎥⎥⎤​. . 그리고 . x=[011]⏟xp+c[021]⏟xs,∀c∈Rx= underbrace{ begin{bmatrix} 0 1 1 end{bmatrix} }_{x_p}+ c underbrace{ begin{bmatrix} 0 2 1 end{bmatrix}}_{x_s}, forall c in mathbb{R}x=xp​ . ⎣⎢⎡​011​⎦⎥⎤​​​+cxs​ . ⎣⎢⎡​021​⎦⎥⎤​​​,∀c∈R . 이 문제를 풀어보자. 우선 $A$을 열 벡터($c_i$)의 관점에서 바라보자. . A=[c1,c2,c3]A = begin{bmatrix} c_1, c_2, c_3 end{bmatrix}A=[c1​,c2​,c3​​] . Ax=Ac2+Ac3+c(2c2+c3)=[1411]. begin{aligned} Ax = A c_2 + A c_3 + c(2 c_2 + c_3) = begin{bmatrix} 1 4 1 1 end{bmatrix}. end{aligned}Ax=Ac2​+Ac3​+c(2c2​+c3​)=⎣⎢⎢⎢⎡​1411​⎦⎥⎥⎥⎤​.​ . 모든 $c$에 관해서 성립해야 하므로, $2c_2 + c_3 = 0$은 항상 성립해야 한다. 따라서 . A=[c1,c2,−2c2].A = begin{bmatrix} c_1, c_2, -2c_2 end{bmatrix}.A=[c1​,c2​,−2c2​​]. . 이제 행렬 $A$의 영 공간을 생각해보자. 영 공간이란 $A x = 0$을 만족하는 $x$로 이루어진 벡터 공간이다. $c$에 관계없이 $Ax_s = 0$을 만족해야 한다. 따라서 이를 만족하는 해는 $x_s$ 하나 밖에 없다. . 이제 앞서 본 4개의 근본 공간의 원리에 따라서 $A^T in { mathbb R}^{3 times 4}$이고, $A^T$는 열 벡터 $a_i( in { mathbb R}^3)$로 구성된다. 따라서 $A^T$의 위수는 $3-1 = 2$가 된다. 그리고 $A^T$의 위수와 $A$의 위수는 같기 때문에 $A$의 위수 역시 2이다. . Why? . 이 네 개의 스페이스가 맺고 있는 관련성은 그 자체만으로도 중요하고 흥미로운 것이지만, 이를 통해 이른바 SVD(Singluar Value Decomposition)을 달성할 수 있다. 만일 위에서 보듯이 $A$의 열 공간과 $A^T$의 열 공간이 같은 위수를 지니지 않는다면 이런 분해는 불가능하다. . . 먼저 매트릭스 $A$의 열 공간에 속하는 원소 중에서 $r$ 개만 서로 독립이라고 하자. 이렇다면 이 성분으로만 구성된 매트릭스 $U$를 만들 수 있다. 매트릭스 $U$의 켤레 전치행렬을 $U^*$라고 하면, 다음의 식이 성립한다. . UU∗=ImU U^{*} = I_mUU∗=Im​ . 그리고, 행 공간에 속하는 원소 역시 $r$ 개만 독립이고, 이를 기반으로 $V$를 만들 수 있다. 그리고 이 사이에 특성값(singular value)을 대각행렬로 지니는 $ Sigma$를 넣으면 $A$는 다음과 같이 세 가지로 분해된다. . . 기본적으로 행렬은 함수다. 즉 어떤 벡터의 변형이다. $M$에 투입되는 $(n times 1)$의 벡터 $x$가 있다고 하자. . 벡터의 방향을 돌린다 ($V^*$). | 특성값 행렬($ Sigma$)로 차원을 바꾸면서 좌표축의 크기를 조정한다. | 마지막으로 $U$를 통해서 벡터의 방향을 돌린다. | 그림의 출처는 여기 &#8617; . | 함수로서의 행렬 $A$에 열 벡터 $x$를 투입했다고 하자. 산출은 행 벡터일까? 열 벡터일까? 열 벡터다.($1 times 1$). k벡터가 나올 수는 있지만 열 벡터가 나온다. 이런 관점에서 이해하면 행렬 $A$의 우측에 열 벡터가 투입되면 열 벡터가 나오고, 좌측에 행 벡터가 투입되면 행 벡터가 나온다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2019/12/31/four-space-LA.html",
            "relUrl": "/math/matrix-theory/2019/12/31/four-space-LA.html",
            "date": " • Dec 31, 2019"
        }
        
    
  
    
        ,"post13": {
            "title": "Five (Deadly!) Questions on Regression and PCA",
            "content": "Question One . PCA는 Regression과 다른가요? . 좋은 질문이다. 아예 둘을 다른 방법이라고 보면 속편하겠지만 한번 쯤 이 질문을 고민해봤을 것이다. 답은 같기도 하고 다르기도 하다, 되겠다. . In common . 우선 둘 다 MSE(Mean Squared Error)를 극소화하는 목적함수를 지니고 있다. 차례로 살펴보자. . Regression . 회귀분석의 목적함수와 극소화는 아래와 같다. . min⁡β(y−Xβ)T(y−Xβ). min_{ beta} (y - X beta)^T (y - X beta).βmin​(y−Xβ)T(y−Xβ). . 여기서 각각 행렬 및 벡터의 크기를 확인해보자. . $y in { mathbb R}^n$, $X in { mathbb R}^{n times k}$, $ beta in { mathbb R}^k$ | . 개념적으로 말하면 regression은 일종의 목표 변수인 $y$와 이를 설명하기 위한 설명 변수인 regressors로 구성된 벡터 공간의 한 점 사이의 거리를 최소화하는 $ beta$를 찾는 것이다. 기하학적으로 보면 이 최소화는 $n$ 차원인 한 점에서 $y$에서 $k$ 차원인 $X$로 구성된 초평면으로 수선의 발을 내릴 경우 달성된다. . Xβ=[x1 … xk][β1⋮βk]=β1x1+⋯+βkxk,X beta = begin{bmatrix} x^1 ~ dotsc ~ x^k end{bmatrix} begin{bmatrix} beta_1 vdots beta_k end{bmatrix} = beta_1 x^1 + dotsb + beta_k x^k,Xβ=[x1 … xk​]⎣⎢⎢⎡​β1​⋮βk​​⎦⎥⎥⎤​=β1​x1+⋯+βk​xk, . where $x^j = [x_1^j, dotsc, x_n^j]^T$ for $j = 1, dotsc, k$. 즉 $x^j in { mathbb R}^n$ 벡터 $k$ 개로 이루어진 벡터 공간, 즉 $X$의 열 공간(column space)으로 $y$를 투영(project)한다는 의미이다. 이 열 공간은 $n$ 차원 안에 속한 $k$ 차원의 초평면이라는 점을 기억해두자. 아래 그림을 참고하자.1 . . 한편 위의 식을 연립방정식의 관점에서 바라보자. . Xβ=yX beta = yXβ=y . $X in { mathbb n times k}$의 경우 $n &gt; k$이므로, 위 연립방정식의 해 $ beta$는 일반적으로 존재할 수 없다. 따라서 해 대신에 MSE를 극소화하는 새로운 목적 함수를 잡았다고 보면 좋다. . PCA . 한편, PCA의 최소화는 다음의 목적 함수로 구현된다. . min⁡w1n∑i=1n(∥xi∥2−2(w⋅xi)2+1), begin{aligned} min_{w} &amp; dfrac{1}{n} sum_{i=1}^n left( Vert x_i Vert^2 - 2(w cdot x_i)^2 + 1 right), end{aligned}wmin​​n1​i=1∑n​(∥xi​∥2−2(w⋅xi​)2+1),​ . $w in { mathbb R}^{k}$, $x_i in { mathbb R}^{k}$ | . 목적 함수의 괄호 부분은 아래와 같이 도출된다. . ∥xi−(w⋅xi)w∥2=∥xi∥2−2(w⋅xi)(w⋅xi)+∥w∥2=∥xi∥2−2(w⋅xi)2+1 begin{aligned} Vert x_i - (w cdot x_i) w Vert^2 &amp; = Vert x_i Vert^2 - 2 (w cdot x_i)(w cdot x_i) + Vert w Vert^2 &amp; = Vert x_i Vert^2 - 2(w cdot x_i)^2 + 1 end{aligned}∥xi​−(w⋅xi​)w∥2​=∥xi​∥2−2(w⋅xi​)(w⋅xi​)+∥w∥2=∥xi​∥2−2(w⋅xi​)2+1​ . 역시 개념적으로 말하면 PCA는 $k$ 차원의 벡터가 $n$ 개 있을 때, $n$ 개의 벡터들을 길이 1의 유닛 벡터 $w( in { mathbb R}^{k})$로 프로젝션할 때, 그 거리 제곱의 합을 최소화하는 $w$를 찾는 것이다. . Difference . MSE를 극소화한다는 점은 같지만, 목적 함수의 형태와 목적 함수를 극대화하는 선택 변수가 다르다. . Objective function . 회귀 분석부터 보자. 회귀 분석은 $n$ 차원의 $y$를 $k$ 차원의 컬럼 스페이스를 지닌 $X$의 컬럼 스페이스로 프로젝션하는 것이다. 이때 프로젝션되는 위치가 곧 MSE를 극소화해주는 가중치, $k$ 차원의 $ beta$가 된다. 위 그림에서 보듯이 $y$에서 X 평면의 거리를 최소화하는 방법은 평면으로 수선의 발을 내리는 것 이외에는 없다. 즉 프로젝션 자체가 거리 최소화가 된다. . 이제 PCA를 보자. PCA는 $X in { mathbb R}^{n times k}$가 있을 때 이를 투영할 길이 1의 또다른 벡터 $w( in { mathbb R}^{k})$를 찾는 것이다. PCA의 경우 타겟 변수 같은 것이 없다. 그저 $k$ 의 벡터가 $n$ 개 있을 때 이들을 투영한 거리가 최소화되도록 유닛 벡터 $w$를 잡는다. . Question Two . PCA에서 아이겐밸류와 아이겐벡터는 어떻게 등장하게 되나요? . 자세한 설명은 여기를 참고하면 된다. 핵심만 요약해보자. . PCA의 목적 함수를 최적화하는 벡터 $w$를 잡기 위해서 $X w$의 분산을 최대화하면 된다. | 분산은 $w^T Sigma w$가 된다. | $w cdot w =1$의 조건을 넣어 제약 아래의 극대화 조건을 찾으면 정확히 아이겐벨류와 아이겐벡터를 찾는 계산과 동일하다. | . 극대화 문제에서 도출되는 라그랑지 승수 $ lambda$는 아이겐밸류가 된다. 아이겐밸류 $k$ 개가 있을 때 이를 큰 순서대로 나열했다고 하자. 즉 $ lambda_1 &gt; lambda_2 &gt; dotsc &gt; lambda_k$. 이제 아이겐밸류가 큰 순서대로 아이겐벡터를 잡으면 MSE가 작은 순서대로 $w$를 택하는 것이 된다. . 여기서 PCA가 MSE를 극소화하는 문제에서 출발했지만 왜 차원 축소의 문제로 변했는지 알 수 있다. MSE 최소화라는 목적 함수에서 볼 때 이를 달성하는 $k$ 개의 아이겐벡터 중에서 임의로 $l(&lt;k)$ 개를 선택할 수 있게 해준다. . Eigenvectors in PCA . PCA 분석에서 등장하는 아이겐벡터는 너무 좋은 특징을 지니고 있다. 우선, 해당 아이겐벡터는 서로 직교한다. 즉, . wi⋅wj={1for i=j0for i≠jw^i cdot w^j = begin{cases} 1 &amp; text{for $i = j$} 0 &amp; text{for $i neq j$} end{cases}wi⋅wj={10​for i=jfor i​=j​ . 아마도 PCA를 2차원에 도해할 경우 아래와 같은 그림을 많이 봤을 것이다. . . 1개의 관찰이 2차원 벡터이므로 이 자료에서는 두 개의 주성분이 나올 것이다. 이 두 개의 주성분은 반드시 직교(orthogonal)해야 할까? 꼭 그렇다는 장담은 없다. 그런데 앞서 살펴본 MSE의 극소화 과정에서 도출되는 결과에 따라서 주성분들끼리는 반드시 직교해야 한다. 이른바 ‘주성분’이 대칭 행렬(분산-공분산 행렬)에서 나온 아이겐밸류이기 때문이다. 그림에서 보듯이 하나의 주성분(북동향)이 다른 주성분(북서향)보다 크다. 만일 여기서 차원축소를 한다면 첫번째 주성분을 택하게 될 것이다. 일반적으로 $n$ 개의 주성분이 있을 때 이중 $k$ 개를 택한다면, 분산이 큰 순서대로 $k$개를 택하면 된다. . Sub-question . PCA에서 차원 축소의 의미를 보다 구체적으로 설명해주실 수 있을까요? . 이렇게 보자. 앞서 보았던 대로 $X in { mathbb R}^{$는 $(n times k}$ 행렬다. 이를 행 벡터로 잘라서 보자. 즉, . X=[x1⋮xn], where xi=[x1, … ,xk].X = begin{bmatrix} x_1 vdots x_n end{bmatrix},~ text{where~} x_i = [x^1,~ dotsc~, x^k].X=⎣⎢⎢⎡​x1​⋮xn​​⎦⎥⎥⎤​, where xi​=[x1, … ,xk]. . 즉. 행 벡터는 $k$ 개의 피처를 지닌 하나의 관찰이 된다. 이제 앞서 우리가 구한 아이겐벡터를 여기에 적용해보자. 편의상 1개로 차원 축소를 한 경우를 가정하겠다. 이때 차원 축소에 활용되는 아이겐벡터는 아이겐밸류가 가장 큰 값에 해당하는 벡터 $w^1$라고 하자. $w^1 in { mathbb R}^{k}$이다. 이제, . Xw1=PC1n×1X w^1 = underset{n times 1}{ rm{PC}^1}Xw1=n×1PC1​ . 첫번째 PC로 변환된 $X$는 $k$ 개의 피처에서 1개의 피처로 변환된다. 만일 주성분(PC)으로 $l$ 개를 택했다면 아이겐밸류가 큰 순서대로 위와 같은 과정을 거치면 되겠다. 각 아이겐밸류와 아이겐벡터의 쌍이 $w^j$, $ lambda^j$라고 할 때, . Xw1=PC1 ⋮Xwl=PCl, where λ1&gt;…&gt;λl.  begin{aligned} X w^1 &amp; ={ rm PC}^1 &amp;~ vdots X w^l &amp; ={ rm PC}^l,~ text{where $ lambda^1 &gt; dotsc &gt; lambda^l$. } end{aligned}Xw1Xwl​=PC1 ⋮=PCl, where λ1&gt;…&gt;λl. ​ . 그리고 이 PC 값들을 모으면 $k$ 개에서 $l$ 개로 차원이 축소된 행렬을 얻을 수 있다. . [PC1, … ,PCl]n×l underset{n times l}{ [{ rm PC}^1,~ dotsc~,{ rm PC}^l] }n×l[PC1, … ,PCl]​ . Question Three . 그림으로 보다 쉽게 볼 수는 없을까요? . 그림으로 간단하게 살펴보자.2 . . 먼저 회귀 분석이다. $y$를 $x$에 대해서 회귀($y sim x$)하는 경우와 $x$를 $y$에 대해서 회귀($x sim y$)하는 경우는 다르다. . $y sim x$: 벡터 $y( in { mathbb R}^{n})$를 $x$ 평면으로 프로젝션한다. 여기서 $y$는 관찰의 $y$ 값만을 모아서 만든 벡터다. 즉, 개별 $y$를 $ alpha + beta x$ 벡터로 수선의 발을 내릴 때 이를 최소화하는 $ alpha$, $ beta$를 찾는 것이다. 그림에서 보라색이 나타내는 바와 같다. | $x sim y$: 벡터 $x( in { mathbb R}^{n})$를 $y$ 평면으로 프로젝션한다. 개별 $x$를 $ alpha + beta y$ 벡터로 수선의 발을 내릴 때 이를 최소화하는 $ alpha$, $ beta$를 찾는 것이다. 위의 그림에서 초록색이 이를 나타낸다. | . . PCA는 그림에서 보듯이 관찰에서 임의의 벡터 $w = (w_1, w_2)$로 프로젝션한다. 즉, 그림에서 분홍색이 나타내는 바와 같다. . Question Four . 하나는 지도 학습, 다른 하나는 비지도 학습으로 이해하면 될까요? . 얼추 맞는 이야기라고 생각한다. 기계 학습에 조예가 없어서 자신있게 답하기는 힘들다. 다만 회귀 분석이 일종의 정답지($y$)를 갖고 있는 형태이기 때문에 지도 학습이 될 것이고 PCA의 경우 정답지 없이 관찰 전체를 대상으로 하기 때문에 비지도 학습으로 볼 수도 있겠다. . Question Five . 회귀 분석과 PCA를 섞을 수 있는 방법은 없나요? . 왜 없겠는가! 이른바 PCA 회귀 분석이라는 게 있다. 지금까지 잘 따라왔다면 PCA 회귀 분석이 어떤 형태가 될지 쉽게 짐작할 수 있다. . 행렬 $X( in { mathbb R}^{n times k})$를 PCA를 통해서 $X’( in { mathbb R}^{n times l})$으로 축소한다. ($l &lt;k$) | $y$를 $X’$에 대해서 회귀한다(“Do Regression $y$ on $X’$”). | . 이런 흐름으로 진행되는 것이 PCA 회귀다. . 그런데 PCA 회귀는 그다지 많이 사용되지 않는다. 왜 그럴까? 보통 기계 학습에서는 회귀 분석을 예측(prediction)의 한 가지 방법으로 이해하곤 한다. 회귀 분석은 선형이기 때문에 사실 여타의 비선형 기계 학습 방법에 비해서 예측 기법으로서는 원시적인 방법이다. 하지만 회귀 분석은 높은 해석력을 지니고 있다. 만일 적절 $X$를 선정할 수 한다면, 인과관계를 해석하는 틀로서도 활용할 수도 있다. 회귀 분석에서 $ beta$를 살피고 이를 해석하는 경우가 많은 것이 이 때문이다. 그런데 PCA 회귀를 할 경우 이러한 회귀 분석의 장점이 거의 사라지고 만다. 축소된 PC를 해석하는 것이 쉽지 않기 때문이다. . 보다 상세한 내용은 여기를 참고하라. &#8617; . | 여기에서 가져왔다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/machine-learning/2019/12/30/five-q-pca.html",
            "relUrl": "/machine-learning/2019/12/30/five-q-pca.html",
            "date": " • Dec 30, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "PageRank as Markov Chain",
            "content": "tl;dr . 페이지와 브린이 창안한 페이지랭크 알고리듬은 수학적으로 보면 마르코프 체인의 극한 분포다. | . PageRank? . 페이지랭크 알고리듬의 핵심이 사실 마르코프 체인이다. 페이지와 브린의 논문&lt;/a&gt;에 보면 “random surfer”라는 표현이 나온다. 이게 딱 확률 과정(stochastic process)을 연상시키지 않나? 문득 이런 생각이 들더라. . 페이지와 브린의 논문 자체가 마르코프 프로세스를 연상시키지는 않는다. 논문을 보면 페이지랭크 스코어링을 위한 몇 가지 원칙들이 언급되어 있을 뿐이다. 더 많은 아웃바운드 링크를 지닌 웹페이지에 내 페이지가 연결되어 있을 때 내 페이지의 스코어를 계산할 때 해당 페이지의 중요성을 낮춘다. 많은 인바운드 링크를 지닌 페이지에 내 페이지가 연결되어 있다면 이 페이지의 중요성을 높인다. 대략 이런 내용들이다. 사실 페이지랭크를 계산하는 과정 자체는 재귀적인 내용을 포함하고 있기 때문에 직관적으로 이해하기는 쉽지 않다. 페이지랭크 스코어링의 함의를 추리고 그 계산 과정은 잘 돌아보지 않는다. 나도 그랬다. . 페이지랭크라는 스코어, 즉 한 웹 페이지가 링크로 구성된 웹 풍경에서 지니는 중요도라는 개념이 마르코프 체인의 극한 분포(limiting distribution)와 개념적으로 연결된다. 내용을 간단히 살펴보도록 하자. . PageRank Model . 일단 페이지와 브린의 논문에서 다룬 모델링의 세팅을 간단히 살펴보자. . $L_{ij}$: 인디케이터 함수다. $j to i$의 링크가 존재하면 1, 그렇지 않으면 0이다. | $m_j = sum_{k=1}^n L_{kj}$: $j$가 지닌 아웃바운드 링크의 합을 나타낸다. | . 이제 웹페이지 $i$의 Broken Rank $p_i$은 다음과 같다. . pi=∑j→ipjmj=∑k=1nLijpjmjp_i = sum_{j to i} dfrac{p_j}{m_j} = sum_{k=1}^{n}L_{ij} dfrac{p_{j}}{m_j}pi​=j→i∑​mj​pj​​=k=1∑n​Lij​mj​pj​​ . 즉, $i$ 웹사이트의 ‘Broken Rank’는 $i$로 연결된 페이지들의 Broken Rank에 의해 정해진다. 이를 계산하는 방법은 다음과 같다. 해당 페이지들의 Broken Rank가 높을수록 나의 랭크도 높아진다. 해당 페이지가 더 많은 링크를 가질수록 그 Broken Rank는 낮게 평가된다. $m_j$로 나눠진 부분이 이를 반영한다. . 여기서 왜 굳이 “broken”이라는 표현을 썼는지는 잠시 후에 나오니 조금만 기다려주시라. . Broken rank as matrix . Broken rank를 매트릭스로 표현해보자. . p=[p1,…,pn]Tp = [p_1, dotsc, p_n]^Tp=[p1​,…,pn​]T . L=[L11…L1n⋮⋱⋮Ln1…Lnn]L = begin{bmatrix} L_{11}&amp; dotsc&amp; L_{1n} vdots&amp; ddots&amp; vdots L_{n1}&amp; dotsc&amp; L_{nn} end{bmatrix}L=⎣⎢⎢⎡​L11​⋮Ln1​​…⋱…​L1n​⋮Lnn​​⎦⎥⎥⎤​ . M=[m1…0⋮⋱⋮0…mn]M = begin{bmatrix} m_{1}&amp; dotsc&amp; 0 vdots&amp; ddots &amp; vdots 0&amp; dotsc&amp; m_{n} end{bmatrix}M=⎣⎢⎢⎡​m1​⋮0​…⋱…​0⋮mn​​⎦⎥⎥⎤​ . 이를 활용하면, . p=LM−1pp = LM^{-1} pp=LM−1p . $LM^{-1} = A$라고 두면 $A$가 마르코프 체인의 확률 행렬과 유사하다는 점을 쉽게 파악할 수 있다. . AT=[L11m1…Ln1m1⋮⋱⋮L1nmn…Lnnmn]A^T = begin{bmatrix} dfrac{L_{11}}{m_1}&amp; dotsc&amp; dfrac{L_{n1}}{m_1} vdots&amp; ddots&amp; vdots dfrac{L_{1n}}{m_n}&amp; dotsc&amp; dfrac{L_{nn}}{m_n} end{bmatrix}AT=⎣⎢⎢⎢⎢⎢⎡​m1​L11​​⋮mn​L1n​​​…⋱…​m1​Ln1​​⋮mn​Lnn​​​⎦⎥⎥⎥⎥⎥⎤​ . $A^T$를 들어다보자. 일단, 행을 더하면 1이 된다. 즉, $i$에서 $i$를 포함해서 어디론가는 가야 한다는 뜻이다. 다음으로 $L_ cdot$의 정의를 살펴보면 $i to j$의 이행확률은 $i$가 $j$로 향하는 링크를 지닐 경우는 $ frac{1}{m_i}$가 되고, 그렇지 않으면 0이다. 페이지와 브린이 정의한 random surfer라는 게 결국 이런 내용이다. $m_i$의 링크 중에서 특별히 선호하는 링크 없이 무작위로 하나를 골라서 나간다는 가정이 숨어 이는 셈이다. 아울러 해당 링크를 선택하는 데에는 현재의 가능성 이외에 그 전의 선택은 고려되지 않는다. 이점에서 이번 기의 선택에 바로 전기만 영향을 끼치는 마르코프 프로세스의 가정과 일치한다. . Why broken? . 왜 깨진 링크인가? 눈치가 빠른 사람이라면 마르코프 체인에서 해당 체인이 수렴하는 분포를 지니기 위한 조건을 알고 있을 것이다. 극한 분포를 지니려면 마르코프 확률 행렬이 우선 irreducible 행렬이어야 한다. . 이 조건을 말로 풀면 어떻게 될까? 해당 상태에서 언젠가는 다른 모든 상태로 갈 수 있어야 한다. 이를 웹사이트 간의 링크로 다시 풀어보자. A 사이트에서 B 사이트로 갈 수 없으면 안된다. 마르코프 체인에서 확률 행렬이 reducible 행렬이면 수렴하는 유일한 극한 분포를 찾을 수 없다. 다음의 예를 보자. . A=[0010010100010000010100010]A = begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 end{bmatrix}A=⎣⎢⎢⎢⎢⎢⎡​01000​00100​11010​00001​00010​⎦⎥⎥⎥⎥⎥⎤​ . 아이겐밸류 1에 해당하는 아이겐벡터는 두 개가 존재한다. 각각 . p=[13,13,13,0,0] or p=[0,0,0,12,12]p = [ dfrac{1}{3}, dfrac{1}{3} , dfrac{1}{3} , 0, 0 ] text{~or~} p = [0, 0, 0, dfrac{1}{2}, dfrac{1}{2} ]p=[31​,31​,31​,0,0] or p=[0,0,0,21​,21​] . 확률 행렬이 비기약 행렬(reducible matrix)이면, 웹 페이지에 관해서 유일한 극한 분포를 찾을 수 없다. 시작이 어디인지에 따라서 전혀 다른 극한 분포를 얻게 된다. 위의 예에서 만일 1,2,3 사이트에서 시작했다면 첫번째 극한 분포를, 4,5 사이트에서 출발했다면 두번째 극한 분포를 얻게 될 것이다. . PageRank score as share . 마르코프 체인을 다루는 분석에서 이런 경우는 종종 생길 수 있다. 이를 해결하는 가장 쉬운 방법은 어떤 상태에 고착되어 벗어날 수 없을 때, 벗어날 수 있는 어떤 기제를 마련해주는 것이다. 예를 들어, 플레이어가 일정한 확률로 실수를 하게 허용한다거나 하는 이론적인 장치가 많이 활용된다. . 페이지랭크 역시 비슷한 해법을 모델에 넣었다. 링크가 없더라도 일정한 확률로 다른 모든 링크로 넘어갈 확률을 임의로 부여하는 것이다. 이를 반영한 페이지랭크의 정의는 아래와 같다. . pi=1−dn(A)+1−dnd∑j→ipjmj(B)=1−dn+d∑k=1nLijpjmjp_i = overset{(A)}{ dfrac{1-d}{n}} + overset{(B)}{ vphantom{ dfrac{1-d}{n}} d sum_{j to i} dfrac{p_j}{m_j}} = dfrac{1-d}{n} + d sum_{k=1}^{n}L_{ij} dfrac{p_{j}}{m_j}pi​=n1−d​(A)​+n1−d​dj→i∑​mj​pj​​(B)​=n1−d​+dk=1∑n​Lij​mj​pj​​ . $d$는 어떤 사이트에서 링크가 존재하는 다른 사이트로 옮겨갈 확률을 의미한다. 따라서 $(1-d)$는 링크 유무와 관계 없이 $n$ 개의 사이트 중 임의의 다른 사이트로 옮겨갈 확률이다. . $(A)$: 링크 유무와 관계없이 ($i$ 자신을 포함해) 어디선가 $i$로 올 확률에 기반한 스코어링이다. 이때 스코어 값은 1이다. | $(B)$: $i$로 오는 링크를 지닌 사이트 중에서 $i$로 올 확률에 기반한 스코어링이다. 각각의 스코어링 값은 $p_j$가 된다. | . 이를 행렬로 나타내면 다음과 같다. $p_t$ 기의 페이지랭크가 주어져 있다면, 링크를 통한 페이지 이동을 거친 이후의 페이지랭크 $p_{t+1}$ 다음과 같다. . pt+1=(1−dn1n+dLM−1)⏞(∗)ptp_{t+1} = overbrace{ left( dfrac{1-d}{n} boldsymbol{1}_n + d L M^{-1} right)}^{(*)}p_{t}pt+1​=(n1−d​1n​+dLM−1) . ​(∗)​pt​ . $ boldsymbol{1}_n$은 모든 원소가 1인 $n times n$ 행렬이다. $(*)$은 기약 행렬(irreducible matrix)이고, $d&gt;0$이 만족하면 비주기 행렬(aperiodic matrix)이 된다. 특정한 사이트 $i$에서 다른 사이트로 갈 확률이 모두 양수가 되기 때문이다. . 이 경우 $()$을 확률 행렬로 지니는 마르코프 체인은 극한 분포 $p^$를 지니게 된다. 마르코프 체인의 논리에 따라서 $p^*$는 초기값 혹은 웹사이트의 초기 지분 $p_0$와 무관하다. . 이 극한 분포 $p^*$가 바로 페이지랭크다! 마르코프 체인의 관점에서 페이지랭크란 페론-프로베니우스 정리에 따라서 아이겐밸류 $1$에 해당하는 좌 아이겐벡터를 찾는 과정이다.1 물론 구글이 구사하는 실제의 알고리즘은 여기 적은 내용보다 훨씬 복잡하고 정교하다. 다만 그 핵심이 마르코프 체인이라는 사실을 기억해두자. . 극한 분포가 페이지랭크가 된다는 의미는 무엇일까? 마르코프 체인에서 극한 분포란 무작위로 상태를 옮겨가는 것이 무한이 반복되었을 때 도달하게 되는 분포다. 이는 충분히 긴 장기에 각 상태가 지니게 되는 ‘지분’(share)으로 이해할 수 있다. . 이러한 마르코프 체인에 기반을 둔 해석은 웹 사이트에도 잘 들어 맞는다. 그럴까? 개인의 입장에서는 그렇지 않다. 적어도 나는 그렇다. 구글에서 검색어를 친 뒤 검색 결과를 보면서 필요한 것을 찾아서 클릭, 클릭하기 때문이다. . 하지만 내가 누른 검색어를 친 사람이 많다면 어떨까? 특정 검색어에 관해서 수많은 웹 서퍼가 존재하고 이들이 각자의 취향대로 페이지를 옮겨다닌다면? 전체적으로 이는 특정 사이트에서 다른 사이트로 무작위로 이동한다는 마르코프 체인의 가정과 크게 다르지 않을 수 있겠다. 이러한 상태에서 각 사이트가 차지하는 ‘지분’ 혹은 점유율이 해당 웹사이트가 전체 웹 풍경에서 누리는 ‘중요성’이 될 것이다. . 아울러 앞서 페이지랭크의 정의가 동어반복처럼 느껴졌던 이유도 이제 알 수 있다. 이는 장기에 도달하게 되는 일종의 수렴 상태인 셈이다. 이렇게 마르코프 체인과 페이지랭크가 연결된다! . Reference . Page, Larry, “The PageRank Citation Ranking: Bringing Order to the Web,” http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf. ↩︎ . Tibshirani, Ryan, “PageRank,” https://www.stat.cmu.edu/~ryantibs/datamining/lectures/03-pr.pdf. . https://anarinsk.github.io/lie-pf1/을 참고하라. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/markov-chain/2019/12/24/PageRank.html",
            "relUrl": "/math/markov-chain/2019/12/24/PageRank.html",
            "date": " • Dec 24, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Perron-Frobenius Theorem, part 2",
            "content": "Input-out Model . Model . 투입-산출 모형을 생각해보자. $a_{ij}$는 $j$ 재화 1 단위를 생산하는 데 필요한 $i$ 재화의 양, 이라고 하자. 이렇게 되면, 열 벡터 $A_j$는 아래과 같다. 그 의미는 $j$ 재화 1단위를 생산하는 데 필요한 $1, dotsc, n$ 까지 필요한 재화의 양이다. . Aj=[a1j⋮anj]A_j = begin{bmatrix} a_{1j} vdots a_{nj} end{bmatrix}Aj​=⎣⎢⎢⎡​a1j​⋮anj​​⎦⎥⎥⎤​ . 여기서 두 가지를 더 가정하겠다. . 투입을 $k$ 배로 하면, 산출도 $k$ 배가 된다. 이를 규모수익불변(constant return to scale)이라고 한다. | 각 재화 생산은 다른 재화 생산에 영향을 주지 않는다. | 이제 $j$ 재화를 $x_j$ 만큼 생산하는 데 필요한 투입량은 $x_j A_j$가 된다. 생산하려는 생산량을 $x_1, dotsc, x_n$이라고 하면 충 투입 벡터는 아래와 같다. . x1A1+…+xnAn=Ax, where x=[x1,⋯ ,xn]Tx_1 A_1 + dotsc + x_n A_n = A x,~ text{where $x = [x_1, cdots, x_n]^T$}x1​A1​+…+xn​An​=Ax, where x=[x1​,⋯,xn​]T . Productive economy . 생산물 벡터 $x$를 생산하는 데 투입 벡터 $Ax$가 필요하다면 순 생산물 벡터는 아래와 같다. . x−Ax=(I−A)xx - Ax = (I-A)xx−Ax=(I−A)x . 순생산물 벡터 $b$를 얻게 위해서는 아래의 식이 성립해야 한다. . (I−A)x=b(I-A) x = b(I−A)x=b . 만일 $(I-A)$가 가역이라면 유일한 해 $x$가 존재하게 된다. 문제는 $x geq 0$를 보장할 수 있는지, 이다. 즉, 어떤 경제가 생산적이라는 정의는 아래와 같다. . x=(I−A)−1b≥0x = (I-A)^{-1} b geq 0x=(I−A)−1b≥0 . Productive matrix . 원래 바실리 레온티에프 (동지?)가 제시했던 생산적인 행렬 $A$의 정의는 살짝 더 엄격하다. 원래 $A$의 정의는 다음과 같다. . $(I- A)x &gt; 0$을 만족하는 비음의 벡터 $x$가 존재할 때, $A$를 생산적 매트릭스 혹은 레온티에프 메트릭스라고 정의한다. 이 정의는 매우 중요하다. 이후의 조건에서 결정적으로 활용되니 잘 기억해두도록 하자. . 잘 생각해보면 경제적인 의미가 있다. 뒤에 다시 나오겠지만, $A x$는 어떤 의미를 지닐까? 원하는 생산량이 $x$라면 이에 필요한 투입량이 $Ax$다. 즉, $x - Ax$란 다음 기 (목표) 산출에서 이를 위해 필요한 이번 기 투입 산출을 뺀 것이다. 이 값이 양이어야 한다는 것이다. 이 값이 음이라면 생산적이지 못한 것이다. 만일 $ boldsymbol{0}$ 벡터라면 연속적인 생산이 불가능한 상태가 된다. . Hawkins-Simon Condition . 이를 보장해주는 것이 호킨스-사이먼 조건(Hawkins-Simon condition) 이다. H-S 조건을 증명하기에 앞서 몇가지 렘마를 깔아보도록 하자. 앞으로 특별한 언급이 없는 이상 벡터와 매트릭스는 적당한 크기(차원)을 지니고 있다고 가정하겠다. . Lemma 1 . 두 개의 벡터 $x^1$, $x^2$가 있다고 하자. if $x^1 geq x^2$ and $A geq 0$, then $A x_1 geq A x_2$. . 증명은 간단하니 생략하도록 한다. 직접 계산해보면 된다. . Lemma 2 . 만일 $A$ 가 생산적 행렬이라면, $A^s$의 모든 원소는 $s to infty$에 따라서 0으로 수렴한다. . 증명해보자. 일단 A가 생산적 행렬이라는 조건에서, 아래와 같이 주장할 수 있다. . x&gt;Ax≥0x &gt; Ax geq 0x&gt;Ax≥0 . $A$와 $x$ 모두 비음이라는 점을 떠올리면 된다. 이것이 성립한다면, $0 &lt; lambda &lt; 1$의 $ lambda$가 존재하고 이는 다음을 만족한다. . λx&gt;Ax≥0 lambda x &gt; A x geq 0λx&gt;Ax≥0 . 여기에 다시 $A$를 앞 곱하면 다음과 같다. . λ(Ax)&gt;A2x≥0 lambda(Ax) &gt; A^2 x geq 0λ(Ax)&gt;A2x≥0 . 한편, 먼저 식에 이번에는 $ lambda$를 곱하면, 다음과 같다. . λ2x&gt;λ(Ax)≥0 lambda^2 x &gt; lambda(Ax) geq 0λ2x&gt;λ(Ax)≥0 . 이제 이렇게 곱한 두 결과를 합치면, 다음과 같다. . λ2x&gt;A2x≥0 lambda^2 x &gt; A^2 x geq 0λ2x&gt;A2x≥0 . 이를 일반화하면, . λkx&gt;Akx≥0 lambda^k x &gt; A^k x geq 0λkx&gt;Akx≥0 . $k to infty$에 따라서, $ lambda^k to 0$ 가 되기 때문에 . lim⁡k→∞Akx=0 lim_{k to infty} A^k x =0k→∞lim​Akx=0 . 벡터 $A^k x$의 $i$ 번째 벡터는 다음과 같다. . lim⁡k→∞∑j=1naijkxj=0 lim_{k to infty} sum_{j=1}^n a^k_{ij} x_j = 0k→∞lim​j=1∑n​aijk​xj​=0 . 이는 임의의 $x_j geq 0$에 대해서 항상 성립해야 한다. 따라서, $ lim_{k to infty} a^k_{ij} = 0$이 성립해야 한다. . Lemma 3 . 만일 $A$가 생산적 행렬이고 어떤 $x$에 대해서 $x geq Ax$가 성립하면, $x geq 0$ . 앞서 Lemma 2와 비슷한 논리로 다음과 같이 적을 수 있다. . x≥Ax≥A2x≥⋯≥Akx.x geq Ax geq A^2x geq dotsb geq A^k x.x≥Ax≥A2x≥⋯≥Akx. . 즉, $x geq A^k x$. $k to infty$일 때 Lemma 2에 따라서, . x≥lim⁡k→∞Akx=0.x geq lim_{k to infty} A^k x = 0.x≥k→∞lim​Akx=0. . Lemma 4 . If $A$ 가 생산적 행렬이라면, $(I-A)$ is non-singular. . 증명은 간단하다. $(I-A)$가 singular 라고 가정하자. 그렇다면, $x neq 0$이면서, $(I-A) x = 0$인 $x$가 존재하게 된다. $(I-A)(-x) = 0$ 역시 마찬가지라고 성립한다. $-x geq 0$가 성립해야 하는데, 이를 만족시키는 $x$는 $x = 0$가 유일하다. 이는 $x neq 0$ 전제와 모순된다. . Theorem 1 . 비음의 벡터 $d$가 있을 때, $A$가 생산적일 때, . (I−A)x=d(I-A)x = d(I−A)x=d . 는 유니크한 비음의 해를 지닌다. . Proof . Lemma 4에 따르면, $(I-A)$는 비특이(nonsingular) 행렬이다. 따라서 위 시스템의 해 $x^*$는 유니크하다. 아울러, $d geq 0$이므로, . (I−A)x∗≥0(I-A) x^* geq 0(I−A)x∗≥0 가 성립한다. $A$는 생산적이고 위 식이 성립하므로 Lemma 3에 따라서 $x^* geq 0$. . Hawkins-Simon Condition . A는 생산적 | 행렬 $(A-I)^{-1}$이 존재하며, 비음 | $B = I - A$ 모든 주 행렬식이 양수 | 보통 호킨스-사이먼 조건은 3번을 뜻한다. 3과 2번이 동치를 밝히는 것을 호킨스-사이먼 정리로 지칭한다. . 먼저 1과 2가 동치임을 증명해보자. . 1 → 2 . A가 생산적 행렬이며, Lemma 4에 의해서 $(I-A)^{-1}$이 존재함을 알 수 있다. . Φs=I+A+…+As Phi_s = I + A + dotsc + A^sΦs​=I+A+…+As . 라고 하자. 이때, . AΦs=A+A2+…+As+1.A Phi_s = A + A^2 + dotsc + A^{s+1}.AΦs​=A+A2+…+As+1. . (I−A)Φs=I−As+1.(I - A) Phi_s = I - A^{s+1}.(I−A)Φs​=I−As+1. . 앙변에 극한값을 취하면, . lim⁡s→∞[(I−A)Φs]=I lim_{s to infty} [(I-A) Phi_s ] = Is→∞lim​[(I−A)Φs​]=I . 이 성립한다. 이는 Lemma 2에 의해서 $ lim_{s to infty} A^{s+1} to 0$가 성립하기 때문이다. . lim⁡s→∞Φs=(I−A)−1 lim_{s to infty} Phi_s = (I-A)^{-1}s→∞lim​Φs​=(I−A)−1 . $A geq 0$이기 때문에, $ Phi_s geq 0$가 항상 성립한다. 따라서, . (I−A)−1≥0(I-A)^{-1} geq 0(I−A)−1≥0 . 1 ← 2 . $(I-A)^{-1} geq 0$가 성립한다고 하자. 임의의 $d &gt; 0$를 잡으면 $x = (I-A)^{-1} d geq 0$ 가 성립한다. 이때, . x=Ax+d&gt;Axx = A x + d &gt; Axx=Ax+d&gt;Ax . 가 성립하므로 1이 만족한다. . 나머지 증명은 테크니컬한 문제이므로 생략하자. . Eigenvalues &amp; Eigenvectors . 이제 생산적 행렬의 아이겐밸류와 아이겐벡터를 고민해보도록 하자. 생산적 행렬의 정의 $x - Ax &gt; 0$에서 출발해보자. 만일 아이겐밸류 $ lambda$가 존재한다면 그 값은 어때야 할까? 즉, . λx−Ax=λx−x&lt;0 lambda x - Ax = lambda x - x &lt; 0λx−Ax=λx−x&lt;0 . 따라서 $ lambda &lt; 1$이 된다. 아울러, 비음 행렬의 P-F 정리에 따르면 가장 아이겐밸류가 존재할 때 가장 큰 값 $ lambda_{ rm pf}$가 존재하는데, 이 역시 $ lambda_{ rm pf} &lt; 1$이 된다. 따라서 이에 상응하는 아이겐벡터 $x_{ rm pf}$는 다음을 만족시켜야 한다. $(I-A) x_{ rm pf} geq 0$. . (I−A)xpf=(λpfI−A)xpf+(1−λpf)xpf=(1−λpf)xpf≥0(I-A) x_{ rm pf} = ( lambda_{ rm pf} I - A ) x_{ rm pf} + (1 - lambda_{ rm pf}) x_{ rm pf} = (1- lambda_{ rm pf}) x_{ rm pf} geq 0(I−A)xpf​=(λpf​I−A)xpf​+(1−λpf​)xpf​=(1−λpf​)xpf​≥0 . 식의 마지막은 각각 $ lambda_{ rm pf} &lt; 1$ 그리고 $x_{ rm pf} geq 0$로부터 성립한다. . Some Applications . Steady-state growth rate . 투입계수 행렬 $A$를 지닌 어떤 경제를 상정하자. $A$가 생산적 행렬이라고 하자. 그리고 금기 초의 투입물 벡터를 $x_0$라고 하자. 금기 말에 생산물 벡터를 $x_1$이라고 하면, 아래의 관계가 성립한다. . x0=Ax1x_0 = A x_1x0​=Ax1​ . 보통 함수를 적을 때 왼쪽에 산출을 오른쪽에 투입을 적어서 약간 의아할 수 있다. 하지만, . A=[A1,…,Ai,…,An]A = [A_1, dotsc, A_i, dotsc, A_n]A=[A1​,…,Ai​,…,An​] . $A_i$는 $A$의 컬럼 벡터를 나타난다. $a_{ij}$는 $j$ 한단위를 생산하기 위해 필요한 투입 $i$를 나타낸다. 따라서, . Ax=a1x1+⋯+aixi+⋯+anxnAx = a_1 x_1 + dotsb + a_i x_i + dotsb + a_n x_nAx=a1​x1​+⋯+ai​xi​+⋯+an​xn​ . 는 $x_i$를 얻기 위해 필요한 투입 벡터들의 선형결합으로 이해하면 쉽다. 따라서 $x_0 = A x_1$이 성립한다. 이제 $x_0$가 $g$의 성장률로 성장한다고 하자. 즉, $x_0 = (1+g) x_0$이다. 이를 정리해서 쓰면 다음과 같다. . Ax0=11+gx0A x_0 = dfrac{1}{1+g} x_0Ax0​=1+g1​x0​ . 이때 $ dfrac{1}{1+g}$는 아이겐밸류, $x_0$는 아이겐벡터임을 알 수 있다. $A$가 생산적 행렬이기 때문에 아이겐밸류는 $[0,1)$ 사이에 존재하게 된다. 따라서, $g = dfrac{1}{ lambda}-1 &gt; 0$로 성장하게 된다. . Equilibrium price . 투입계수 행렬이 $A$인 경제를 생각하자. 각 재화를 생산하는 기업의 이윤율이 모두 $ pi$로 동일하다고 하자. 이때 가격 백터 $p = [p_1, dotsc, p_n]^T$가 균형 가격이 되기 위한 조건은 무엇일까? 우선 $i$ 기업의 이윤을 따져보자. $i$ 기업이 생산물 1 단위를 생산하기 위해 필요한 원자재는 ($i$ 자신의 물건을 포함해) $n$ 개이고 그 각각 필요량은 $a_{1i}, dotsc, a_{ni}$가 된다. 이들을 시장가격 $p_ cdot$으로 조달해와야 하므로 1단위당 원자재의 가격은 $a_{1i} p_1 + dotsc + a_{ni} p_n$이 된다. 이제 원료 가격에 마크업 $ pi$를 붙이면 이것이 해당 기업이 설정한 균형 가격이 된다. 즉, . pi=(1+π)(a1ip1+…+anipn)p_i = (1+ pi)(a_{1i} p_1 + dotsc + a_{ni} p_n)pi​=(1+π)(a1i​p1​+…+ani​pn​) . 이를 행렬로 나타나면 다음과 같다. . p=(1+π)ATpp = (1 + pi) A^T pp=(1+π)ATp . 위 식에서 $p$는 $A^T$의 아이겐벡터이고 균등 이윤과 해당 아이겐밸류는 다음과 같다. . 11+π=λpf dfrac{1}{1+ pi} = lambda_{ rm pf}1+π1​=λpf​ . 아이겐벡터는 스케일링이 가능하다. 즉, 가격 벡터는 다루기 편한 방식으로 정규화화면 된다.1 . 통상적인 경우 임의의 재화 하나의 가격을 1로 둔다. 이런 재화를 단위재(numeraire)라고 부른다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2019/12/20/Peron-Frobenius-2.html",
            "relUrl": "/math/matrix-theory/2019/12/20/Peron-Frobenius-2.html",
            "date": " • Dec 20, 2019"
        }
        
    
  
    
        ,"post16": {
            "title": "Perron-Frobenius Theorem, part 1",
            "content": "tl;dr . 정칙 행렬(regular matrix)의 경우 페론-프로베니우스의 정리는 아이겐밸류와 아이겐벡터에 관해서 강력한 조건을 걸어준다. | 정칙 행렬의 경우 가장 큰 양의 유일한 실수 아이겐밸류가 존재하며, 이 값에 상응하는 아이겐벡터는 양이다. | 이 정리를 쓸모 있게 활용할 수 있는 사례는 마르코프 체인의 극한 분포다. 이때 가장 큰 아이겐밸류는 1이 되며, 좌 아이겐벡터와 우 아이겐벡터를 활용해 마르코프 과정의 극한 분포를 아이겐벡터로 손쉽게 구할 수 있다. | . Definitions . positive, nonnegative . 양(positive): 행렬의 모든 원소가 양의 값을 지닐 때 | 비음(nonnegative): 행렬의 모든 원소가 비음일 때 | . 벡터 $x$, $y$가 있을 때 $x &gt; y$는 $(x-y)$가 양이라는 뜻이다. 앞으로 벡터와 행렬에 대해서 $&gt;$ 그리고 $ geq$는 모두 원소-단위(element-wise)를 뜻한다. . Basic facts . If $A geq 0$ and $z geq 0$, then $A z geq 0$ | If $A&gt;0$ and $z geq 0$, then $A z &gt; 0$. 역도 성립한다. 즉, whenever $z geq 0$ with $z neq 0$ and $A z &gt; 0$, then $A &gt;0$ | . | If $x geq 0$ and $x neq 0$, $ pi = ( frac{1}{ boldsymbol{1}^T x}) x$와 같은 표준화된 형태를 확률 분포로 활용할 수 있다. $ pi$ 벡터에 속하는 원소 $i$는 $ pi_i = dfrac{x_i}{ sum_j x_j}$. | . | . Regular nonnegative matrices . $A in mathbb{R}^{n times n}$ 그리고 $A geq 0$를 가정하자. $A$는 다음의 조건을 만족할 때 정치 행렬(regular matrix)이라고 한다. 1보다 큰 정수 $k$에 대해서 $A^k &gt; 0$를 만족한다. . For graph matrices . 노드들 간의 그래프 관계를 가장 쉽게 나타낼 수 있는 것이 행렬이다. 즉, $i, j in { 1, 2, dotsc, n }$ 일 때, $A_{ij}&gt;0$이면 $i to j$의 엣지를 그릴 수 있음을 나타낸다. 이때 $(A^k)_{ij} &gt; 0$와 동치는 $j to i$ 로 연결되는 길이 $k$의 경로가 존재함을 의미한다. . 이때 $A$가 정칙 행렬이 의미는 무엇일까? 이는 모든 노드에서 다른 어떤 노드로 이동하는 임의의 $k$ 길이의 경로가 항상 존재한다는 뜻이다. . Examples . Not regular . [1101] begin{bmatrix} 1&amp; 1 0&amp; 1 end{bmatrix}[10​11​] [0110] begin{bmatrix} 0&amp; 1 1&amp; 0 end{bmatrix}[01​10​] . Regular . [110001100] begin{bmatrix} 1&amp; 1&amp; 0 0&amp; 0&amp; 1 1&amp;0&amp;0 end{bmatrix}⎣⎢⎡​101​100​010​⎦⎥⎤​ . 차이를 알겠는가? $A$가 레귤러 행렬이라면 $A geq 0$이더라도 $A^k &gt; 0$가 된다. . Perron-Frobenius theorem . 증명은 일단 생략하자. 그리 어렵지 않지만 페론-프로베니우스 정리 내용이 중요하기 때문에 이에 집중하겠다. . For regular matrices . $A geq 0$이고 $A$가 레귤러 행렬이면 아래를 모두 만족한다. . 아이겐밸류 $ lambda_{ rm pf}$는 실수이며 양이다. | 좌 아이겐벡터와 우 아이겐벡터 모두 양이다. | 다른 모든 아이겐밸류 $ lambda$에 대해서, $ lvert lambda rvert &lt; lambda_{ rm pf}$ | 아이겐밸류 $ lambda_{ rm pf}$의 근은 1개다. | $ lambda_{ rm pf}$의 좌 아이겐벡터, 우 아이겐벡터는 유일하다(unique). | . 물론 아이겐벡터는 아이겐스페이스에 속하므로 벡터 전체에 대한 스케일링이 가능하다. $ lambda_{ rm pf}$는 행렬 $A$의 페론-프로베니우스의 근(혹은 PF 아이겐밸류)라고 부른다. . For nonnegative matrices . $A geq 0$. . 아이겐밸류 $ lambda_{ rm pf}$는 실수이며 비음이다. | $ lambda_{ rm pf}$의 좌 아이겐벡터, 우 아이겐벡터 모두 비음이다. | 다른 이외의 아이겐밸류가 존재한다면, 해당 아이겐밸류 $ lambda$에 대해서, $ lvert lambda rvert leq lambda_{ rm pf}$ | 아이겐밸류, 아이겐벡터는 유일하지 않다. | . Markov chain . 페론-프로베니우스 정리가 가장 아름답게 활용되는 사례는 마르코프 체인 모델이다. 확률 과정(stochastic process) $X_0, X_1, dotsc, X_n$이 아래와 같은 과정을 따른다고 하자. . Prob(Xt+1=j∣Xt=i)=pij{ rm Prob}(X_{t+1} = j vert X_t =i) = p_{ij}Prob(Xt+1​=j∣Xt​=i)=pij​ . 즉, 이는 $i to j$의 확률, 즉 $i$ 상태에서 $j$ 상태로 옮겨갈 확률을 의미한다. 마르코프 체인의 특징은 $(t+1)$ 기의 상태를 결정하는 것은 오직 $t$ 기의 상태다. 즉, $t-k$ for $k=2, dots, t$ 는 $(t+1)$의 상태를 결정하는 데 영향을 주지 않는다. $P$는 이행 행렬(transition matrix) 혹은 확률 행렬(stochastic matrix)라고 부른다. . P=[p11p12…p1np21p22…p2n⋮⋮⋱⋮pn1pn2…pnn]P = begin{bmatrix} p_{11}&amp; p_{12}&amp; dotsc&amp; p_{1n} p_{21}&amp; p_{22}&amp; dotsc&amp; p_{2n} vdots &amp; vdots&amp; ddots&amp; vdots p_{n1}&amp; p_{n2}&amp; dotsc&amp; p_{nn} end{bmatrix}P=⎣⎢⎢⎢⎢⎡​p11​p21​⋮pn1​​p12​p22​⋮pn2​​……⋱…​p1n​p2n​⋮pnn​​⎦⎥⎥⎥⎥⎤​ . $P$의 각 행의 합은 1이 된다는 점을 새겨두자. $t$ 기에 $i$ 상태에 있었다면, $(t+1)$ 기에는 $1, 2, dotsc, n$ 중 어느 하나로는 상태를 변경해야 한다. . 행 벡터 $p_t in mathbb{R}^n$ 를 $X_t$의 분포라고 하자. . (ptT)i=Prob(Xt=i){(p_t^T)}_i = { rm Prob}(X_t = i)(ptT​)i​=Prob(Xt​=i) . 를 의미한다. 즉, $t$ 기에 $i$ 상태가 실현될 혹은 존재할 확률이다. $(t+1)$ 기의 확률 분포를 벡터로 표현하면 다음과 같다. $p_{t+1} = p_t P$. . 확률 행렬를 활용해서 마르코프 체인의 문제를 어떻게 풀까? 일단 적당한 형태의 확률 행렬 $P$가 있다고 하자. 즉 $P$는 비음이고 $P^k &gt; 0$이 성립한다($P$는 레귤러 행렬). 이제 $P$에 페론-프로베니우스의 정리를 적용할 수 있다. 즉, . 아이겐밸류 $ lambda_{ rm pf}$는 실수이고 양수이며 유일하다. | 좌 아이겐벡터, 우 아이겐벡터 모두 양수이고 유일하다(unique). | $ lambda_{ rm pf}$를 제외한 다른 모든 아이겐밸류 $ lambda$에 대해서, $ lambda_i &lt; lambda_{ rm pf}$ for $i neq { rm pf}$. | . $P$라는 확률 과정에 대해서 아래의 두 사실을 증명하도록 하자. . 아이겐밸류 1이 존재한다. | 1이 가장 큰 유일한 아이겐밸류, 즉 $ lambda_{ rm pf} = 1$. | 이것이 증명되면 마르코프 체인의 극한 분포를 찾는데, 페론-프로베니우스의 정리를 활용할 수 있다. . Eigenvalue 1 exists! . 1의 아이겐밸류가 존재한다. 어떻게 알 수 있을까? . P1n=(1)1nP { boldsymbol 1}_n = (1){ boldsymbol 1}_nP1n​=(1)1n​ . 확률 행렬 $P$에 대해서 모든 원소가 1인 $n$ 차원의 열 벡터 ${ boldsymbol 1}_n$에 관해서 위의 식이 당연히 성립한다. 즉, 확률 행렬 $P$의 우 아이겐벡터는 ${ boldsymbol 1}_n$이고 그때의 아이겐밸류는 $1$이다. 그러면 아이겐밸류 1에 해당하는 좌 아이겐벡터 $ pi$ 가 존재한다고 하자. 이를 적으면 다음과 같다. . πP=π(1) pi P = pi (1)πP=π(1) . 앞서 페론-프로베니우스의 정리에 따라서 아이겐밸류 1에 해당하는 유일한 좌 아이겐벡터 $ pi$가 존재한다. 이 좌 아이겐벡터는 당연히 아이겐벡터의 특성을 지니고 있다. 즉, 어떤 상태 $ pi$에서 한번의 확률 과정을 거치더라도 여전히 그 상태에 머물러 있게 된다. . 1 is the largest eigenvalue! . 페론-프로베니우스의 정리에 따라서 아이겐밸류 1이 가장 큰 아이겐밸류라면, 이에 상응하는 아이겐벡터 $ pi$는 양이며 유일하다. $ pi$는 스케일링이 가능하기 때문에 스케일링을 거치면 $P$에 의해 표현되는 마르코프 체인 확률 과정의 무한 반복, 즉 $P^ infty$가 수렴하는 유일한 분포가 된다. 어떻게 증명할까? 생각보다 쉽다. . Proof . 만일 1 이외의 아이겐밸류 $ hat lambda &gt; 1$가 존재한다고 하자. 이제 어떤 열 벡터 $x$에 속하는 최대값을 $x_{ max}$라고 하자. 이때 $P x$의 결과 생산되는 열 벡터의 각 원소는 $x_i$ for $i = 1, 2, dotsc, n$의 컨벡스 결합이다.1 따라서 $Px = x^c$에 속한 어떤 원소 $x_i^c$도 $x_{ max}$ 보다 클 수 없다. 즉, . xic≤xmax⁡x^c_i leq x_{ max}xic​≤xmax​ . 그런데 $P x = hat lambda x$가 성립하고 $ hat lambda &gt;1$이기 때문에, $x_{ max} hat lambda &gt; x_{ max}$가 성립해야 한다. 즉 모순이다. 따라서 $ lambda leq 1$이고, $ lambda_{ rm pf} = 1$이다. . 확률 행렬 $P$, 아이겐밸류 1에 상응하는 좌 아이겐밸류 $ pi^*$라고 하자. 페론-프로베니우스의 정리 활용하면 아래와 같다. . 좌 아이겐벨류 $ pi^*$는 유일하고, 모든 원소는 실수이며 양이다. | 이를 적절하게 스케일링하면 분포가 된다. | 마르코프 체인의 초기 상태와 무관하게 확률 과정은 이 분포로 수렴한다. | . 보통 마르코프 체인을 설명할 때 좌 아이겐밸류와 우 아이겐밸류를 구별하지 않는 경우가 있다. 이는 여러모로 손해다. $P$의 정의를 확인하고, 위와 같이 정의한 경우라면 1에 대응하는 좌 아이겐벡터가 극한수렴 분포가 된다. 반대로, 확률 행렬을 $P^T$로 정의했다면 우 아이겐벡터가 극한 분포다. . Definition of limiting distribution . 확률 분포 $ pi = [ pi_0, pi_1, pi_2, dotsc]$는 아래 조건을 만족할 때 마르코프 체인 $P_n$의 극한 분포라고 부른다. 만일 . πj=lim⁡n→∞P(Xn=j∣X0=i), ∀i,j∈S pi_j = lim_{n to infty} P(X_n = j | X_0 = i) ,~ forall i, j in Sπj​=n→∞lim​P(Xn​=j∣X0​=i), ∀i,j∈S . 이고 . ∑j∈Sπj=1. sum_{j in S} pi_j = 1.j∈S∑​πj​=1. . 몇 가지 점만 확인해보자. . 극한 분포는 초기 상태에 의존하지 않는다. | 만일 극한 분포가 존재한다면, 아래 같은 식이 성립한다. | . lim⁡n→∞Pn=[π⋮π] lim_{n to infty} P^n = begin{bmatrix} pi vdots pi end{bmatrix}n→∞lim​Pn=⎣⎢⎢⎡​π⋮π​⎦⎥⎥⎤​ . Limiting Behavior of Markov Chain . 마르코프 체인은 어떻게 $ pi^*$로 수렴하게 될까? 확률 행렬 $P$의 아이겐밸류 $1 = lambda_1 &gt; lambda_2 geq dotsc geq lambda_n &gt; 0$, 각각에 대응하는 아이겐벡터를 $v_1, v_2, dotsc, v_n$이라고 하자. 아이겐벡터가 각각 선형독립이라고 가정하자. 이때 아이겐벡터로 구성된 행렬 $Q$는 가역이다. $ pi^T_t = Q y_t$ 로 나타낼 수 있다. 즉 $Q$의 선형결합을 통해 $n$ 차원의 임의의 벡터를 표현할 수 있다. $ pi$가 열 벡터이기 떄문에 이를 행 벡터로 바꿨다는 점에 유의하자.2 한편, . πt+1T=PTπtTQyt+1=PTQtytQ−1Qyt+1=Q−1PTQtytyt+1=Dyt begin{aligned} pi_{t+1}^T &amp; = P^T pi_t^T Q y_{t+1} &amp; = P^T Q_{t} y_t Q^{-1} Q y_{t+1} &amp; = Q^{-1}P^T Q_{t} y_t y_{t+1} &amp; = D y_t end{aligned}πt+1T​Qyt+1​Q−1Qyt+1​yt+1​​=PTπtT​=PTQt​yt​=Q−1PTQt​yt​=Dyt​​ . 두 가지를 상기하자. . 행렬 $A$와 전치 행렬 $A^T$는 동일한 아이겐밸류를 지니게 된다.3 | D는 아이겐밸류로 구성된 대각행렬이다. 편의상 크기 순서대로 나열되어 있다고 하자. | 앞서 본 바에 따라서 $ lambda_1 =1$이다. . yt=[λ1…0⋮⋱⋮0…λn]yt−1.y_t = begin{bmatrix} lambda_1&amp; dotsc&amp; 0 vdots&amp; ddots&amp; vdots 0&amp; dotsc&amp; lambda_n end{bmatrix} y_{t-1}.yt​=⎣⎢⎢⎡​λ1​⋮0​…⋱…​0⋮λn​​⎦⎥⎥⎤​yt−1​. . 이 차분방정식의 해는 아래와 같다. . yt=[λ1…0⋮⋱⋮0…λn]ty0=[λ1t…0⋮⋱⋮0…λnt]y0y_t = begin{bmatrix} lambda_1&amp; dotsc&amp; 0 vdots&amp; ddots&amp; vdots 0&amp; dotsc&amp; lambda_n end{bmatrix}^t y_{0} = begin{bmatrix} lambda_1^t&amp; dotsc&amp; 0 vdots&amp; ddots&amp; vdots 0&amp; dotsc&amp; lambda_n^t end{bmatrix} y_0yt​=⎣⎢⎢⎡​λ1​⋮0​…⋱…​0⋮λn​​⎦⎥⎥⎤​ty0​=⎣⎢⎢⎡​λ1t​⋮0​…⋱…​0⋮λnt​​⎦⎥⎥⎤​y0​ . $y_0 = [c_1, dotsc, c_n]^T$라고 두면, $y_t = [c_1 lambda_1^t, dotsc, c_1 lambda_n^t]^T$가 된다. . πtT≡Qyt=c1λ1tv1+⋯+c1λntvn. pi_t^T equiv Q y_t = c_1 lambda_1^t v_1 + dotsb + c_1 lambda_n^t v_n.πtT​≡Qyt​=c1​λ1t​v1​+⋯+c1​λnt​vn​. . 이제 $t to infty$를 적용해보자. $ lambda_i &lt; lambda_1 = 1$ for $i = 2, 3, dotsc, n$이므로, $ pi_{ infty}^T = c_1 v_1$가 된다. $v_1$이 표준화된 확률 분포 벡터이므로 $c_1 =1$이어야 한다. . Appendix: Irreducibility and Aperiodicity . 마르코프 체인이 수렴하는 분포를 갖게 될 조건으로 보통 두 가지 조건, 기약성(irreducibility) 그리고 비주기성(aperiodicity)이 제시된다. 먼저 간단히 둘의 내용을 살펴보자. . Irreducible matrix . 기약성의 정의는 아래와 같다. . $P$가 확률 행렬이라고 할 때, 상태 $x$, $y$에 대해서 양의 실수 $j$, $k$가 존재하면, 두 상태는 서로 교류할 수 있다고 칭한다. . Pj(x,y)&gt;0  and  Pk(y,x)&gt;0P^j (x, y) &gt; 0~~ text{and}~~P^k(y,x) &gt; 0Pj(x,y)&gt;0  and  Pk(y,x)&gt;0 . 이 정의의 의미는 무엇일까? 일정한 상태를 거치면 $x to y$ 그리고 $y to x$로 옮기는 것이 확률적으로 가능하다는 뜻이다. 기약성이란 모든 상태가 교류할 수 있는 상태를 뜻한다. 다시 말하면 어떤 상태에 들어가서 여기서 빠져나올 수 없는 경우가 발생한 가능성이 없을 때 기약성이 성립한다. . Irreducible . [0.90.10.00.40.40.20.10.10.8] begin{bmatrix} 0.9&amp; 0.1&amp; 0.0 0.4&amp; 0.4&amp; 0.2 0.1&amp; 0.1&amp; 0.8 end{bmatrix}⎣⎢⎡​0.90.40.1​0.10.40.1​0.00.20.8​⎦⎥⎤​ . Reducible . [1.00.00.00.10.80.10.00.20.8] begin{bmatrix} 1.0&amp; 0.0&amp; 0.0 0.1&amp; 0.8&amp; 0.1 0.0&amp; 0.2&amp; 0.8 end{bmatrix}⎣⎢⎡​1.00.10.0​0.00.80.2​0.00.10.8​⎦⎥⎤​ . Aperiodic matrix . 대충 말하면 마르코프 체인 위의 이동이 예측 가능한 형태로 이루어질 수 있으면 안된다. 먼저 예를 보도록 하자. . [010001100] begin{bmatrix} 0&amp; 1&amp; 0 0&amp; 0&amp; 1 1&amp; 0&amp; 0 end{bmatrix}⎣⎢⎡​001​100​010​⎦⎥⎤​ . . 각 상태가 일정한 간격으로 존재하게 된다. 즉, . 1번, 2번, 3번 상태는 ${ k, k+3, k+6, dotsc }$ 번째에 존재하게 된다. | . 주기성(periodicity)의 수학적인 정의는 다음과 같다. . k=gcd{n&gt;0 ∣ Pr(Xn=i∣X0=i)&gt;0}k = { rm gcd} { n &gt; 0~|~{ rm Pr}(X_n = i | X_0 = i) &gt; 0 }k=gcd{n&gt;0 ∣ Pr(Xn​=i∣X0​=i)&gt;0} . gcd란 greatest common divisor, 즉 최대공약수를 의미한다. 만일 해당 상태로 ${ 6, 8, 12, dotsc }$ 번에 돌아갈 확률이 양이라면, gcd는 2가 된다.4 . 이때 $k &gt; 1$이면 주기 행렬이고 $k=1$이면 비주기 행렬이다. $k=1$의 의미는 무엇일까? $t$ 기에 상태 $s$에 있을 때, $t+1$ 기에 역시 $s$에 있을 확률이 양이라는 뜻이다. 그리고 확률 행렬을 구성하는 모든 상태에 주기성이 없을 때, 이러한 확률 행렬을 비주기 행렬이라고 한다. . Regular matrix vs irreducible and aperiodic matrix . 페론-프로베니우스 정리에 따르면 정칙 행렬일 때 극한 분포가 존재하게 된다. 정칙 행렬과 기약 행렬, 비주기 행렬 사이의 수학적 관계는 어떨까? 결론부터 말하면, 둘은 동치다. 증명은 간단하다. . A: 정칙 행렬 | B: 기약 행렬이면서 비주기 행렬 | . A → B . Irreducible . 정칙 행렬이면 $P^k &gt; 0$이다. $P^{k+1}$의 $i$ 행 $j$ 열의 원소는 다음과 같다. . pijk+1=∑t=1npitptjk&gt;0p_{ij}^{k+1} = sum_{t=1}^{n} p_{it} p_{tj}^k &gt; 0pijk+1​=t=1∑n​pit​ptjk​&gt;0 . 위의 식이 성립하는 이유는 정칙 행렬의 경우 적어도 하나의 $t$에 관해서 $p_{it} &gt; 0$이 성립해야 한다. 만일 이것이 성립하지 않으면 정칙 행렬이 될 수 없다. 즉, $P^{k+1} &gt;0$이고, $P^{k+2}, P^{k+3}, dotsc$모두 양이다. . Aperiodic . ${k ,k+1, k+2, dotsc}$의 gcd는 1이다. . A ← B . 기약 행렬이면 언제나 정칙 행렬이다. . Comments . Irreducible and aperiodic . 만일 확률 행렬이 기약이면 이때 비주기성 여부를 어떻게 확인할까? 행렬이 기약 행렬이라면, 해당 행렬을 구성하는 하나의 상태만 비주기성을 지니면 전체 행렬이 비주기성을 지닌다. 증명은 조금만 생각해보시라. . Why Aperiodicity? . 극한 분포 존재에 비주기성은 왜 필요할까? 만일 이 조건이 성립하지 않으면 극한 분포로 계산된 것이 사실은 극한 분포라고 할 수 없게 된다. 즉, . lim⁡n→∞Pn≠[π⋮π]. lim_{n to infty} P^n neq begin{bmatrix} pi vdots pi end{bmatrix}.n→∞lim​Pn​=⎣⎢⎢⎡​π⋮π​⎦⎥⎥⎤​. . 예를 들어보자. 상태 2개이고 확률 행렬이 아래와 같다고 하자. . P=[0110]P = begin{bmatrix} 0 &amp;1 1 &amp; 0 end{bmatrix}P=[01​10​] 극한 분포를 계산하면 $[ dfrac{1}{2}, dfrac{1}{2}]$이 나온다. 상태 1에서 출발했다면, 상태 1이 ${2, 4, 6}$ 번에 나타나게 되고, 이때 gcd는 2가 된다. 이때 확률 행렬의 곱을 살펴보면 아래와 같다. $k = 1, 2, cdots$에 대해서 . Pn=[0110], for n=2k−1Pn=[1001], for n=2k begin{aligned} P^n &amp; = begin{bmatrix} 0 &amp; 1 1 &amp; 0 end{bmatrix},~ text{for}~n = 2k-1 P^n &amp; = begin{bmatrix} 1 &amp; 0 0 &amp; 1 end{bmatrix},~ text{for}~n = 2k end{aligned}PnPn​=[01​10​], for n=2k−1=[10​01​], for n=2k​ . 따라서 $ lim_{n to infty} P^n$은 수렴하지 않는다. 앞서 계산한 극한 분포는 두 극단의 평균일 뿐이다. 이때 극한 분포 역시 존재하지 않는다. . $p_{i1} x^c_1 + p_{i2} x^c_2 + dotsb + p_{in} x^c_n$ with $p_{i1} + p_{i2} + dotsc + p_{in} =1$ for $i = 1, 2, dotsc, n$ &#8617; . | 사실 기존의 $ pi$가 좌 아이겐벡터였다면 여기서는 이를 우 아이겐벡터로 바꾼 것이다. 물론, 확률 행렬 역시 $P^T$가 되어야 한다. &#8617; . | $ det(A^T− lambda I) = det((A− lambda I)^T)= det(A− lambda I)$ &#8617; . | 확률 행렬의 원소가 0, 1로만 되어 있지 않아도 주기 행렬이 될 수 있다는 사실에 유의하자. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2019/12/11/Peron-Frobenus-1.html",
            "relUrl": "/math/matrix-theory/2019/12/11/Peron-Frobenus-1.html",
            "date": " • Dec 11, 2019"
        }
        
    
  
    
        ,"post17": {
            "title": "Eigenvalues and Eigenvectors",
            "content": "tl;dr . 행렬 대각화를 이루는 방법 중 하나는 아이겐벡터와 아이겐밸류를 재구성하는 것이다. | 대각화가 가능한 행렬 $A$가 있고, 그 아이겐벡터를 열 벡터로 하는 행렬을 $Q$라고 하면 | . AQ=A[x1,…,xn]=[λ1x1,⋯ ,λnxn]=Qλ begin{aligned} A Q &amp; = A[x_1, dotsc, x_n] &amp; = [ lambda_1 x_1, cdots, lambda_n x_n] &amp;= Q boldsymbol{ lambda} end{aligned}AQ​=A[x1​,…,xn​]=[λ1​x1​,⋯,λn​xn​]=Qλ​ . AQQ−1=A=QλQ−1AQ Q^{-1} = A = Q boldsymbol{ lambda}Q^{-1}AQQ−1=A=QλQ−1 . 대각화가 가능하기 위해서는 우선 행렬 $Q$가 비특이(non-singular) 행렬이어야 한다. $Q$는 아이겐벡터의 조합이기 때문에 서로 다른 아이겐밸류를 지니거나 혹은 아이겐벡터의 선형독립이 성립해야 한다. | 대칭 행렬의 경우 모든 아이겐밸류가 실수이고, $Q$는 직교 행렬(orthogonal matrix)이 된다. 이 경우 | . A=QλQTA = Q boldsymbol{ lambda}Q^TA=QλQT . Definition . A(n×n)x(n×1)=λx, for x≠0 underset{(n times n)}{ boldsymbol{A}} underset{(n times 1)}{x} = lambda x, text{ for $x neq 0$}(n×n)A​(n×1)x​=λx, for x​=0 . 벡터 $x( in { mathbb R}^n)$가 있다고 하자. $ boldsymbol{A}$는 일종의 함수이고 이는 $x$를 변형시키게 된다. 이 변형이 그 다시 자신이 되고 벡터의 크기만 조정해주는 형태가 될 때, $ lambda$를 아이겐밸류 그리고 그 벡터 $x$를 아이겐벡터라고 한다. 잠깐! 여기서 아이겐 벡터는 우 아이겐벡터다. 왜냐하면, 벡터가 매트릭스의 오른쪽에 곱해지기 때문이다. 우(right) 아이겐벡터를 보통 아이겐벡터라고 쓴다. 하지만 좌(left) 아이겐벡터도 있다. 일단 이 점만 지적해두도록 하자.1 . Eigenspace . 벡터들의 집합 중에서 아래의 조건을 만족하면 이를 벡터 (부분) 공간이라고 부른다. . 영 벡터 $ boldsymbol 0$가 원소이다. | $x$가 원소일 때 $ alpha x$ ($ alpha in mathbb{R})$도 원소다. | $x$, $y$가 원소일 때 $x + y$도 원소다. | 쉽게 말해서 벡터 (부분) 공간 위에서는 덧셈과 스칼라 곱셈이 정의된다. 아이겐벡터로 구성된 공간은 이런 벡터 공간이 될까? . $A { boldsymbol 0} = lambda { boldsymbol 0}$ | $A( alpha x) = alpha(A x) = alpha ( lambda x) = lambda( alpha x)$ | $A(x_1 + x_2) = A x_1 + A x_2 = lambda x_1 + lambda x_2 = lambda(x_1 + x_2)$ | 3번의 경우 정의대로 하나의 아이겐밸류 $ lambda$에 대해서 두 개 이상의 아이겐벡터가 대응될 때에 해당한다.2 아이겐벡터를 원소로 하는 공간은 벡터 부분 공간이 되며, 이를 아이겐스페이스(eigenspace)라고 부른다. . Determinant and Eigenvalues . 아이겐밸류의 아이겐벡터를 구하는 과정은 다음과 같다. . ∣(A−λI)∣=0|( boldsymbol{A} - lambda I)| = 0∣(A−λI)∣=0 . 즉, 임의의아이겐밸류, 아이겐벡터 정의에서 $ boldsymbol 0$ 벡터가 아닌 매트릭스 $x$를 $ boldsymbol 0$ 벡터로 만들기 위해서는 위의 행렬식 값이 0이어야 한다. 행렬식이 $ lambda$의 $n$ 차 방정식이고, $n$차 방정식의 근이 각각 아이겐밸류가 된다. 즉, . ∣(A−λI)∣=(λ1−λ)=(λ2−λ)⋯(λn−λ)|( boldsymbol{A} - lambda { boldsymbol I})| = ( lambda_1 - lambda) = ( lambda_2 - lambda) dotsb ( lambda_n - lambda)∣(A−λI)∣=(λ1​−λ)=(λ2​−λ)⋯(λn​−λ) . 위의 식에서 두가지 사실을 알 수 있다. . $ boldsymbol{A} $의 행렬식은 아이겐밸류의 곱과 같다. 즉, $ lambda = 0$을 넣으면 이 결과를 쉽게 얻을 수 있다. | 가역행렬(invertible matrix)이 될 조건, 즉 역행렬이 존재할 조건은 행렬식의 값이 0이 아닌 경우다. 1에 따르면 이는 모든 아이겐밸류의 값이 0이 아닌 조건과 동치다. | Transpose . 원래 행렬의 행렬식과 전치행렬의 행렬식은 같다. 이를 아이겐밸류 계산에 응용해보자. . ∣AT−λI∣=∣(A−λI)T∣=∣A−λI∣| boldsymbol{A}^T - lambda { boldsymbol I}| = |( boldsymbol{A}- lambda { boldsymbol I})^T| = | boldsymbol{A}- lambda { boldsymbol I}|∣AT−λI∣=∣(A−λI)T∣=∣A−λI∣ . 즉, 원래 행렬 $ boldsymbol{A}$와 전치 행렬 $ boldsymbol{A}^T$는 같은 아이겐밸류를 갖는다. . Diagonalization . 아이겐밸류와 아이겐벡터가 가장 많이 사용되는 경우는 행렬의 대각화이다. 여기서 대각화란, 정방 행렬의 대각에 위치한 원소, $a_{ii}$를 제외한 나머지 원소가 모두 0인 행렬, 즉 대각 행렬을 품는 변형을 의미한다. 대각 행렬은 여러가지로 쓸모가 많다. 특히 행렬의 $k$ 제곱이 필요한 경우 대각행렬은 그냥 해당 대각원소의 $k$ 제곱과 동일해진다. 행렬의 대각화를 살펴보자. . y=Axy = boldsymbol{A} xy=Ax . 기본적으로 행렬은 함수다. 즉, 벡터 $x$를 투입(input)으로 보면 이를 선형 결합을 통해서 다른 어떤 산출(output) 벡터로 보내는 것이다. 일단 설명의 편의를 위해서 $x$가 2차원 벡터라고 두고 설명해보자. 즉, . x=[x1x2].x = begin{bmatrix} x_1 x_2 end{bmatrix}.x=[x1​x2​​]. . $ boldsymbol{A}$에 의해 변형된 결과를 아이겐벡터 $v_1$과 $v_2$를 통해서 표현할 수 있다고 가정하자. $v_1$, $v_2$를 통해 $x$, $y$를 표현하면 다음과 같다. . x=w1v1+w2v2y=z1v1+z2v2 begin{aligned} x &amp; = w_1 v_1 + w_2 v_2 y &amp; = z_1 v_1 + z_2 v_2 end{aligned}xy​=w1​v1​+w2​v2​=z1​v1​+z2​v2​​ . 이때, $w_ cdot$, $z_ cdot$은 스칼라 값임에 유의하자. 이를 매트릭스로 표시하면 다음과 같다. . x=Q[w1w2], y=Q[z1z2],wherex = boldsymbol{Q} begin{bmatrix} w_1 w_2 end{bmatrix},~ y = boldsymbol{Q} begin{bmatrix} z_1 z_2 end{bmatrix}, text{where}x=Q[w1​w2​​], y=Q[z1​z2​​],where . Q=[v1,v2] boldsymbol{Q} = begin{bmatrix} v_1, v_2 end{bmatrix}Q=[v1​,v2​​] . $v_i’ v_i = 1(i = 1,2)$ 이고, $v_1 cdot v_2 = 0$ 이라고 가정하자. 즉, $Q$가 직교행렬이라고 가정하자. . Qz=AQwz=Q−1AQwz=Q′AQw begin{aligned} boldsymbol{Q}z &amp; = boldsymbol{A} boldsymbol{Q} w z &amp; = boldsymbol{Q}^{-1} boldsymbol{A} boldsymbol{Q} w z &amp; = boldsymbol{Q}&amp;#x27; boldsymbol{A} boldsymbol{Q} w end{aligned}Qzzz​=AQw=Q−1AQw=Q′AQw​ . 직교행렬의 경우 $ boldsymbol{Q}^{-1} = boldsymbol{Q}^T$가 성립한다.3 이때 . AQ=[Av1,Av2]=[λ1v1,λ2v2]=[v1,v2][λ1,00,λ2]=Qλ. boldsymbol{A} boldsymbol{Q} = [A v_1, A v_2] = [ lambda_1 v_1, lambda_2 v_2] = [v_1, v_2] begin{bmatrix} lambda_1, 0 0, lambda_2 end{bmatrix} = boldsymbol{Q} boldsymbol{ lambda}.AQ=[Av1​,Av2​]=[λ1​v1​,λ2​v2​]=[v1​,v2​][λ1​,00,λ2​​]=Qλ. . 따라서, . z=QTQλwz = boldsymbol{Q}^T boldsymbol{Q} boldsymbol{ lambda} wz=QTQλw . 여기서 대각화의 핵심은 . QTAQ=λ or A=QλQT boldsymbol{Q}^T boldsymbol{A} boldsymbol{Q} = boldsymbol{ lambda} text{ or } boldsymbol{A} = boldsymbol{Q} boldsymbol{ lambda} boldsymbol{Q}^TQTAQ=λ or A=QλQT . 즉, 어떤 매트릭스의 아이겐벡터가 서로 직교하면 이를 통해 직교행렬 $ boldsymbol{Q}$를 얻을 수 있다. 이를 원래 매트릭스의 좌우로 곱하면 아이겐밸류를 대각원소로 갖는 대각행렬을 얻을 수 있다. . Generalization . 일반화 해보자. 대각화는 다음과 같다. . AQ=A[x1,…,xn]=[λ1x1,…,λnxn]=[x1,…,xn][λ1…0⋮⋱⋮0…λn]=Qλ boldsymbol{AQ} = boldsymbol{A} [x_1, dotsc, x_n] = [ lambda_1 x_1, dotsc, lambda_n x_n] = [x_1, dotsc, x_n] begin{bmatrix} lambda_1&amp; dotsc &amp; 0 vdots&amp; ddots&amp; vdots 0&amp; dotsc&amp; lambda_n end{bmatrix} = boldsymbol{Q} boldsymbol{ lambda}AQ=A[x1​,…,xn​]=[λ1​x1​,…,λn​xn​]=[x1​,…,xn​]⎣⎢⎢⎡​λ1​⋮0​…⋱…​0⋮λn​​⎦⎥⎥⎤​=Qλ . 행렬의 인수분해(factorization)은 다음을 의미한다. . A=QλQ−1 boldsymbol{A} = boldsymbol{Q} boldsymbol{ lambda} boldsymbol{Q}^{-1}A=QλQ−1 . Diagonalizable . 쉽게 생각하자. 우선 $Q^{-1}$이 존재해야 한다. 즉, $Q$가 비특이 행렬이어야 한다. $Q$가 비특이 행렬이 되기 위한 조건들은 일단 행렬 대각화를 위한 필요 조건이다. . 그런데 $Q$는 아이겐벡터들로 구성된 행렬이다. 따라서 $Q$가 비특이 행렬이 되기 위해서는 해당 아이겐밸류들이 모두 다르거나, 중복된 아이겐밸류가 있다면 아이겐벡터의 선형 독립이 성립해야 한다. 아이겐밸류가 모두 다르면, 즉 각기 다른 아이겐백터가 존재하면, 대각화가 가능하다. 만일 아이겐밸류가 중복이라면, 경우에 따라서 다르다. . Symmetric matrix . 행렬이 대칭이면 $Q$에 더 좋은 특성이 생긴다. . $ boldsymbol A$ 의 아이겐밸류는 모두 실수이다. | $Q$는 직교행렬이다. 즉, $Q^T = Q^{-1}$ 가 성립한다. | 이를 종합하면 아래와 같다. . A=QλQ−1=QλQT boldsymbol{A} = boldsymbol{Q} boldsymbol{ lambda} boldsymbol{Q}^{-1} = boldsymbol{Q} boldsymbol{ lambda} boldsymbol{Q}^{T}A=QλQ−1=QλQT . Graphic interpretation . 그림으로 이해하면 어떨까? 대각화라는 것은 행렬 $A$의 성분을 아이겐벡터로 재정렬해주기 위해서 축을 회전하는 것과 같은 개념이다. 이때 축의 스케일링을 담당하는 것이 가운데 대각 행렬이다. . . 좌 아이겐벡터와 우 아이겐벡터가 절묘하게 사용되는 사례는 마르코프 체인이다. 아이겐밸류 1이 존재하고, 그 아이겐벡터가 해당 상태의 극한 분포가 된다는 사실이 좌, 우 아이겐벡터를 번갈아 사용하면서 도출된다. 자세한 것은 여기를 참고하자. &#8617; . | 얼핏 생각하면 이런 경우가 있나 싶을 수 있다. 가장 좋은 사례는 항등 행렬, ${ boldsymbol I}_n$이다. 이 항등행렬의 경우 아이겐밸류는 1이고, 대응 가능한 어떤 벡터도 아이겐벡터가 된다. &#8617; . | 직교행렬의 경우 $ boldsymbol{Q}^T boldsymbol{Q} = boldsymbol{Q} boldsymbol{Q}^T = { boldsymbol I}$가 성립한다. 따라서 $ boldsymbol{Q}^{-1} = boldsymbol{Q}^T$ &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/matrix-theory/2019/12/03/eigenvalue-eigenvector.html",
            "relUrl": "/math/matrix-theory/2019/12/03/eigenvalue-eigenvector.html",
            "date": " • Dec 3, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "Understanding Logit Regression",
            "content": "tl;dr . 로짓 회귀는 우리가 알던 그 ‘회귀’가 아니야하고는 조금 달라! | 로짓 회귀는 아래와 같은 데이터 모형에 기댄반을 둔 추정법이다. | . ln⁡p(xi)1−p(xi)=xiβ ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} = boldsymbol{x_i} boldsymbol{ beta}ln1−p(xi​)p(xi​)​=xi​β . 회귀식의 계수 $ boldsymbol{ beta}$를 해석할 때 주의하자. 이를 통상적인 회귀분석의 계수(한계 효과)처럼 해석하려면 별도의 계산이 필요하다. | . 로짓 회귀, 제대로 알고 있나? . 잘 알고 있다고 여기고 있지만 사실 잘 모르는 것이 있다. 로지스틱 회귀(logistic regression) 혹은 로짓 회귀가 그렇지 않을까? 요즘 기계 학습을 배우게 되면, 로짓 회귀는 첫 챕터에서그냥 쓱 지나치기 쉽다. 뒤에 소개될 랜덤 포레스트, SVM, 뉴럴넷 등이 더 진보된 방법으로 보이기 때문이다. . 로짓 회귀는 단순하면서도 강력한 방법이다. 필자는 분류(classification)의 문제에 접근할 때 로짓 회귀를 먼저 해볼 것을 권한다. 로짓 회귀에서 ‘견적’이 나오면 그 질문 혹은 문제는 더 흥미로운 형태로 진화할 가능성이 높다. 반면 로짓 회귀에서 싹수가 없으면 더 복잡한 고급 방법도 소용이 없는 경우가 많다. 요컨대 로짓 회귀는 시간 낭비를 막는 일종의 ‘맛보기’로서 유용하다. . 로짓 함수는 어떻게 등장하나? . 로짓 회귀는 말 그대로 로지스틱 함수(logistic function)를 활용하는 회귀 분석이라는 뜻이다. 로지스틱 함수는 어디서 어떻게 등장할까? 로짓 회귀의 데이터 형식을 살펴보자. 종속 변수, 반응(response), regressand 등 다양한 형태로 불리는 식의 좌변은 0 또는 1로만 되어 있다. 어떤 상태라면 1, 그렇지 않으면 0이다. 이때 0 또는 1은 값 자체로 의미를 지니지 않는다. 이는 범주형 변수로서 True or False와 같은 이항 변수로 이해하면 좋겠다. 식의 우변에는 통상적인 독립 변수, 예측 변수(predictor), regressor가 등장한다. . 덤: 잠시, 용어 정리 . 생각난 김에 짬을 내 회귀식에서 좌변과 우변을 지칭하는 용어를 정리해보자. 취향에 맞춰서 쓰되 일관성만 갖추면 되겠다. . 좌변 우변 . $y$ | $x$, $ boldsymbol X$ | . dependent variable | independent variables | . response variable | predictor variable | . regressand | regressor | . criterion | covariate | . predicted variable | controlled variable | . measured variable | manipulated variable | . explained variable | explanatory variable | . experimental variable | exposure variable | . responding variable | covariate | . outcome variable | covariate | . output variable | input variable | . endogenous variable | exogenous variable | . target | feature | . label | feature | . 왜 OLS는 곤란한가? . . regressor가 하나인 상황을 가정하고 위의 그림을 보자. regressand가 0, 1인 상태로 그대로 회귀분석을 하면 그림의 왼쪽 같이 나온다. 이 상태에서는 뭐가 문제일까? 회귀식의 기울기를 구할 수 있으니 된 것 아니냐고 생각할 수 있겠지만 큰 문제가 있다. 앞서 0과 1은 값 그 자체로 의미를 지니지 않는다고 말했다. 그런데 이를 그대로 값으로 바꿔서 회귀분석을 해도 좋은 것일까? 그러기에는 뭔가 찜찜하다. 게다가 $x$ 값에 따라서 회귀식의 예측치가 $[0,1]$을 벗어날 수 있다. 임의로 계단 함수를 다시 얹어서 OLS(통상적인 최소자승법이라는 의미에서 Ordinary Least Squares라고 흔히 부른다)를 구제할 수도 있겠다. 즉, . s(βx)={1if βx&gt;10if βx&lt;0s( beta x) = begin{cases} 1 &amp; text{if $ beta x &gt; 1$} 0 &amp; text{if $ beta x &lt; 0$} end{cases}s(βx)={10​if βx&gt;1if βx&lt;0​ . 회귀식의 예측을 확률에 맞게 임의로 꺾어주는 것이다. 이러한 과정을 조금 더 ‘부드럽게’ 혹은 보다 ‘그럴 듯하게’ 할 수 없을까? . 범주형에서 확률로 . 앞서 0 또는 1의 문제가 범주형의 문제라고 했다. 따라서 0, 1은 값 자체로는 아무 의미를 지니지 않는다. 회귀분석의 regressand가 실수값을 지닐 때 분석으로서 의미를 지닌다. 그렇다면 그렇게 바뀌보면 어떨까. 범주형 변수가 결국 상태를 나타내는 것이라면 1이라는 상태를 지닐 ‘확률’을 추정하는 문제로 바꿔 생각해볼 수 있겠다. 즉, . p(yi=1∣xi)=p(xi)p(y_i = 1 | boldsymbol{x_i}) = p( boldsymbol{x_i})p(yi​=1∣xi​)=p(xi​) . 그런데 앞서 보았듯이 확률 값을 선형 회귀로 추정하는 데에는 어려움이 있다. 통상적인 회귀 모형의 특성을 유지하면서도 확률 값을 [0,1] 사이에 ‘잘’ 떨궈야 하기 때문이다. . p(xi)1−p(xi) dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})}1−p(xi​)p(xi​)​ . regressand를 이렇게 바꾸면 우선 가능한 값의 범위가 $[0, infty)$로 확장된다. 여기서 음수 값을 포괄하는 방법은 없을까? 여기에 자연 로그를 적용해보자. . ln⁡p(xi)1−p(xi) ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})}ln1−p(xi​)p(xi​)​ . 이렇게 되면 가능한 값의 범위가 $(- infty, infty)$가 된다. Regressand가 이 형태라면 선형 회귀를 적용해도 좋겠다. . 즉, . ln⁡p(xi)1−p(xi)=βxiβ1×kβk×1, k=1,2,…,n ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} = underset{1 times k}{ phantom{ boldsymbol{ beta}} boldsymbol{ boldsymbol{x_i}} phantom{ boldsymbol{ beta}}} underset{k times 1}{ boldsymbol{ beta}}, ~k = 1, 2, dotsc, nln1−p(xi​)p(xi​)​=1×kβxi​β​k×1β​, k=1,2,…,n . $n$ 개의 관찰에 있을 때 이를 매트릭스 형태로 적으면 다음과 같다. . ln⁡p(X)1−p(X)=βXβn×kβk×1 ln dfrac{p(X)}{1-p(X)} = underset{n times k}{ phantom{ boldsymbol{ beta}} boldsymbol{X} phantom{ boldsymbol{ beta}}} underset{k times 1}{ boldsymbol{ beta}}ln1−p(X)p(X)​=n×kβXβ​k×1β​ . 애석하지만 이 식은 우리에게 주어진 자료로는 추정할 수 없다. 당장 좌변의 regressand를 얻으려면 관찰별로 확률값을 알아야 하는데 우리에게 주어진 값은 0 또는 1 뿐이다. 따라서 위의 식은 선형 회귀와 유사한 모형을 적용할 수 있는 방법을 찾은 것일 뿐 실제로 추정할 수 있는 식이 아니다. $ boldsymbol{ beta}$는 어떻게 구해야 할까? . 이를 보기 전에 추정 모형에서 로지스틱 함수를 찾아보자. 위의 식에서 $p(x_i)$를 구해보자. . ln⁡p(xi)1−p(xi)=xiβp(xi)1−p(xi)=exiβp(xi)=exiβ1+exiβp(xi)=1e−xiβ+1 begin{aligned} ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} &amp; = boldsymbol{x_i} boldsymbol{ beta} dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} &amp; = e^{ boldsymbol{x_i} boldsymbol{ beta}} p( boldsymbol{x_i}) &amp; = dfrac{e^{ boldsymbol{x_i} boldsymbol{ beta}}}{1+e^{ boldsymbol{x_i} boldsymbol{ beta}}} p( boldsymbol{x_i}) &amp; = dfrac{1}{e^{- boldsymbol{x_i} boldsymbol{ beta}} + 1} end{aligned}ln1−p(xi​)p(xi​)​1−p(xi​)p(xi​)​p(xi​)p(xi​)​=xi​β=exi​β=1+exi​βexi​β​=e−xi​β+11​​ . 앞서 본 그림의 오른쪽과 같은 형태의 로지스틱 함수가 도출된다. 요컨대, 로지스틱 함수는 regressor를 해당 사건이 발생할 확률과 연결시키는 장치다. 때문에 종종 로지스틱 함수는 연결 함수(link function)로 불리기도 한다. . 계수는 어떻게 구하나? . $ hat boldsymbol{ beta}$은 어떻게 구할 수 있을까? 현재까지 우리에게 주어진 조건을 보자. . regressand는 0 또는 1 뿐이다. | 로짓 함수를 통해서 regressor를 해당 관찰의 확률과 연결시킬 수 있게 되었다. | 어차피 regressand를 실수로 바꿀 수 없는 이상 우리가 아는 선형 회귀분석을 쓸 수는 없다. 그래서 이름과 달리 우리가 아는 ‘회귀분석’, 즉 OLS를 여기서 쓸 수는 없다. 미안하다. 앞에서 거짓말 했다. . ln⁡p(xi)1−p(xi)=βxiβ1×kβk×1, k=1,2,…,n ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} = underset{1 times k}{ phantom{ boldsymbol{ beta}} boldsymbol{x_i} phantom{ boldsymbol{ beta}}} underset{k times 1}{ boldsymbol{ beta}}, ~k = 1, 2, dotsc, nln1−p(xi​)p(xi​)​=1×kβxi​β​k×1β​, k=1,2,…,n . 애초에 $ boldsymbol{ beta}$을 알고 있어야, 이 식으로부터 $p( cdot)$을 구해낼 수 있다. 그런데 우리가 아는 회귀분석이란 $y$와 $ boldsymbol{ rm X}$가 주어졌을 때 하는 것 아닌가? 회귀분석의 좌변이 없기에 OLS로는 추정할 수 없다. . 그래서 로짓 회귀는 이름과 달리 $ boldsymbol{ beta}$의 추정치 $ hat boldsymbol{ beta}$를 구하는데 OLS의 방법을 쓰지 않는다. 로짓 회귀에서는 어떻게 $ hat boldsymbol{ beta}$를 얻는 것일까? . 확률 vs 우도 . 잠시 로짓 회귀의 추정을 살펴보기 전에 확률과 우도의 차이점에 관해서 알아보자. 확률이라면 분포와 그 분포의 주요 파리미터가 주어졌을 때 해당 관찰이 발견될 확률을 구하는 것인가? 즉, 거칠게 표현하면 이렇다. 연속 확률변수 $x$가 따르는 어떤 분포가 있다고 할 때, 해당 분포의 파라미터(정규분포라면 평균과 분산이다) $ theta$에 대해서 $P(x_1 &lt; x &lt;x_2 | theta)$이 확률이다. 우도(likelihood)는 무엇일까? 우도는 해당 확률 분포의 컨셉을 뒤집은 것이다. 즉 어떤 관찰이 있을 때 분포의 특정 파라미터 하에서 얻게 되는 확률 밀도함수를 의미한다. 연속 확률변수가 아니라 이산 확률변수라면, 확률 질량함수가 될 것이고 이 경우에는 확률과 같다. 요컨대 목표에 따라서 주어진 것(conditioning)의 차이에 따라서 확률과 우도의 정의가 달라진다. . 최대 우도 추정(maximum likelihood estimation) . 다시 문제로 돌아오자. 우리는 0, 1 즉 존재/비존재의 특징을 지니는 변수를 갖고 있다. 그리고 이 녀석은 이산 확률 분포를 따르기 때문에 우도 역시 그냥 확률이 된다. 이는 이른바 베르누이 분포를 따른다. 즉, 해당 확률이 발생할 확률 $p$일 때 발생하면 1, 아니면 0이 된다. 베르누이 분포의 확률 질량 함수를 간단하게 표현하는 방법은 없을까? . L(yi)=p(xi;β)yi(1−p(xi;β))1−yiL(y_i) = p( boldsymbol{x_i}; boldsymbol{ beta})^{y_i} (1-p( boldsymbol{x_i}; boldsymbol{ beta}))^{1-y_i}L(yi​)=p(xi​;β)yi​(1−p(xi​;β))1−yi​ . 위와 같은 식으로 간단히 해결된다. 위 식에서 $y_i$가 1이면 $p( cdot)$가, 0이면 $1-p( cdot)$가 할당된다. 이제 우리는 $n$ 개의 관찰에 관해서 식의 좌변에는 0,1을 갖고 있다. 각각의 시행이 독립적이라고 가정하면 주어진 형태의 데이터를 관찰하게 될 우도는 이 확률을 곱한 것과 같다. 즉, . L(y∣X)=∏i=1nL(yi)=∏p(xi;β)yi(1−p(xi;β))1−yi=L(β∣y,X)L(y|{ boldsymbol X}) = prod_{i = 1}^{n} L(y_i) = prod p( boldsymbol{x_i}; boldsymbol{ beta})^{y_i} (1-p( boldsymbol{x_i}; boldsymbol{ beta}))^{1-y_i} = L( boldsymbol{ beta}|y, { boldsymbol X})L(y∣X)=i=1∏n​L(yi​)=∏p(xi​;β)yi​(1−p(xi​;β))1−yi​=L(β∣y,X) . $ boldsymbol{ beta}$에 따라서 우도가 달라지게 되므로 우도 함수가 일종의 목적 함수가 된다. 우도를 극대화해주는 $ boldsymbol{ beta}$가 최대 우도 추정치, 즉 MLE(MLE, maximum likelihood estimator) $ hat boldsymbol{ beta}$ 이다. 즉, . β^=arg max L(β∣y,X) hat { boldsymbol beta} = text{arg max }{L({ boldsymbol beta}|y, { boldsymbol X})}β^​=arg max L(β∣y,X) . 간혹 목적 함수, 즉 우도 함수가 비선형이라서 목적 함수의 최대화를 달성하는 해 $ hat boldsymbol{ beta}$를 축약형(reduced form)으로 구하기 힘들다는 내용을 접하곤 한다. 이는 반쪽만 맞다. 반례로 선형 회귀 모형에서 에러 항이 정규 분포를 따른다고 가정하면 최대 우도 추정을 적용할 수 있다. 정규 분포의 우도 역시 비선형이지만, 축약형 해를 쉽게 구할 수 있다. 1 . 왜 로짓 회귀에서는 축약형 해를 구할 수 없을까? 우도 추정에서 우리가 관심이 있는 것은 목적함수를 극대화하는 $ hat boldsymbol{ beta}$ 값이지 우도 자체가 아니다. 따라서 원래의 우도를 적절한 형태로 변형해도 변형된 목적함수를 극대화해주는 $ hat boldsymbol{ beta}$가 바뀌지 않는다면 목적 함수를 변형해도 괜찮다. 때로는 변형이 계산을 쉽게 바꿔준다. 정규분포는 오일러 수($e$)의 지수 위에 최대화에 필요한 파라미터가 다 올라가 있다. 따라서 원래 목적함수에 $ log_n$를 취하면 곱셈이 덧셈으로 변하고 오일러 수 위에 지수로 올라가 있단 파라미터들이 앞으로 나오게 된다. 하지만 애석하게도(?) 아래 식에서 보듯이 . p(xi)=1e−xiβ+1p( boldsymbol{x_i}) = dfrac{1}{e^{- boldsymbol{ boldsymbol{x_i}} boldsymbol{ beta}} + 1}p(xi​)=e−xi​β+11​ . 는 $ log_n$을 취하는 것으로는 비슷한 형태로 만들 수 없다. 로짓 함수의 경우 우도 극대화에서 축약형을 해를 구할 수 있는 변형이 없기 때문에 해당 우도를 극대화하는 파라미터를 찾기 위해서는 수치 최적화(numerical optimization)를 활용하는 것이다.2 . 계수의 의미는? . 보통 기계 학습의 맥락에서 로짓 회귀는 가장 원시적인 분류기(classifier)로 소개된다. 즉 $y_i$의 속성을 예측하는 분류 장치이기 때문에 $ hat boldsymbol{ beta}$에 특별한 의미를 부여하지 않는다. 하지만 회귀분석을 인과관계의 추론의 활용해 온 여타 분야에서는 $ hat boldsymbol{ beta}$를 어떻게 해석할 것인지는 중요한 질문이다. . 먼저 OLS의 경우 변수를 어떻게 변형했는지에 따라서 다르지만, 대체적으로 $ beta_i$는 해당 regressor $x_i$의 한계효과로 해석할 수 있다. 즉, $x_i$가 한 단위 변할 때 이에 따른 $y_i$의 변화량을 의미한다. 그런데, 로짓 회귀에서는 이렇게 해석할 수 없다. 앞서 보았듯이, 로짓 회귀의 추정식은 다음과 같다. . ln⁡p(xi)1−p(xi)=βxiβ1×kβk×1, k=1,2,…,n ln dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})} = underset{1 times k}{ phantom{ boldsymbol{ beta}} boldsymbol{ boldsymbol{x_i}} phantom{ boldsymbol{ beta}}} underset{k times 1}{ boldsymbol{ beta}}, ~k = 1, 2, dotsc, nln1−p(xi​)p(xi​)​=1×kβxi​β​k×1β​, k=1,2,…,n . 흔히 좌변의 $ dfrac{p( boldsymbol{x_i})}{1-p( boldsymbol{x_i})}$를 오즈(odds) 혹은 승산(勝計)이라고 부른다. 이때 $ boldsymbol{ beta} = [ beta_1, dotsc, beta_j, dotsc, beta_k]$에서 $ beta_j$는 regressor $ boldsymbol{x_i}$에 속한 $x_j$ 한 단위가 변할 때 오즈에 미치는 영향을 뜻한다. . 간혹 $ beta_i$를 좌변의 상태를 변화시킬 확률의 변화로 해석하는 경우가 있는데 그렇게 하면 안된다. $x_i$의 변화가 상태를 0에서 1로 바꾸는 데 미치는 확률에 관심이 있다면, 아래의 두 가지에 유의하자. . OLS의 경우 회귀식 자체가 선형이기 때문에 $ beta_i$에 관한 해석이 쉽고 단순하다. 그냥 직선의 기울기다. 그런데 로짓 회귀에서는 그렇게 분명하지 않다. 회귀식 자체가 비선형이므로 한계 효과를 구하기 위한 기울기를 ‘어디서’ 구할지가 문제가 된다. 측정 위치에 따라서 기울기가 달라지기 때문이다. | 도출하는 방법이 크게 어렵지는 않다. 측정에는 대체로 두 가지 방식이 많이 활용된다. regressor 값에서 $x_i$별로 확률값의 변화를 추정한 뒤 이를 평균하는 방식이다. 반대로 regressor 값의 평균에서 한번만 $p_i$에 미치는 영향을 추정하는 방법이다. 통계 패키지가 대체로 두 방법을 모두 지원하니 필요할 경우 찾아서 값을 구하면 되겠다. | 계수의 수학적인 이해 . 이 부분이 부담스럽다면 넘어가도 좋다. 앞서 말한 한계 효과는 $ frac{ partial p}{ partial x_j}$이다. 즉, regressor의 한 변수$x_j$ 가 한 단위 변할 때 확률 변화를 측정한다. 즉, . p(xi)=1e−xiβ+1p( boldsymbol{x_i}) = dfrac{1}{e^{- boldsymbol{x_i} boldsymbol{ beta}} + 1}p(xi​)=e−xi​β+11​ . 이 녀석을 그대로 미분해보자. 먼저 아래와 같이 $p( cdot)$을 변형하자. . p(xi)=1e−xiβ+1=exiβ1+exiβ begin{aligned} p( boldsymbol{x_i}) &amp; = dfrac{1}{e^{- boldsymbol{x_i} boldsymbol{ beta}} + 1} &amp; = dfrac{e^{ boldsymbol{x_i} boldsymbol{ beta}}}{1 + e^{ boldsymbol{x_i} boldsymbol{ beta}}} end{aligned}p(xi​)​=e−xi​β+11​=1+exi​βexi​β​​ . 그리고 이 녀석을 $ boldsymbol x_i$에 속한 $x_j$에 대해서 미분하면 아래와 같다. . ∂p(xi)∂xj=βjexiβ(exiβ+1)2 dfrac{ partial p( boldsymbol{x_i})}{ partial x_j} = dfrac{ beta_j e^{ boldsymbol{x_i} boldsymbol{ beta}}}{(e^{ boldsymbol{x_i} boldsymbol{ beta}} + 1)^2}∂xj​∂p(xi​)​=(exi​β+1)2βj​exi​β​ . 이처럼 한계 효과는 미분을 통해 간단히 도출할 수 있다. 앞서 말했던 측정의 어려운 점이 무엇인지 이제 잘 볼 수 있다. $ boldsymbol{ beta}$의 추정치로 MLE를 통해 구한 $ hat boldsymbol{ beta}$를 활용한다고 해도, 어떤 $ boldsymbol{x_i}$에서 측정하는지에 따라서 값이 달라진다. 평균(marginal effect at mean)에서 한 번만 측정할 것인지 아니면 $n$개의 모든 데이터 포인트에 대해서 계산한 뒤 이를 평균(average marginal effect)할 것인지 등의 선택이 필요하다. 보통 후자를 많이 활용하고 통계 패키지마다 해당 옵션을 제공하고 있다. . 이때 MLE $ hat boldsymbol{ beta}$은 통상적인 방법으로 구한 OLS 추정량와 동일하다. 본문에서 말했듯이, 이는 수치적인 방법이 아니라 분석적인 방법을 통해 축약형으로 도출 가능하다. 여기를 참고하라. &#8617; . | 수치 최적화의 자세한 내용과 사례는 이 글을 참고하라. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/econometrics/statistics/2019/11/10/understanding-logit-regression.html",
            "relUrl": "/math/econometrics/statistics/2019/11/10/understanding-logit-regression.html",
            "date": " • Nov 10, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "Understanding Regression",
            "content": "Understanding Regression . tl;dr . 회귀분석을 선형 대수를 통해서 이해하면 새로운 깨달음을 얻을 수 있다. | 회귀분석이라는 것은 PCA와 마찬가지로 차원을 ‘축소’하는 방법이다. $n$ 개의 관찰을 지닌 어떤 한 대상을 이보다 낮은 $k(&lt;n)$ 개의 변수로 요약하는 것이 핵심이다. | 선형대수로 보면 $ mathrm R^2$은 피타고라스 정리의 흔한 응용 사례에 불과하다. | . Regression in vector space공간에서 바라본 회귀분석 . 여기서 회귀분석을 해설할 생각은 없다. 이미 너무나 많은 그리고 매우 훌륭한 내용들이 책, 웹, 강의로 넘쳐날테니까. 이 글의 용도는 그림 하나로 지나치기 쉬운 회귀분석의 ‘핵심’을 살피는 것이다. crossvalidated에서 아래 그림을 보는 순간 일종의 ‘돈오돈수’가 강림했다. (이렇게 이해하면 쉬웠을 것을…) . . 먼저 우리에게 익숙한 회귀분석 모델을 매트릭스로 적어보자. . γYγn×1=γXγn×kβk×1+βεγn×1 underset{n times 1}{ phantom{ boldsymbol gamma} mathbf{Y} phantom{ boldsymbol gamma}} = underset{n times k}{ phantom{ boldsymbol gamma} mathbf{X} phantom{ boldsymbol gamma} } underset{k times 1}{ boldsymbol beta} + underset{n times 1}{ phantom{ boldsymbol beta} boldsymbol varepsilon phantom{ boldsymbol gamma} }n×1γYγ​=n×kγXγ​k×1β​+n×1βεγ​ . 식에 관한 자세한 설명 역시 생략한다. 대충 $n$ 개의 관찰 수과 $k$ 개의 regressor를 지닌 중회귀분석 모형이라고 생각하면 되겠다. 앞서 본 그림은 보통 회귀분석의 예시로 많이 활용되는 아래 그림과는 다르다.1 . . 위 그림은 1개의 regressor가 존재할 때 이것과 regressand를 그대로 2차원 평면에 관찰 수만큼 찍은 것이다. 첫번째 그림에서 “Observed Y”는 점 하나가 $n$ 개의 regressand를 모두 포괄한다. $ mathbf{Y} in { mathbb R}^{n}$ 벡터, 즉 $n$ 차원 벡터이고, 이것이 회귀식 좌변의 관찰값 $n$ 개를 표현한다. . 이제 선형대수의 세계로 들어가보자. $ mathbf X$의 열(column)이 각각 $n$ 개의 관찰 값을 지닌 regressor에 해당한다. 이 각각의 컬럼 $x_i$는 $x_i in { mathbb R}^{n}$ 벡터이다. $x_i$ 벡터 $k$ 개가 생성할 수 있는 공간이 $ mathbf X$의 열 공간(column space)이다. 앞으로 이를 col $ mathbf X$로 표기하자. . 이렇게 생각하면 좋겠다. $x_i in { mathbb R}^{n}$ 벡터가 $k(&lt;n)$ 개 있다고 하자. 이는 $n$ 보다 차원이 낮기 때문에 기하학적으로는 $n$ 차원 공간에서 초평면(hyperplane)으로 나타날 것이다. 위 그림에서 컬럼 스페이스가 평면으로 표현되는 것은 이러한 취지다. 3차원 벡터의 열 공간이 생성하는 초평면을 예시하면 아래와 같다. . . col $ mathbf X$의 최대 차원, 즉 $ mathbf X$의 랭크(위수)는 무엇일까? 회귀분석에서는 대체로 $n &gt; k$가 일반적이고 이런 상황에서 $ mathbf X$의 랭크는 $k$를 넘을 수 없다. 다시 말하면, $ mathbf X$가 생성하는(span)하는 컬럼 스페이스의 차원의 크기가 $k$를 넘을 수 없다. 그리고 잘 된 회귀분석이라면 ${ rm rank}( mathbf X) = k$를 만족한다. . 맨 앞에 제시했던 그림을 다시 보자. 아래 색칠된 평면이 $ mathbf X$가 생성하는 컬럼 스페이스, 즉 col $ mathbf X$를 표현하고 있다. 몹시 특별한 경우가 아니라면 $ mathbf Y in { mathbb R}^{n}$ 벡터가 col $ mathbf X$에 속할 가능성은 없다. 그렇다면 회귀분석의 필요도 애초에 없었을 것이다. col $ mathbf X$를 통해서 $ mathbf Y$를 완벽하게 예측할 수 있는데 무슨 걱정이 있겠는가? 대체로 우리가 마주하는 상황은 $n$ 차원 벡터를 $k$ 차원 공간에 끼워 넣기 힘든 상황이다.2 . 회귀분석의 목표는 regressor를 통해서, 더 정확하게는 regressor의 집합이 생성하는 공간을 통해서 regressand를 ‘가장’ 잘 설명하는 것이다. 회귀분석이란 regressand와 ‘닮은’ 것 col $ mathbf X$에서 찾는 것이다. 즉 $ mathbf Y$와 닮은 무엇을 $ mathbf X$의 컬럼 스페이스에 찾아야 한다. 직관적으로 쉽게 떠올릴 수 있는 것은 이 평면과 $ mathbf Y$의 (유클리드) 거리를 가장 짧게 만들어주는 벡터일 것이다. 그리고 이 최단거리는 $ mathbf Y$에서 $ mathbf X$ 컬럼 스페이스로 내린 수선의 발이 닿는 col $ mathbf X$의 지점이다. col $ mathbf X$ 내에 있는 이런 지점을 찾는 연산자(operator)가 회귀분석 계수 $ hat{ boldsymbol beta}$이다. 즉, . β^=(X′X)−1(X′Y) hat{ boldsymbol beta} = ({ mathbf X}&amp;#x27;{ mathbf X})^{-1} ({ mathbf X}&amp;#x27; mathbf Y)β^​=(X′X)−1(X′Y) . 그리고 이 연산자를 reggressor의 모음인 col $ mathbf X$에 적용하면, $ mathbf{X} hat{ boldsymbol beta} = hat{ mathbf Y}$이 계산된다. 그림에서 보듯이 $ hat{ mathbf Y}$은 $ mathbf Y$와 col $ mathbf X$의 거리를 최소화하는 위치에 존재한다. $ hat{ mathbf Y}$는 어떤 벡터일까? $ hat{ mathbf Y} in { mathbb R}^n$ 벡터지만, col $ mathbf X( in { mathbb R}^k)$내에 위치하고 있다. $n$ 차원이 $k$ 차원으로 축소된 셈이다. 이것이 회귀분석의 핵심이다. . 선형 대수의 관점에서 내용을 다시 음미해보자. $x_i in { mathbb R}^n$ (for $i = 1, dotsc, k$) 벡터의 리그레서 $k$ 개를 선형 결합해서 초평면의 한점, 즉 원래의 관찰 $y$와 최소 거리를 지니는 벡터를 찾아야 한다. 이 거리를 최소화하는 $ beta_i$를 알고 있다면 이 벡터는 다음과 같이 표현된다. ${ mathbf X} = [x_1, dotsc, x_k]$일 때, . Y^(n×1)=Xβ=[x1,…,xk][β1⋮βk]=x1β1+…+xkβk begin{aligned} underset{(n times 1)}{ hat{ mathbf Y}} &amp; = X beta &amp; = [x_1, dotsc, x_k] begin{bmatrix} beta_1 vdots beta_k end{bmatrix} &amp;= x_1 beta_1 + dotsc + x_k beta_k end{aligned}(n×1)Y^​​=Xβ=[x1​,…,xk​]⎣⎢⎢⎡​β1​⋮βk​​⎦⎥⎥⎤​=x1​β1​+…+xk​βk​​ . 아래 그림을 참고하자. . . R squared as Pythagorean Theorem . 이제 $ mathrm R^2$의 의미를 살펴보자. 결론부터 이야기하면 $ mathrm R^2$는 그림에서 $( mathbf Y - overline{ mathbf Y})$ 벡터와 $( hat{ mathbf Y}- overline{ mathbf Y})$ 벡터가 이루는 각의 코사인 값, 즉 $ cos theta$다. . $ overline{ mathbf Y}$는 무엇일까? 포스팅의 맨 처음 보았던 그림과 같이 $ overline{Y} mathbf{1}_n$로 표기할 수 있다. $ mathbf Y$의 평균값 $ overline{Y}$만으로 구성된 $(n times 1)$ 벡터다. 이 벡터는 col $ mathbf X$ 안에 있을까? 당연히 그렇다. $ mathbf X$는 최대한 $k(&lt;n)$ 차원의 벡터이고, $ overline{ mathbf Y}$는 1차원 벡터다.3 . 어쨌든 이 코사인 값의 의미는 무엇일까? . 그림에서 보듯이 세 개의 벡터가 직각삼각형을 이루고 있으므로 아래의 식이 성립한다. . ∥Y−Y‾∥2TSS=∥Y−Y^∥2RSS+∥Y^−Y‾∥2ESS underset{ text{TSS}}{ Vert mathbf Y - overline{ mathbf Y} Vert^2} = underset{ text{RSS}}{ Vert mathbf Y - hat{ mathbf Y} Vert^2} + underset{ text{ESS}}{ Vert hat{ mathbf Y} - overline{ mathbf Y} Vert^2}TSS∥Y−Y∥2​=RSS∥Y−Y^∥2​+ESS∥Y^−Y∥2​ . 흔한 피타고라스의 정리다. 그런데 이것 어디서 많이 보던 식이다. 회귀분석 배우면 언제나 나오는 식이다. Regressand의 평균과 관찰의 이른바 총 제곱의 합(TSS: Total Sum of Squares)은 설명된 제곱의 합(ESS: Explained Sum of Squares)과 잔차 제곱의 합(RSS:Residual Sum of Squares)와 같다. 대체로 복잡하게 소개되는 이 식이 기하학적으로 보면 그냥 피타고라스의 공식에 불과한 것이다. . 양변을 $ Vert mathbf Y - overline{ mathbf Y} Vert^2$으로 나누면 다음과 같다. . 1=∥Y−Y^∥2∥Y−Y‾∥2+∥Y^−Y‾∥2∥Y−Y‾∥21 = dfrac{ Vert mathbf Y - hat{ mathbf Y} Vert^2}{ Vert mathbf Y - overline{ mathbf Y} Vert^2} + dfrac{ Vert hat{ mathbf Y} - overline{ mathbf Y} Vert^2}{ Vert mathbf Y - overline{ mathbf Y} Vert^2}1=∥Y−Y∥2∥Y−Y^∥2​+∥Y−Y∥2∥Y^−Y∥2​ . 정의에 따라서 $1 = dfrac{ text{RSS}} { text{TSS}} + { mathrm R}^2$가 된다. 즉, . R2=1−RSSTSS{ mathrm R}^2 = 1 - dfrac{ text{RSS}}{ text{TSS}}R2=1−TSSRSS​ . 사실 $ textrm R^2$는 가끔 회귀분석의 성과 지표로 남용되는 경우가 있다. 이렇게 기하학적으로 보면 col $ mathbf X$ 내에 표현된 $ hat{ mathbf Y}$ 가 $ mathbf Y$와 얼마나 가깝게 있는지를 $ overline{ mathbf Y}$를 기준으로 지표화한 것에 불과하다. . ${ mathrm R}^2$는 회귀분석의 성과 지표로 어떤 의미가 있을까? 분석의 목표가 회귀분석을 통한 예측이라면, 즉 원래 관찰값과 예측된 값이 얼마나 떨어져 있는지 여부가 중요하다면 $ textrm R^2$는 의미를 지닐 수 있다. 반면, 분석의 목표가 회귀분석을 통한 이러한 종류의 예측이 아니라 특정한 regressor의 인과관계에 관한 추정이라면, $ textrm R^2$는 거의 무시해도 좋다. . 아울러 회귀분석이라는 이름을 달고 있지만 전형적인 회귀분석의 방법을 따르지 않는 기법에서 $ textrm R^2$가 정의되지 않는 경우도 있다. 잘 알려진 로지스틱 회귀가 이에 해당한다. 로지스틱 회귀에서 회귀 계수의 추정은 여기서 봤듯이 관찰과 col $ mathbf X$ 사이의 거리를 최소화하는 방식이 아니라 우도(likelihood)를 극대화하는 방식을 따른다. 따라서 벡터 공간의 피타고라스 정리를 따르는 $ textrm R^2$는 정의되지 않는다.4 . Bonus: Regression vs PCA덤: 회귀 분석과 PCA는 서로 얼마나 다를까? . 회귀 분석과 PCA는 어떻게 다른가? 여러가지 답이 있을 수 있다. 그걸 다 소개하겠다는 게 아니다. 둘이 데이터 모델링의 시야에서 어떻게 다른지를 살펴보는 게 이 글의 목적이다.5 이 글은 둘을 어떻게 실행하는지를 다루지 않는다. 앞선 PCA 관련 포스팅을 읽고 오시면 이해에 도움이 될 것이다. . 공통점 . Feature . 일단 둘 다 $k$ 개의 feature 비슷한 것을 지닌다. 그리고 $n$ 개의 데이터 포인트가 제시된다. . 차원 축소 . 흔히 PCA를 차원축소의 방법으로 이해하는데, 이 말은 맞다. 반대로 회귀분석은 이와 다르다고 이해하는 경우가 많은데 이 말은 틀리다. 회귀 분석도 어떻게 보면 ‘차원 축소’다. 회귀 분석에서 target 변수는 $n$ 차원 공간 위의 주어진 한 점이다. 우리의 목표는 이 한 점을 잘 설명하는 더 낮은 차원의 어떤 지점을 찾는 것이다. 이런 점에서 본다면 회귀 분석 역시 차원 축소의 한 방법이라고 봐야 할 것이다. . 극소화 . PCA에서 ‘분산 최대화’에 이르기까지 과정을 생략하다보면, PCA를 별도의 어떤 방법으로 인식하곤 한다. 그런데 앞선 포스팅에서 보았듯이 ‘분산 최대화’란 사실 피처 벡터와 이를 예측하기 위한 어떤 벡터 사이의 거리를 최소화하는 과정에서 도출된 결과다. 이런 점에서 본다면, 회귀 분석이든 PCA는 MSE(Mean Square Error)를 최소화한다는 점에서는 목적 함수의 유형은 동일하다. . 차이점 . Supervised or Unsupervised? . 회귀 분석과 PCA를 지도 학습(supervised learning), 비지도 학습(unsupervised learning)으로 구분할 수는 없다. 다만 이 구분과 어느 정도 비슷한 부분이 있다. 회귀 분석은 target이 있다. 이 타겟과의 거리를 최소화하는 feature 공간의 어떤 위치를 찾는 것이 목적이다. 반면, PCA에는 target이 없다. $k$ 개의 feature를 최소 거리로 투영할 수 있는 스크린 벡터를 찾는게 목적이다. 간단히 말해서 PCA는 target 없이 벡터의 거리가 1인 임의의 프로젝션 벡터를 찾는 것이 목적이다. . 흔히 $ mathbf Y$를 종속변수, $ mathbf X$를 독립변수로 부르기도 한다. 하지만 이러한 이름에는 혼란의 여지가 있다. 여기서는 regressor, regressand라는 영어 표현을 그대로 쓰도록 하겠다. &#8617; . | 이를 선형 대수에서 배운 연립방정식을 푸는 문제로 이해해도 좋겠다. $A x = b(b neq boldsymbol{0})$라고 하자. $n$ 개의 미지수가 유일한 해를 지니고 위해서는 ${ rm rank}(A) = n$이어야 한다. 즉 서로 독립인 식이 $n$ 개 주어져야 고유의 해 $x$를 찾을 수 있다. 그런데 회귀분석은 식이 $k(&lt;n)$ 개만 주어진 상황이다. &#8617; . | 왜 그런지 잠시 따져보자. 앞서 보았듯이 regressor의 평면은 $ alpha_1 x_1 + dotsb + alpha_k x_k$ 와 같은 형태의 선형 결합을 통해 달성된다. 즉, $ alpha_i$를 어떻게 잡는지에 따라서 $k$ 차원까지 이 식을 통해서 생성할 수 있다. 그런데 $ overline{ mathbf Y}$는 1차원 즉, 모든 원소가 $ overline{y}$, 즉 $y$의 평균이다. 따라서 이를 만족하는 $ alpha_i$(for $i = 1, dotsc, k$)를 ${ rm col}~{ mathbf X}$에서 반드시 찾을 수 있다. &#8617; . | 궁여지책으로 이와 유사한 지표를 만들어낼 수는 있겠다. 여기를 참고하라. &#8617; . | 더 자세한 내용은 여기를 참고하자. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/econometrics/2019/10/25/understanding-regression.html",
            "relUrl": "/math/econometrics/2019/10/25/understanding-regression.html",
            "date": " • Oct 25, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "Dot Product",
            "content": "tl;dr . 닷 프로덕트 혹은 내적을 기하적으로 이해하면 여러모로 편리하고 기억에도 잘 남는다. | . Definition . dot product는 내적이라고도 번역하지만 여기서는 닷 프로덕트로 쓰기로 하겠다. 먼저 정의부터 살펴보자. . u=[u1,u2,…,un]∈Rn mathbf{u}= left[u_{1}, {u}_{2}, ldots, {u}_{n} right] in { mathbb R}^nu=[u1​,u2​,…,un​]∈Rn . v=[v1,v2,…,vn]∈Rn mathbf{v}= left[{v}_{1}, {v}_{2}, ldots, {v}_{n} right] in { mathbb R}^nv=[v1​,v2​,…,vn​]∈Rn . u⋅v=∑nuivi mathbf{u} cdot mathbf{v} = sum_{n} u_i v_iu⋅v=n∑​ui​vi​ . 쉽게 말해서 닷 프로덕트는 차원이 같은 두 개의 인풋 벡터를 하나의 스칼라로 바꿔주는 일종의 함수로 이해할 수 있다. 두 개의 벡터를 서로 연관 짓는데 이를 해당 벡터의 길이라는 정보로 압축한다고 보면 얼추 맞을 듯 싶다. 하나의 숫자로 요약된다는 뜻에서 스칼라 프로덕트라고도 불린다. 그림으로 나타내면 아래와 같다. . . $ mathbf{v}$ 벡터를 $ mathbf u$ 벡터 위에, 즉 이 벡터를 스크린 삼아 직각으로 쏜 프로젝션 벡터($ mathrm{Proj}_{ mathbf u}{ mathbf v}$)의 길이와 $ mathbf u$ 길이를 곱하면 그것이 $ mathbf u$와 $ mathbf v$의 닷 프로덕트가 된다. 어느 벡터로 프로젝션 하는지는 관계가 없다. 즉, . v⋅u=∥u∥∥Projuv∥=∥u∥(∥v∥cos⁡θ){ mathbf v} cdot { mathbf u} = Vert mathbf u Vert Vert { rm Proj}_{ mathbf u} { mathbf v} Vert = Vert mathbf u Vert ( Vert mathbf v Vert cos theta)v⋅u=∥u∥∥Proju​v∥=∥u∥(∥v∥cosθ) . u⋅v=∥v∥∥Projvu∥=∥v∥(∥u∥cos⁡θ){ mathbf u} cdot { mathbf v} = Vert mathbf v Vert Vert { rm Proj}_{ mathbf v} { mathbf u} Vert = Vert mathbf v Vert ( Vert mathbf u Vert cos theta)u⋅v=∥v∥∥Projv​u∥=∥v∥(∥u∥cosθ) . 다시 보면, 닷 프로덕트의 정의상 ${ mathbf v} cdot { mathbf u} = { mathbf u} cdot { mathbf v}$인 셈이다. 닷 프로덕트는 어떻게 도출하는가? 그 기하학적인 구조는 무엇인가? . With ‘Law of Cosine’ . 우선 정석부터 가보자. 코사인 법칙을 활용해서 닷 프로덕트를 도출할 수 있다. 1 . 우선 코사인의 법칙부터 살펴보자. . Law of COS | . ∥u−v∥2=∥u∥2+∥v∥2−2∥u∥∥v∥cos⁡θ lVert mathbf u - mathbf v rVert^2 = lVert mathbf u rVert^2 + lVert mathbf v rVert^2 - 2 lVert mathbf u rVert lVert mathbf v rVert cos theta∥u−v∥2=∥u∥2+∥v∥2−2∥u∥∥v∥cosθ . 노름(norm, 길이)에 대해서는 대칭과 쌍방선형이 유지되기 때문에 아래와 같이 쓸 수 있다. | . ∥u−v∥2=(u−v)⋅(u−v)=∥u∥2+∥v∥2−2(u⋅v) lVert mathbf u - mathbf v rVert^2 = ( mathbf u - mathbf v) cdot ( mathbf u - mathbf v ) = lVert mathbf u rVert^2 + lVert mathbf v rVert^2 - 2 ( mathbf u cdot mathbf v)∥u−v∥2=(u−v)⋅(u−v)=∥u∥2+∥v∥2−2(u⋅v) . 따라서 . u⋅v=∥u∥∥v∥cos⁡θ=∥u∥(∥v∥cos⁡θ)=∥u∥∥Projuv∥ mathbf u cdot mathbf v = lVert mathbf u rVert lVert mathbf v rVert cos theta = lVert mathbf u rVert ( lVert mathbf v rVert cos theta) = lVert mathbf u rVert rVert text{Proj}_{ mathbf u} mathbf v lVertu⋅v=∥u∥∥v∥cosθ=∥u∥(∥v∥cosθ)=∥u∥∥Proju​v∥ . 한편 반대로 $ mathbf u$에서 $ mathbf v$로 프로젝션하는 경우를 생각해 볼 수도 있겠다. . u⋅v=∥u∥∥v∥cos⁡θ=∥v∥(∥u∥cos⁡θ)=∥v∥∥Projvu∥ mathbf u cdot mathbf v = lVert mathbf u rVert lVert mathbf v rVert cos theta = lVert mathbf v rVert ( lVert mathbf u rVert cos theta) = lVert mathbf v rVert rVert text{Proj}_{ mathbf v} mathbf u lVertu⋅v=∥u∥∥v∥cosθ=∥v∥(∥u∥cosθ)=∥v∥∥Projv​u∥ . Geometrically . 기하적으로 도출하는 흥미로운 방법도 있다.2 이해를 돕기 위해서 2차원 벡터공간으로 한정해서 논의하겠다. $n$ 차원으로 확대하는 것이 수학적으로 어렵지는 않다. . 일단 $ mathbf u$가 길이 1로 표준화된 벡터라고 정의를 살짝 바꾸겠다. 즉, 새로운 $ mathbf u$는 $ rVert mathbf u lVert$로 $ mathbf u$를 나눈 벡터다. 아래 그림처럼 이 벡터를 향해서 2차원 평면의 기저를 구성하는 $(1,0)( equiv i)$과 $(0,1)( equiv j)$에서 벡터로 프로젝션을 해보자. . 이렇게 프로젝션을 하면 프로젝션된 지점의 $x$ 좌표는 공교롭게도 원점에서부터 해당 프로젝션된 지점까지의 벡터의 길이가 된다. $y$에 대해서도 마찬가지다. . . . 이제 (1,1)에서 벡터 $ mathbf u$로 프로젝션을 해보자. (1,1)은 각각 두 개의 기저를 1의 가중치로 선형결합한 벡터다. 이 벡터의 프로젝션의 길이는 어떻게 구성될까? 그림에서 보듯이 $u_x + u_y$가 된다. 이를 일반적인 논리로 확장해보자. 어떤 임의의 벡터 $ mathbf v(=(x,y))$가 존재할 때 해당 벡터는 각각 두 개의 기저의 선형 결합으로 이해할 수 있다. . . 따라서 $ mathbf v$ 벡터를 $ mathbf u$로 프로젝션한 길이는 다음과 같다. . [uxuy]u프로젝션[xy]=ux⋅x+uy⋅y=u⋅v underset{ mathbf u 프로젝션}{ left[ begin{array}{ll}{u_{x}} &amp; { u_{y}} end{array} right]} left[ begin{array}{l}{x} {y} end{array} right]= u_{x} cdot x + u_{y} cdot y = mathbf u cdot mathbf vu프로젝션[ux​​uy​​]​[xy​]=ux​⋅x+uy​⋅y=u⋅v . 벡터를 기저의 선형결합을 통해 나타낼 수 있듯이, 벡터의 프로젝션의 길이 역시 비슷한 방식의 선형결합을 동원해서 나타낼 수 있다. 앞서 $ mathbf u$가 표준화된 벡터라고 했다. 따라서 원래대로 돌려 놓으면 닷 프로덕트는 프로젝션된 지점까지의 벡터의 거리와 해당 벡터의 길이의 곱이 된다. 즉, . u⋅v=∥u∥∥Projuv∥ mathbf u cdot mathbf v = rVert mathbf u lVert rVert text{Proj}_{ mathbf u} mathbf v lVertu⋅v=∥u∥∥Proju​v∥ . Applications . Cosine similarity . 두 개의 벡터가 얼마나 유사한지를 나타내는 지표로 코사인 유사도라는 게 있다. 위에서 보듯이 두 개의 벡터($ mathbf v, mathbf u$)가 이루는 각 $ theta$의 코사인 값은 다음과 같다. . cos⁡θ=u⋅v∥u∥∥v∥ cos theta = dfrac{ mathbf u cdot mathbf v}{ lVert mathbf u rVert lVert mathbf v rVert}cosθ=∥u∥∥v∥u⋅v​ . 두 벡터가 가까울수록 코사인 값이 1에 가깝게 될 것이고, 멀수록 -1에 가깝게 될 것이다. . Hyperplane . 닷 프로덕트를 이해하고 있으면 기하학 문제를 쉽게 풀 수 있는 게 많다. 가장 좋은 예가 초평면(hyperplane)이다. 예를 들어 3차원 공간에서 점 $ mathbf{x}^0 = (x_1^0, x_2^0, x_3^0)$를 지나면서 벡터 $ mathbf{p} = (p_1, p_2, p_3)$에 수직인 평면을 찾고 있다고 하자. 복잡해보이지만 닷 프로덕트를 활용하면 쉽게 풀린다. 즉, . p⋅(x−x0)=0 mathbf{p} cdot ( mathbf{x} - mathbf{x}^0) = 0p⋅(x−x0)=0 . 평면 $ mathbf x$가 $ mathbf{x}^0$를 지나는 것은 분명하다. 이 평면이 $ mathbf{p}$에 수직이라는 것은 두 벡터의 닷 프로덕트가 0이 되면 된다. . . 여기를 참고 했다. &#8617; . | 보다 상세한 내용은 여기를 참고하라. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/2019/07/18/dot-product.html",
            "relUrl": "/math/2019/07/18/dot-product.html",
            "date": " • Jul 18, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "Poisson Distribution, Intuitively",
            "content": "tl;dr . 푸아송 분포는 이항 분포를 주어진 시간(기간) 안에 발생하는 평균 발생 횟수로 다시 모델링한 분포다. | 이항 분포의 평균은 $ lambda = n p$이고, 이항 분포에 $p = frac{ lambda}{n}$을 다시 넣은 후 $n to infty$ 극한값을 얻은 결과가 푸아송 분포다. | . Defining Poisson Distribution . 사실, 푸아송 분포 이해하기 쉽지 않다. 정의부터 먼저 살펴보자. 한글 위키피디어에서 가져왔다. . 푸아송 분포(Poisson distribution)는 확률론에서 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지를 표현하는 이산 확률 분포이다. 일단 정의에서 알 수 있는 것: . 푸아송이라는 사람이 만들었다. | 단위 시간 안에 발생하는 사건이 중요하다. | 이산 확률 분포다. | 수학적인 정의는 일단 생략하자. 뒤에서 다시 만나게 될 테니까. 정의만 봐서는 이해가 쉽지 않은 분포다. 직관적인 방식으로 푸아송 분포의 정의를 끌어낼 수는 없을까?1 . Poisson as Binary . 잠시 이항 분포를 떠올려보자. 아마도 모든 확률 관련 수업에서 제일 처음 배우는 분포가 아닐까 한다. 이항 분포란, 1회 시행 시 발생할 확률이 $p ;$인 어떤 사건을 $n$ 번 시행할 때 해당 사건이 $k$ 번 발생할 확률의 분포를 나타낸다. 개별적으로 존재하는 사건이니까 당연히 이산 확률 분포에 속한다. 수학적으로는 아래와 같다. . B(k;n,p)=(nk)pk(1−p)n−kB(k; n,p) = {n choose k} p^k(1-p)^{n-k}B(k;n,p)=(kn​)pk(1−p)n−k . 위 정의를 말로 풀어보자. n번 시행 중에 사건이 발생한 경우 $k$ 개를 뽑는다. 즉, 이런 경우가 $n$ 번 중에서 얼마나 발생할 수 있는지 따진다. 그리고 해당 경우의 수가 발생할 확률을 곱해준 것이다. . 이항 분포를 ‘시간’의 맥락으로 한번 바꿔보자. 하루 동안 내 사이트에 방문객이 한 명 찾아올 확률을 0.1이라고 하자. 28일 동안 10 명의 방문자가 찾아올 확률은? 이항 분포의 맥락에서 생각한다면, $B(10; 28, 0.1)$로 나타낼 수 있다. 그런데 이 모델링 어딘가 찜찜하다. . 이항 분포의 모델링은 하루 동안 사이트 방문객이 한 명일 확률에 기대고 있다. 그런데 하루에 방문객이 두 명 이상이라면? 하루에 방문객이 한 명 씩만 온다는 가정이 비현실적이지 않나? . 두 명 이상의 방문자를 수용하려면 이항 분포의 모형을 어떻게 수정해야 할까? 가장 손쉬운 방법은 단위 시간을 바꾸는 것이다. 단위 시간을 하루가 아니라 시간으로 바꿔보자. 하루는 24시간이고 매 시간 동일한 정도로 이용자가 방문한다면, 시간당 1명이 방문할 확률은 0.1/24이 된다. 낮은 확률이지만 이러한 가정에서 하루에 두 명 이상도 방문할 수 있게 된다. . Deriving Poisson . 이항 분포와 달리 푸아송 분포에서 파라미터는 한 개다. 푸아송 분포의 파라미터 $ lambda$는 단위 시간 당 평균 발생 횟수를 뜻한다. 비율(rate)이라고도 부른다. . $ lambda$가 ‘파라미터’라는 의미는 무엇일까? 과거의 데이터로부터 혹은 어떤 방법으로든 푸아송 분포를 정의하는 시점에서 알려진 혹은 정의된 사실이라는 의미다. 즉, 우리가 관심을 두고 있는 기간 내에 평균적인 방문자 수가 주어진다는 뜻이다. . 기간 내에 특정 순간 동안 어느 정도의 방문자가 올지 미리 알 수 없다. 방문자가 없다가 어느 순간 10명이 몰려올 수도 있다. 일정한 간격으로 한 명씩 10 명이 올 수도 있다. 매번 시행 때마다 해당 사건이 발생할 확률을 모형화하는 이항분포를 시간의 맥락으로 바꾸면 이렇게 뭔가 어색해 보인다. . 이항 분포를 통해 푸아송 분포를 표현하는 방법은 없을까? 어떤 패턴으로 방문자가 올지 알 수 없다면, 이항 분포의 기준이 되는 시행(시점)을 최대한 잘게 쪼개면 어떨까? 이항 분포에서 평균은 $np ;$ 다. 즉, 이항 분포의 맥락에서 보면 푸아송 분포의 파라미터 $ lambda = n p ;$다. $ lambda$가 고정되어 있다면 $n$을 무한대로 갈 때 $p$는 적절한 방식으로 0으로 접근한다고 가정해도 무방하다. 정리하면 푸아송 분포를 이항 분포에서 도출하기 위한 전략은 다음과 같다. . $n$과 $p$를 $ lambda$ 파라미터 하나로 바꾼다. | $n$이 무한대로 갈 때 이항 분포의 확률 밀도 함수의 극한값을 도출한다. | lim⁡n→∞B(k;n,p)=lim⁡n→∞(nk)pk(1−p)n−k=lim⁡n→∞n!k!(n−k)!(λn)k(1−λn)n−k=lim⁡n→∞[n!(n−k)!nk]⏟(가)[λkk!(1−λn)n][(1−λn)−k]⏟(나)=1[λkk!e−λ]1=f(k;λ) begin{aligned} lim_{n to infty} B(k; n, p) = &amp; lim_{n to infty} {n choose k } p^k(1-p)^{n-k} = &amp; lim_{n to infty} dfrac{n!}{k!(n-k)!}( dfrac{ lambda}{n})^k (1- dfrac{ lambda}{n})^{n-k} = &amp; lim_{n to infty} underbrace{ left[ dfrac{n!}{(n-k)! n^k} right]}_{(가)} left[ dfrac{ lambda^k}{k!} (1- dfrac{ lambda}{n})^{n} right] underbrace{ left[ (1- dfrac{ lambda}{n})^{-k} right]}_{(나)} = &amp; 1 left[ dfrac{ lambda^k}{k!} e^{- lambda} right] 1 = &amp; f(k; lambda) end{aligned}n→∞lim​B(k;n,p)=====​n→∞lim​(kn​)pk(1−p)n−kn→∞lim​k!(n−k)!n!​(nλ​)k(1−nλ​)n−kn→∞lim​(가) . [(n−k)!nkn!​]​​[k!λk​(1−nλ​)n](나) . [(1−nλ​)−k]​​1[k!λk​e−λ]1f(k;λ)​ . 푸아송 분포 확률 질량 함수 $f(k; lambda)$ 앞 뒤로 붙은 극한값 (가)와 (나)에 관해 사족을 덧붙여보자. 먼저 (가)는 아래와 같다. . lim⁡n→∞n!(n−k)!nk=lim⁡n→∞n(n−1)⋯(n−(k−1))nk=1 lim_{n to infty} dfrac{n!}{(n-k)! n^k} = lim_{n to infty} dfrac{n(n-1) dotsb(n-(k-1))}{n^k} = 1n→∞lim​(n−k)!nkn!​=n→∞lim​nkn(n−1)⋯(n−(k−1))​=1 . 분자와 분모 모두 가장 높은 $n$의 차수는 $k$이고, 그 계수가 모두 1이므로 극한값은 1이다. (나)의 극한값은 쉽게 알 수 있으니 생략하자. . 이항 분포의 확률 혹은 확률 질량 함수의 극한값이 바로 푸아송 분포의 그것이다! 이거 참 재미있는 연결 고리가 아닌가! 사실 푸아송 분포의 정의를 그냥 들여다봐서는 그다지 직관적인 이해를 얻기는 쉽지 않다. 이렇게 이항 분포에서 출발하는 것이 직관적인 이해에는 도움이 된다. . 정확하게 언급하자면 분포의 정의라기보다는 분포의 확률 질량 함수 혹은 확률 밀도 함수라고 적는 게 맞을 것이다. 하지만 편의상 특별한 언급이 없는 경우 분포의 정의를 이런 의미로 사용하도록 하자. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/statistics/2019/07/13/Poisson.html",
            "relUrl": "/math/statistics/2019/07/13/Poisson.html",
            "date": " • Jul 13, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Math Behind PCA",
            "content": "tl; dr . PCA를 차원을 축소하는 방법으로 막연하게 이해하지 말자. PCA 역시 다른 방법처럼 어떤 목적 함수를 최적화하는 방법의 하나다. | PCA는 $k$ 개의 피처를 어떤 스크린 벡터 위에 쏴서 이를 단순화하겠다는 것이다. 이렇게 투영된 이미지와 원래 벡터와의 거리를 최소화하는 과정에서 분산 최대화라는 PCA의 새로운 목적 함수가 도출된다. | PCA가 스크린으로 활용할 벡터가 하나가 아니라고 할 때, 이 여러 개의 스크린 벡터를 활용해 거리를 최소화하는 과정(즉 분산의 합을 극대화하는 과정)에서 eigenvalue와 eigenvector가 등장한다. | . PCA . “차원의 저주”라는 표현이 있다. 언뜻 보면 자명한 이야기 같지만, 곰곰이 생각해보면 모호한 구석이 많다. 관찰 수는 많을수록 좋은데 차원은 관찰과 어떻게 다를까? 쉽게 생각해보자. 관찰 수란 활용할 수 있는 샘플의 수다. 이는 당연히 많을수록 좋다. (물론 미칠 듯이 많으면 새로운 문제가 발생하긴 하나, 대체로 우리는 샘플이 부족해서 문제를 겪는다) 하나의 샘플에서 관찰 가능한 변수가 7개라고 해보자. 샘플 수에 따라서는 적당해 보일 수 있다. . 그런데, 샘플은 100 개인데, 한 샘플에서 관찰할 수 있는 포인트가 1,000 개라고 치자. 이 데이터 셋은 10만 개의 개별 포인트를 지닌 제법 큰 데이터 셋이지만 별 쓸모는 없다. 관찰 수에 비해서 개체의 차원이 지나치게 크기 때문이다. 이럴 경우 어떻게 차원을 줄이면 좋을까? 쉽게 생각할 수 있는 방법은 1,000 개의 특징들을 좀 줄여보는 것이다. 주성분분석(Principal Component Analysis)은 이를 위해 필요한 방법이다. . Objective function for PCA? . 대체로 많은 PCA에 관한 설명들이 원래 하고 싶은 게 무엇인지에 관해 묻지 않는다. PCA란 데이터의 특성을 압축하는 방법이라는 이야기만 할 뿐. 수학적으로 말하면 목적함수에 관한 질문이고 우리는 먼저 이 질문에 집중하겠다. . 대체로 통계학의 알고리듬은 목적 함수를 최적화하는 형태이다. PCA도 마찬가지다. 관찰 대상 $i$(for $i = 1, dotsc, n$)에 관한 $k$ 차원의 피처 벡터 $x_i$가 있다고 하자. $x_i$는 $k times 1$의 칼럼 벡터이다. 앞으로 특별한 언급이 없는 이상 앞으로 $x_i$ 벡터는 $n$개의 관찰에 대한 평균으로 구성된 벡터 $ mu = [ mu^1~ mu^2~ dotsc~ mu^k]^T$를 뺀 값이라고 간주하자. 즉, $X_i$가 평균을 빼지 않은 $i$ 라고 할 때, . xik×1=[Xi1−μ1Xi2−μ2⋮Xik−μk] underset{k times 1}{x_i} = left[ begin{array}{c}{X^1_i - mu^1} {X^2_i - mu^2} { vdots} {X^k_i - mu^k} end{array} right]k×1xi​​=⎣⎢⎢⎢⎢⎡​Xi1​−μ1Xi2​−μ2⋮Xik​−μk​⎦⎥⎥⎥⎥⎤​ . 이제 해당 피쳐를 쏠 스크린으로 활용할 유닛 벡터를 $w$라고 하자. 유닛 벡터란 $w cdot w = 1$를 의미한다. 여기서 스크린이라는 의미는 개별 관찰이 지니는 특징을 이 벡터로 프로젝션해서 그 특징을 요약하겠다는 것이다. 우리에게 익숙한 회귀분석 역시 $y_i$라는 관찰을 설명변수 $ mathbf X$가 형성하는 선형 부분공간으로 프로젝션하는 방법이다. $x_i$를 $w$로 프로젝션 하면 다음과 같다. . Proj⁡w(xi)=w⋅xi∥w∥=w⋅xi operatorname{Proj}_{w}(x_i) = dfrac{w cdot x_i}{ Vert w Vert} = w cdot x_ iProjw​(xi​)=∥w∥w⋅xi​​=w⋅xi​ . 이 프로젝션의 벡터 $w$ 위의 이미지는 $(w cdot x_i) w$가 된다. . 이 프로젝션 스칼라 값 혹은 프로젝션 벡터의 기댓값은 아래와 같이 0이 된다. 1n∑i=1n(w⋅xi)=(1n∑i=1nxi)⋅w=0⋅w=0 dfrac{1}{n} sum^n_{i=1} (w cdot x_i) = left( dfrac{1}{n} sum_{i=1}^n x_i right) cdot w = boldsymbol{0} cdot w = 0n1​∑i=1n​(w⋅xi​)=(n1​∑i=1n​xi​)⋅w=0⋅w=0 . 벡터 $x_i$와 이 프로젝션 이미지 사이의 유클리드 거리를 구해보자. . ∥xi−(w⋅xi)w∥2=∥xi∥2−2(w⋅xi)(w⋅xi)+∥w∥2=∥xi∥2−2(w⋅xi)2+1 begin{aligned} Vert x_i - (w cdot x_i) w Vert^2 &amp; = Vert x_i Vert^2 - 2 (w cdot x_i)(w cdot x_i) + Vert w Vert^2 &amp; = Vert x_i Vert^2 - 2 (w cdot x_i)^2 + 1 end{aligned}∥xi​−(w⋅xi​)w∥2​=∥xi​∥2−2(w⋅xi​)(w⋅xi​)+∥w∥2=∥xi​∥2−2(w⋅xi​)2+1​ . 모든 관찰 수 $n$에 대해서 거리를 구해 더하면 이것이 일종의 MSE(Mean Squared Error)가 된다. . MSE(w)=1n∑i=1n(∥xi∥2−2(w⋅xi)2+1)=1+1n∑i=1n∥xi∥2⏟(∗)−21n∑i=1n(w⋅xi)2 begin{aligned} mathrm{MSE}(w) &amp; = dfrac{1}{n} sum_{i=1}^n left( Vert x_i Vert^2 - 2(w cdot x_i)^2 + 1 right) &amp; = underbrace{1 + dfrac{1}{n} sum_{i=1}^n Vert x_i Vert^2}_{( ast)} - 2 dfrac{1}{n} sum_{i=1}^n (w cdot x_i)^2 end{aligned}MSE(w)​=n1​i=1∑n​(∥xi​∥2−2(w⋅xi​)2+1)=(∗) . 1+n1​i=1∑n​∥xi​∥2​​−2n1​i=1∑n​(w⋅xi​)2​ . MSE를 최소화하는 게 목표라고 하자. 목적함수를 최적화 하기 위해서는 $w$를 조정할 수 있다. $(*)$는 $w$를 포함하고 있지 않으므로 나머지 부분을 최소화하면 MSE가 극대화된다. . 1n∑i=1n(w⋅xi)2=(1n∑i=1nw⋅xi)2+Vari[w⋅xi] dfrac{1}{n} sum_{i=1}^n (w cdot x_i)^2 = left( dfrac{1}{n} sum_{i=1}^n w cdot x_i right)^2 + underset{i}{ mathrm{Var}}[w cdot x_i]n1​i=1∑n​(w⋅xi​)2=(n1​i=1∑n​w⋅xi​)2+iVar​[w⋅xi​] . 이 식이 성립하는 이유는 일반적으로 $ mathrm{Var}(y)= mathrm{E}(y^2) - ( mathrm{E}(y))^2$이 성립하기 때문이다. 그리고 앞에서 보았듯이 $ mathrm{E} (w cdot x_i) = 0$ 성립한다. 따라서 MSE를 최소화한다는 것은 $ mathrm{Var}_i [ cdot]$을 최대화하는 것과 같게 된다. PCA에 분산에 관한 이야기가 자꾸 나오는 것은 이 때문이다. . Variance Maximization . Variance-covariance matrix . 왜 분산이 등장하는지를 파악했으니 분산 최대화를 계산해볼 차례다. 아래 행렬 $X$를 통해 쉽게 분산-공분산 행렬을 나타낼 수 있다. $x_i^j$ 에서 $i (=1,2, dotsc, n)$는 관찰을, $j(=1,2, dotsc,k)$는 피쳐를 나타낸다. . Xn×k=[x1Tx2T⋮xnT]=[x11x12⋯x1kx21x22⋯x2k⋮⋮⋱⋮xn1xn2⋯xnk] underset{n times k}{X} = begin{bmatrix} {x_1}^T {x_2}^T vdots {x_n}^T end{bmatrix} = begin{bmatrix} {x_1^1} &amp; {x_1^2} &amp; { cdots} &amp; {x_1^k} {x_2^1} &amp; {x_2^2} &amp; { cdots} &amp; {x_2^k} { vdots} &amp; { vdots} &amp; { ddots} &amp; { vdots} {x_n^1} &amp; {x_n^2} &amp; { cdots} &amp; {x_n^k} end{bmatrix}n×kX​=⎣⎢⎢⎢⎢⎡​x1​Tx2​T⋮xn​T​⎦⎥⎥⎥⎥⎤​=⎣⎢⎢⎢⎢⎡​x11​x21​⋮xn1​​x12​x22​⋮xn2​​⋯⋯⋱⋯​x1k​x2k​⋮xnk​​⎦⎥⎥⎥⎥⎤​ . 1n−1XTX(k×n)(n×k)=[cov(x1,x1)cov(x1,x2)⋯cov(x1,xk)cov(x2,x1)cov(x2,x2)⋯cov(x2,xk)cov(xk,x1)cov(xk,x2)⋯cov(xk,xk)]=Σ, where begin{aligned} dfrac{1}{n-1} underset{(k times n) (n times k)}{X^{T} X} = begin{bmatrix} text{cov}(x^1, x^1) &amp; text{cov}(x^1, x^2) &amp; cdots &amp; text{cov}(x^1, x^k) text{cov}(x^2, x^1) &amp; text{cov}(x^2, x^2) &amp; cdots &amp; text{cov}(x^2, x^k) text{cov}(x^k, x^1) &amp; text{cov}(x^k, x^2) &amp; cdots &amp; text{cov}(x^k, x^k) end{bmatrix} = Sigma, text{~where} end{aligned}n−11​(k×n)(n×k)XTX​=⎣⎢⎡​cov(x1,x1)cov(x2,x1)cov(xk,x1)​cov(x1,x2)cov(x2,x2)cov(xk,x2)​⋯⋯⋯​cov(x1,xk)cov(x2,xk)cov(xk,xk)​⎦⎥⎤​=Σ, where​ . cov(xi,xj)=1n−1∑k=1nxkixkj text{cov}(x^i, x^j) = dfrac{1}{n-1} sum_{k=1}^{n} x^i_k x^j_kcov(xi,xj)=n−11​k=1∑n​xki​xkj​ . eigenvalue는 어떻게 등장하나? . 임의의 단위 벡터 $w$와 그 프로젝션을 다시 적어보자. 표기를 간단히 하기 위해서 상첨자는 생략한다. 이제 하나의 벡터가 아니라 $X$라는 매트릭스 전체에 대해서 프로젝션을 하면 아래와 같다. . Proj⁡w(X)=Xw∥w∥∈Rn×1 operatorname{Proj}_{w} (X) = dfrac{X w}{ Vert w Vert} in { mathbb R}^{n times 1}Projw​(X)=∥w∥Xw​∈Rn×1 . 이제 극대화의 목적은 이렇게 프로젝션된 이미지의 분산을 가장 크게 만드는 것이다. 앞서의 가정에 따라서 $ mathrm{E}(X) = 0$가 성립함을 기억해두자. . Var(Xw)=1n−1(Xw)T(Xw)=1n−1wTXTXw=1n−1wT(XTX)w=wT(XTXn−1)w=wTΣw begin{aligned} mathrm{Var}(X {w}) &amp;= frac{1}{n-1}(X {w})^{T}(X {w}) &amp;= frac{1}{n-1} {w}^{T} X^{T} X {w} = frac{1}{n-1} {w}^{T} left(X^{T} X right) {w} &amp;={w}^{T} left( frac{X^{T} X}{n-1} right) {w} &amp;={w}^{T} Sigma {w} end{aligned}Var(Xw)​=n−11​(Xw)T(Xw)=n−11​wTXTXw=n−11​wT(XTX)w=wT(n−1XTX​)w=wTΣw​ . 그런데, $w$는 단위벡터임으로 $w cdot w = 1$이다. 이를 제약 조건으로 두고 제약 하의 극대화 문제를 정식화하면 다음과 같다. . L=wT⁡Σw−λ(w⋅w−1){ mathcal L} =w^{ operatorname T} Sigma w - lambda (w cdot w -1)L=wTΣw−λ(w⋅w−1) . ∂L∂w=0=2Σw−2λw∂L∂λ=0=w⋅w−1 begin{aligned} dfrac{ partial mathcal L}{ partial w} &amp; = 0 = 2 Sigma w - 2 lambda w dfrac{ partial mathcal L}{ partial lambda} &amp; = 0 = w cdot w - 1 end{aligned}∂w∂L​∂λ∂L​​=0=2Σw−2λw=0=w⋅w−1​ . 1계 조건을 다시 보자.1 $ Sigma w = lambda w$ 조건이 흥미롭다. 1계 조건이 정확하게 아이겐밸류(eigenvalue, 고유값)와 아이겐벡터(eigenvector, 고유벡터)를 구하는 방법이다. 어떤 매트릭스가 있을 때 해당 매트릭스의 분산-공분산 행렬의 아이겐밸류와 아이겐벡터를 구하면 그 아이겐밸류와 벡터가 바로 MSE를 최적화해주는 값이 된다. 이때 $w$는 아이겐벡터이며 $ lambda$는 아이겐밸류가 된다. 아이겐밸류는 아래 식에서 보듯이 분산이다.2 . Var⁡(Xw)=wTΣw=wT(λw)=λw⋅w=λ operatorname{Var}(X w) = w^{ mathrm T} Sigma w = w^{ mathrm T} ( lambda w) = lambda w cdot w = lambdaVar(Xw)=wTΣw=wT(λw)=λw⋅w=λ . 아마도 최적화 공부를 해본 사람이라면 갸우뚱할지 모르겠다. 1계 조건은 필요 조건이다. 즉, 극대화, 극소화 모두를 그 안에 담고 있을 수 있다. 그렇다면 2계 충분 조건을 따져야 하지 않을까? 그런데, 위 식에 대해서 사실 2계 충분 조건을 따지는 것이 쉽지 않다. 다만 이 문제는 다행스럽게도 지름길이 있다. 위에서 보듯이 1계 조건을 만족하는 $w$는 $ Sigma$의 아이겐벡터다. 따라서 이 아이겐벡터에서만 극대값과 극소값이 존재한다. 1계 조건을 만족하는 아이겐벡터를 $ omega$라고 하자. . L(ω)=ωTΣω−λ(ω⋅ω−1)=ωT(Σw−λw)+λ=λ begin{aligned} { mathcal L}( omega) &amp; = omega^{T} Sigma omega - lambda ( omega cdot omega -1) &amp; = omega^T ( Sigma w - lambda w) + lambda &amp; = lambda end{aligned}L(ω)​=ωTΣω−λ(ω⋅ω−1)=ωT(Σw−λw)+λ=λ​ . 즉, 1계 조건을 만족하는 값에서 목적 함수의 값은 아이겐밸류 $ lambda$가 된다. 그리고 위에서 보았듯이 $ lambda$는 $Xw$의 분산이 되기 때문에 분산이 큰 값의 아이겐벡터가 목적 함수를 극대화하는 $w$가 된다. . 앞서 $ lambda$가 분산이 된다고 말했다. 잠깐, 분산이라면 항상 0보다 커야 하는데, $ lambda$가 0보다 크다는 보장이 있는가? 이 문제를 포함하여 앞에서 정리하지 못한 몇 가지 문제를 모아서 살펴보자. . Properties of var-cov matrix . 분산-공분산 행렬은 아래와 같은 두 가지 특징을 지닌다. . 대칭 행렬 . 우선, 분산-공분산 행렬이므로 대칭이다. 행렬이 대칭일 경우 아이겐밸류는 모두 실수이며 아이겐벡터들은 서로 직교(orthogonal)한다. . For $i, j in { 1, 2, dotsc, k}~ text{with}~i ne j, w^i cdot w^j = 0$, and for $i in { 1, 2, dotsc, k}~, w^i cdot w^i =1$ . 여러개의 프로젝션 스크린 벡터들이 존재할 경우 해당 벡터들이 서로 직교하면 분산값의 합을 최대화하는 것과 MSE를 최소화하는 것이 같은 의미를 지닌다. 이 조건이 분산-공분산 행렬의 속성을 통해 성립한다. . 분산-공분산 행렬의 이 특징이 PCA의 흥미로운 점 하나를 드러난다. 2 차원 평면에서 사 분면을 떠올려보자. 사 분면을 구성하는 $x$, $y$ 축은 서로 직교한다. 2 차원 평면 위에 어떤 관찰에 대해서 PCA를 했다고 하자. PCA의 스크린으로 두 개를 사용했고, 해당 스크린이 아이겐벡터라면 이 두 벡터는 서로 직교한다. 즉, 원래 직교했던 두 축에서 직교하는 다른 두 축으로 좌표의 기준을 이동하는 개념이다. 즉 PCA는 분산을 가장 크게 하는 방식으로 좌표축을 이동하는 방법이라고 이해하면 좋겠다. PCA에 관한 소개에서 아래의 그림처럼 축을 돌린 예시가 자주 등장하는 까닭이기도 하겠다.3 . . Positive-definite . $ Sigma$는 준양정행렬(positive semi-definite) 행렬이다.4 즉, . xTΣx≥0 for any x.x^T Sigma x geq 0 ~ text{for any $x$.}xTΣx≥0 for any x. . wTXTXw=(Xw)T(Xw)닷 프로덕트≥0w^T X^T X w = underset{ text{닷 프로덕트}}{ (Xw)^T (Xw) } geq 0wTXTXw=닷 프로덕트(Xw)T(Xw)​≥0 . 이 경우 모든 아이겐밸류의 값은 음수가 되지 않는다. 앞서 아이겐밸류가 분산이 된다는 사실을 보았다. 아이겐밸류가 음수가 될 수 없고 따라서 분산이 될 수 있다. . Principal component? . 프로젝션 스크린 벡터 $w$에 따른 극대화 문제를 풀면 아이겐밸류와 아이겐벡터를 각각 하나씩 얻게 된다. $k$ 개의 스크린 벡터 혹은 아이겐벡터가 가능하다고 할 때, 분산(아이겐밸류)이 큰 순서대로 아이겐벡터를 정렬한다고 생각해보자. 이렇게 정렬하면 프로젝션 스크린 벡터 중에서 MSE를 더 줄일 수 있는 벡터 순으로 정렬하는 셈이다. 이렇게 분산이 큰 순서대로 나열한 서로 다른 스크린벡터가 바로 주성분(pricipal component)다. . $k$ 개의 주성분 중에서 임의로 $l$ 개를 취한다면(이게 차원 축소가 아닐까?), MSE를 낮추기 위해서는 분산이 큰 순서대로, 즉 아이겐밸류가 큰 순서대로 주성분을 취하면 된다. 이게 PCA를 직관적으로 이해하는 방법이다. . 그런데 한 가지 찜찜한 점이 남는다. 주성분은 이렇게 순서대로 취할 수 있다는 것은 주성분을 결합해서 더 큰 분산을 얻을 수 없을 때만 가능하다. 예컨대, $w_1$과 $w_2$를 적당히 선형결합해 분산을 높일 수 있다면 분산이 큰 순서대로 아이겐벡터를 선택한다는 논의는 깨지게 된다. 이 가능성을 살펴봐야 하겠다. . 프로젝션의 스크린으로 동원되는 벡터가 $w^1, w^2, dotsc, w^k$라고 하자. 이 프로젝션을 통해 생성되는 벡터들이 이루는 부분공간은 다음과 같이 나타낼 수 있다. . ∑j=1k(xi⋅wj)가중치wj sum_{j=1}^k underset{ mathrm{가중치}}{( x_i cdot w^j) } w^jj=1∑k​가중치(xi​⋅wj)​wj . $x_i$와 $w^j$ 모두 $k times 1$ 벡터임을 확인하고 가자. 이 녀석과 $x_i$의 MSE를 최소화하는 문제는 어떻게 될까? 계산이 다소 복잡하니 직관만 짚고 넘어가자. . 앞서 스크린이 하나였던 경우와 마친가지로 $x_i$와 저 값의 내적의 분산을 최대화 해야 한다. | 만일 $w_ cdot$들이 서로 직교한다면, $w_i cdot w_j (i neq j)$는 0이 되어 사라질 것이고, $w_i cdot w_i$(=1)로 구성된 텀만 만게 된다. 결국 | 스크린을 이루는 축들과 $x_i$의 크로스 프로덕트 값의 분산($ mathrm{Var} (x_i cdot w^j)$)을 더한 값을 최대화하는 것이 MSE를 극소화 문제가 된다. 즉, 각각 $w^j$와 $x_i$의 닷 프로덕트의 분산을 최대화하면 된다. 즉, | Vari[∑j=1k(xi⋅wj)wj]=∑j=1kλj underset{i}{ text{Var}}[ sum_{j=1}^k {( x_i cdot w^j) } w^j] = sum_{j=1}^k { lambda^j}iVar​[j=1∑k​(xi​⋅wj)wj]=j=1∑k​λj . 마침내 차원 축소 . 이제 마침내 차원 축소를 다룰 수 있다! 앞서 MSE 최소화 문제에서 보았듯이 분산이 클수록 좋다. 임의의 갯수로 주성분을 취한다고 할 때 의 기준은 분산이 큰 순서이고 분산은 아이겐밸류와 같다. $l(&lt;k)$ 개의 주성분을 취할 때 취할 때 아이겐밸류가 큰 순서대로 취하면 되겠다.5 . Key Questions . Q1. MSE 최소화는 무엇으로 연결되는가? . $Xw$의 분산을 최대화하는 것이다. . Q2. 아이겐밸류 아이겐벡터는 어떻게 등장하는가? . $Xw$의 분산을 $w^T w =1$의 제약하에서 극대화할 때 1계 조건에서 등장한다. . Q3. 아이겐벡터와 아이겐밸류는 어떤 특징을 지니고 있는가? . 1계 조건에서 아이겐벡터와 아이겐밸류를 찾아야 하는 매트릭스는 var-cov 행렬 $ Sigma$다. 그리고 이 행렬은 대칭행렬이며 Positive definite 행렬이다. 이 조건으로부터, 아이겐벡터들은 서로 orthogonal하고, 아이겐밸류는 모두 양수이다. . Q4. 결국 PCA란 무엇인가? . 분산이 큰 순서대로 $k$ 개의 주성분 중에서 임의로 $l(&lt;k)$ 개의 아이겐벡터를 선택하는 것이다. 그리고 이 아이겐벡터는 일종의 피처에 관한 가중치로 이해할 수 있다. . Resource . 이 글은 아래 자료를 바탕으로 만들었습니다. . https://www.stat.cmu.edu/~cshalizi/350/lectures/10/lecture-10.pdf . 사실 여기 적은 1계 조건은 엄밀하지 않다. 이해를 돕기 위해서 여러가지를 퉁쳤는데, 최적화의 결과는 동일하다. 보다 상세한 도출은 여기를 참고하시기 바란다. &#8617; . | 흥미로운 일치를 확인하셨는지? 제약 하 극대화에서 라그랑쥬 승수와 아이겐밸류를 나타내는 수학 기호가 모두 $ lambda$다. 약간 소름 돋는 대목이다. 일치는 여기서 끝나지 않는다. 라그랑쥬 승수는 제약 하의 극대화에서 잠재 가격(shadow price)로 불리기도 한다. 이는 해당 조건이 제약하는 자원의 잠재적인 가치를 나타낸다. 이는 분산이 클수록 MSE가 작다는 PCA의 목적 함수의 해석과 일치한다. &#8617; . | 하나 주의할 점이 있다. 이 그림은 차원 회전에 관한 것이지 차원 축소에 관한 것이 아니다. 즉, 변이가 잘 드러나도록 축을 회전할 수 있다는 예시다. 축소는 다른 문제인데, 아래 본문의 내용에서 보듯이 축을 돌려 변이를 상당 부분 설명했다면 변이의 설명력이 낮은 축들을 제거할 수 있다는 것이 차원 축소다. &#8617; . | 증명은 몹시 간단하다. $w^T Sigma w$ 라고 하자. &#8617; . | 주성분의 갯수를 취하는 방법은 PCA에 관한 튜토리얼에서 항상 등장하는 주제이니 구글링을 해서 확인하시면 되겠다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/machine-learning/2019/05/17/math-pca.html",
            "relUrl": "/math/machine-learning/2019/05/17/math-pca.html",
            "date": " • May 17, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Mathematics of Support Vector Machine",
            "content": "Key Questions . Support Vector Machine (SVM)의 알고리듬을 수학적으로 어떻게 도출할 수 있을까? | 보다 직관적으로 이해할 수 있는 방법은 없을까? | . Key Synopsis . SVM은 기본적으로 최소화(minimize)를 한 후 이를 다시 극대화(maximize)를 하는 최대최소(maxmin) 형태의 최적화 문제이다. 최소화: 분류(classification)의 기준이 되는 두 영역을 나누는 하이퍼플레인을 찾은 후 이 하이퍼플레인과 가장 가깝게 위치하는 두 영역의 벡터(서포트 벡터)를 찾는다. | 최대화: 분류 기준이 되는 하이퍼플레인과 평행한 두 서포트 벡터를 지나는 하이플레인의 거리를 최대화 한다. | . | 이 maxmin 문제를 풀면 목적함수에는 training set에 속한 벡터들의 닷 프로덕트만 남게 되고, 덕분에 최적화 문제가 단순해진다. | 이 닷 프로덕트들로 구성된 부분을 다른 함수 형태로 바꿔서 SVM 알고리듬의 ‘커널’을 유연하게 바꿀 수 있다. 이것이 커널 트릭 (Kernel trick)이다. | . SVM Mathematically . Preliminary concepts . Length of a vector . 벡터 ${ bf x} = (x_1, x_2, dotsc, x_n)$ 의 벡터의 길이, 즉 유클리드 노름(norm), 은 다음과 같이 정의된다. . ∥x∥=x12+x22+…+xn2 Vert { bf x} Vert = sqrt{x_1^2 + x_2^2 + dotsc + x_n^2}∥x∥=x12​+x22​+…+xn2​ . ​ . Direction of a vector . 벡터 $ bf x$의 방향성 $ bf w$는 다음과 같이 정의할 수 있다. . w=(x1∥x∥,x2∥x∥) mathbf{w} = left( dfrac{x_1}{ Vert bf x Vert }, dfrac{x_2}{ Vert bf x Vert } right)w=(∥x∥x1​​,∥x∥x2​​) . 그림으로 나타내보자. . . 이는 다음과 같이 삼각함수로 표시할 수 있다. w=(cos⁡ θ,cos⁡ α) mathbf{w} = left( cos~ theta, cos~ alpha right)w=(cos θ,cos α) . Dot product (inner product) . 내적은 벡터 연산의 일종으로, 이는 두 벡터를 스칼라 값으로 바꿔주는 일종의 함수다. . . cos⁡θ=cos⁡(β−α)=cos⁡βcos⁡α+sin⁡β sin⁡α=x1∥x∥y1∥y∥+x2∥x∥y2∥y∥=x1y1+x2y2∥x∥∥y∥ begin{aligned} cos theta &amp; = cos ( beta - alpha) &amp; = cos beta cos alpha + sin beta sin alpha &amp; = dfrac{x_1}{ Vert rm x Vert} dfrac{y_1}{ Vert rm y Vert} + dfrac{x_2}{ Vert rm x Vert} dfrac{y_2}{ Vert rm y Vert} &amp; = dfrac{x_1 y_1 + x_2 y_2}{ Vert rm x Vert Vert rm y Vert} end{aligned}cosθ​=cos(β−α)=cosβcosα+sinβ sinα=∥x∥x1​​∥y∥y1​​+∥x∥x2​​∥y∥y2​​=∥x∥∥y∥x1​y1​+x2​y2​​​ . 이를 아래와 같이 정리할 수도 있다. . x⋅y=∥x∥∥y∥cos⁡θ rm x cdot rm y = Vert rm x Vert Vert rm y Vert cos thetax⋅y=∥x∥∥y∥cosθ . Hyperplane . . $n$ 차원 공간을 가를 수 있는 해당 공간의 차원보다 하나 낮은 수학적 관계라고 풀어서 쓸 수 있다. . 즉, $x_1$이나 $x_2$ 중 하나만 주어지면 나머지 위치가 주어진다. | 쉽게 $y = x + 1$의 직선을 생각하면 된다. 2차원 평면에서 $x$의 값이 주어지면 y값이 정해진다. 이 직선은 2차원 평면에 위치하지만 사실상 1차원의 속성을 지니게 된다. | . ${ bf x} = (x_1, x_2)$의 벡터가 있다고 할 때, 하이퍼플레인은 벡터 $ bf w$와 $b$에 의해 정의된다. 즉, . w⋅x+b=0 mathbf{w} cdot { bf x} + b = 0w⋅x+b=0 . Classifier . 하이퍼플레인을 기준으로 클래시파이어를 다음과 같이 정의한다. 특정한 관찰 벡터 $ bf x$가 있다고 하자. 이때 분류 $h$의 정의는 아래와 같다. . h(x)={+1ifw⋅x+b≥0−1ifw⋅x+b&lt;0h({ bf x}) = begin{cases} +1 hspace{3em} &amp; text{if} hspace{1em} mathbf{w} cdot { bf x} + b geq 0 -1 hspace{3em} &amp; text{if} hspace{1em} mathbf{w} cdot { bf x} + b &lt; 0 end{cases}h(x)=⎩⎪⎪⎨⎪⎪⎧​+1−1​ifw⋅x+b≥0ifw⋅x+b&lt;0​ . Explained Visually . 그림으로 보다 직관적으로 이해해보자. . . 어떤 원점을 기준으로 training example까지의 벡터를 ${ bf x}_i$라고 하자. 이때 둘을 가르는 하이퍼플레인이 있을 때 이와 직교하는 벡터 (orthogonal vector) $ mathbf{w} $를 생각해보자. 왜 orthogonal해야 하는가? 잠시 후 그 이유를 알 수 있다. 하이퍼플레인은 기본적으로는 두 벡터 사이의 닷 프로덕트다1. 닷 프로덕트를 그림으로 나타낼 수 있는 방법은 이를 projection으로 생각해보는 것이다. . 즉, ${ bf x}_i$를 $ mathbf{w} $로 프로젝션을 한다면(projection of ${ bf x}_i$ on $ mathbf{w} $), 이는 . Projwxi=w⋅xi∥w∥ text{Proj}_ mathbf{w} { bf x}_i = dfrac{ mathbf{w} cdot{ bf x}_i}{ Vert bf w Vert}Projw​xi​=∥w∥w⋅xi​​ . 닷 프로덕트의 부분이 시각적으로는 projection 결과 곱하기 $ Vert bf w Vert$로 나타난다. 즉, ${ bf x}_i$에서 $ bf w$를 향해 내린 선분이 프로젝션이고 이를 $ Vert bf w Vert$로 스케일링 한 $ bf w$ 위에서의 길이가 닷 프로덕트를 시각적으로 나타낸 것이다. 이 프로젝션의 길이에 따라서 해당 트레이닝 샘플이 어떤 것으로 분류될지에 관해서 파악할 수 있다. $ bf Vert w Vert$가 고정되어 있다고 하면, 프로젝션의 크기가 일정 숫자보다 크면 분류의 오른쪽에 작으면 분류의 왼쪽에 위치하는 것이다. 이를 아래와 같이 표시해보자. . w⋅xr+b≥1 mathbf{w} cdot { bf x}_{ mathrm r} + b geq 1w⋅xr​+b≥1 . w⋅xl+b≤1 mathbf{w} cdot { bf x}_{ mathrm l} + b leq 1w⋅xl​+b≤1 . 프로젝션의 길이가 일정한 기준보다 길면 오른쪽에 짧으면 왼쪽에 위치한 것으로 분류할 수 있다. 이 조건을 $y_i$와 함께 나타내보자. 즉, . yi(w⋅xi+b)−1≥0y_i ( mathbf{w} cdot { bf x}_ i + b) - 1 geq 0yi​(w⋅xi​+b)−1≥0 . 앞서 분류기에서 해당 값이 0보다 크면 $y_i ( mathbf{w} cdot { bf x}_ i + b) - 1 geq 0$가 성립한다. 반면, 해당 값이 0보다 작으면 음수를 곱하는 것이 되어 부등호가 바뀌게 되고, 이 경우 역시 위의 식이 성립한다. . . 이제 $ cos theta$를 벡터 ${ bf x}_{ rm svr} - { bf x} _{ rm svl}$와 $ mathbf{w}$가 이루는 각이라고 생각하자. 이때 $ mathbf{w}$는 하이퍼플레인과 orthogonal하며 적절한 training sample 즉, 적절한 하나의 서포트 벡터를 지난다. 이때 $ cos theta$는 다음과 같이 쉽게 정의된다.2 . cos⁡θ=(xsvr−xsvl)⋅w∥xsvr−xsvl∥∥w∥ cos theta = dfrac{({ bf x} _ { rm svr} - { bf x} _ { rm svl}) cdot bf w}{ Vert { bf x} _ { rm svr} - { bf x} _ { rm svl} Vert Vert mathbf{w} Vert}cosθ=∥xsvr​−xsvl​∥∥w∥(xsvr​−xsvl​)⋅w​ . 한편, 하이퍼플레인과 평행하면서 서포트 벡터를 지나가는 하이퍼플레인의 거리 $ Delta_{ bf x}$는 다음과 같다. . Δx∥xsvr−xsvl∥=cos⁡θ=(xsvr−xsvl)⋅w∥xsvr−xsvl∥∥w∥ dfrac{ Delta _ { bf x} }{ Vert { bf x} _ { rm svr} - { bf x} _ { rm svl} Vert } = cos theta = dfrac{({ bf x} _ { rm svr} - { bf x} _ { rm svl}) cdot bf w}{ Vert { bf x} _ { rm svr} - { bf x} _ { rm svl} Vert Vert mathbf{w} Vert}∥xsvr​−xsvl​∥Δx​​=cosθ=∥xsvr​−xsvl​∥∥w∥(xsvr​−xsvl​)⋅w​ . 따라서 . Δx=(xsvr−xsvl)⋅w∥w∥ Delta _ { bf x} = dfrac{({ bf x} _ { rm svr} - { bf x} _ { rm svl}) cdot bf w}{ Vert mathbf{w} Vert}Δx​=∥w∥(xsvr​−xsvl​)⋅w​ . $y_i ( mathbf{w} cdot { bf x}_ i + b) - 1 = 0$의 양변에 $y_i$를 곱하면, $y_i^2 ( mathbf{w} cdot { bf x}_ i + b) = y_i$가 된다. $y_i^2 =1$이므로, . xsvr⋅w+b=1xsvl⋅w+b=−1 begin{aligned} { bf x}_ { rm svr} cdot mathbf{w} + b &amp; = 1 { bf x}_ { rm svl} cdot mathbf{w} + b &amp; = -1 end{aligned}xsvr​⋅w+bxsvl​⋅w+b​=1=−1​ . 여기서 $({ bf x} _ { rm svr} - { bf x} _ { rm svl}) cdot mathbf{w} = 2$를 쉽게 도출할 수 있다. 결론적으로 두 서포트 벡터 사이의 거리를 최대화하는 문제는 $ Vert bf w Vert$를 최소화하는 문제와 같다. . Optimization for SVM . Metrics to compare hyperplanes . Defining functional margin . $f_i = y_i( mathbf{w} cdot { bf x}_i + b)$가 있다고 하자. 이때 분류가 제대로 이루어졌다면, $f_i$의 부호는 언제나 양수다. 위의 분류의 정의에 따르면 그렇다. 데이터 셋 $D$의 정의는 다음과 같다. . D={(xi,yi)∣xi∈Rn, yi∈{−1,1}}i=1mD = left lbrace ({ bf x}_i, y_i) mid { bf x}_ i in mathbb R^n,~y_ i in lbrace -1, 1 rbrace right rbrace_{i=1}^mD={(xi​,yi​)∣xi​∈Rn, yi​∈{−1,1}}i=1m​ . 펑셔널 마진(functional margin)이라고 불리는 $F$ 는 다음과 같다. . F=min⁡i=1,…,myi(w⋅xi+b)F = min_{i = 1, dotsc, m} y_i( mathbf{w} cdot { bf x} _i + b )F=i=1,…,mmin​yi​(w⋅xi​+b) . $ mathbf{w}$와 $b$로 정의되는 하이퍼플레인이 모든 트레이닝 셋을 잘 분류했다면, $f_i &gt; 0$가 성립한다. 이 $f_i$ 중 가장 작은 값이 functional margin이다. 그리고 두 번째로 서로 다른 하이퍼플레인 중에서 가장 큰 $F$를 지니는 하이퍼플레인이 최적이 하이퍼플레인이다. . $F$를 얻기 위한 과정에서 최소화 로직이 들어간다. 즉, 해당 하이퍼플레인과 가장 가깝게 위치한 관측치를 얻어내는 과정 | 이렇게 얻어낸 $F$들을 서로 다른 하이퍼플레인들 사이에 비교하고, 가장 큰 $F$를 주는 하이퍼플레인을 채택한다. | 표준화를 위해서 $ bf w$의 norm으로 $f_i$ 값을 나누고 극대화 문제를 정식화하면 아래와 같다. . Derivation of SVM optimization problem . 표준화를 위해서 $ Vert bf w Vert$로 목적함수와 제약을 나누자. . max⁡w,bMs.t.γi≥Mfori=1,…,m max_{ mathbf{w} , b} M hspace{1em} text{s.t.} hspace{1em} gamma_i geq M hspace{1em} text{for} hspace{1em}i = 1, dotsc, mw,bmax​Ms.t.γi​≥Mfori=1,…,m . where . γi=yi(w∥w∥⋅xi+b∥w∥) gamma_i = y_i left( dfrac{ mathbf{w} }{ Vert mathbf{w} Vert} cdot { bf x}_i + dfrac{b}{ Vert mathbf{w} Vert} right)γi​=yi​(∥w∥w​⋅xi​+∥w∥b​) . M=min⁡i=1,…,mγiM = min_{i=1, dotsc, m} gamma_iM=i=1,…,mmin​γi​ . 표준화된 펑셔널 마진을 최대화하되, 트레이닝 샘플들이 이것보다 커야 한다는 조건(최소화)이 제약으로 들어간다. 즉, 아래의 식은 최소화 제약 하에서 $F$를 최대화한다는 이중 최적화 과정을 보여 준다. . max⁡w,bF∥w∥s.t.fi≥Ffori=1,2,…,m max_{ mathbf{w} , b} dfrac{F}{ Vert w Vert} hspace{1em} text{s.t.} hspace{1em}f_i geq F hspace{1em} text{for} hspace{1em}i = 1,2, dotsc, mw,bmax​∥w∥F​s.t.fi​≥Ffori=1,2,…,m . 위 극대화 문제에서 모든 변수는 상대값으로 정의할 수 있으므로 $F$를 1로 제한해도 해는 바뀌지 않는다. 그리고 아래와 같은 차례로 정식화할 수 있다. . max⁡w,b1∥w∥s.t.fi≥1fori=1,2,…,m max_{ mathbf{w} , b} dfrac{1}{ Vert w Vert} hspace{1em} text{s.t.} hspace{1em}f_i geq 1 hspace{1em} text{for} hspace{1em}i = 1,2, dotsc, mw,bmax​∥w∥1​s.t.fi​≥1fori=1,2,…,m . min⁡w,b∥w∥s.t.fi≥1fori=1,2,…,m min_{ mathbf{w} , b} { Vert w Vert} hspace{1em} text{s.t.} hspace{1em}f_i geq 1 hspace{1em} text{for} hspace{1em}i = 1,2, dotsc, mw,bmin​∥w∥s.t.fi​≥1fori=1,2,…,m . min⁡w,b12∥w∥2s.t.fi≥1fori=1,2,…,m min_{ mathbf{w} , b} dfrac{1}{2}{ Vert w Vert}^2 hspace{1em} text{s.t.} hspace{1em}f_i geq 1 hspace{1em} text{for} hspace{1em}i = 1,2, dotsc, mw,bmin​21​∥w∥2s.t.fi​≥1fori=1,2,…,m . Optimization by Wolfe duality . 제약 하의 극대화 문제이므로 라그랑주 최적화로 바뀌서 볼 수 있다. 다음과 같이 라그랑주 방정식을 정의하자. . L(w,b,α)=12w⋅w−∑i=1mαi[yi(w⋅x+b)−1]{ mathcal L}( mathbf{w} , b, { boldsymbol alpha}) = frac{1}{2} mathbf{w} cdot mathbf{w} - sum_{i=1}^m alpha_i left [ y_i ( mathbf{w} cdot { bf x} + b) -1 right]L(w,b,α)=21​w⋅w−i=1∑m​αi​[yi​(w⋅x+b)−1] . 여기서 벡터 $ boldsymbol alpha$는 라그랑주 최적화의 라그랑주 승수로 제약식을 반영하는 부분이다. 일단, $ alpha_i$를 무시하고 두 최적화 변수인 $ bf w$와 $b$ 에 대해서면 1계 조건을 풀면 다음과 같다. . ∇wL(w,b,α)=w−∑imαiyixi=0∇bL(w,b,α)=−∑imαiyi=0 begin{aligned} nabla_ mathbf{w} { mathcal L}( { mathbf{w}, b, boldsymbol alpha} ) &amp; = mathbf{w} - sum_{i}^{m} { alpha_i} {y_i} {x_i} = 0 nabla_{b} { mathcal L}({ mathbf{w}, b, boldsymbol alpha}) &amp; = - sum_{i}^{m} alpha_i y_i = 0 end{aligned}∇w​L(w,b,α)∇b​L(w,b,α)​=w−i∑m​αi​yi​xi​=0=−i∑m​αi​yi​=0​ . 이 녀석들을 다시 라그랑주 방정식에 대입하면, $ boldsymbol alpha$로만 된 일종의 maximal function 혹은 라그랑주 방정식의 하한(infimum)을 만들 수 있다. Wolfe duality에 따르면, 최소화 최대화를 한 번에 푸는 것과 한 가지 문제를 먼저 푼 후 해당 결과를 목적 함수에 넣고 두 번째 문제를 순차적으로 푸는 것이 동일하다. 따라서 위의 일계 조건을 목적 함수에 넣은 목적함수는 구해보자. . W(α)=∑i=1mαi−12∑i=1m∑j=1mαiαjyiyjxi⋅yjW( boldsymbol alpha) = sum_{i=1}^m alpha_i - dfrac{1}{2} sum_{i=1}^{m} sum_{j=1}^m alpha_i alpha_j y_i y_j { bf x}_i cdot { bf y}_jW(α)=i=1∑m​αi​−21​i=1∑m​j=1∑m​αi​αj​yi​yj​xi​⋅yj​ . 이제 문제는 $ boldsymbol alpha$에 관해서 극대화 문제를 푸는 것으로 바뀐다. 즉, . max⁡αW(α)s.t.αi≥0,∑i=1mαi(yi(w⋅x∗+b)−1)=0 max_{ boldsymbol alpha} W( { boldsymbol alpha} ) hspace{1em} text{s.t.} hspace{1em}{ alpha_i} geq 0, sum_{i=1}^m alpha_i { left( y_i ( mathbf{w} cdot { bf x}^* + b) -1 right)} = 0αmax​W(α)s.t.αi​≥0,i=1∑m​αi​(yi​(w⋅x∗+b)−1)=0 . 제약 부분이 부등식이므로 KT(Kuhn-Tucker) 조건에 따라서 풀면 된다. . αi[yi(w⋅x∗+b)−1]=0 alpha_i left[ y_i ( mathbf{w} cdot { bf x}^* + b) -1 right] = 0αi​[yi​(w⋅x∗+b)−1]=0 . KT 조건이란 부등식 제약을 푸는 테크닉이다. 즉, $ alpha_i &gt;0$의 제약이 유효하다면 제약을 만족시키기 위해서는 $y_i ( mathbf{w} cdot { bf x}^* + b) -1 = 0$이 만족해야 한다. 이렇게 제약이 걸리는 경우에 위치한 $x^*$가 바로 ‘서포트 벡터’다. 반면, $ alpha_i =0$는 제약이 등호로 걸릴 필요가 없는 트레이닝 셋의 관찰들이다. 이들은 분류 하이퍼플레인까지의 길이가 서포트 벡터의 길이보다 크다. . Compute w and b . $ bf w$ 의 경우 1계 조건에서 쉽게 얻을 수 있다. . w−∑i=1mαiyixi=0 mathbf{w} - sum_{i=1}^m alpha_i y_i { bf x} _i = 0w−i=1∑m​αi​yi​xi​=0 . 한편, $b$의 경우 서포트 벡터의 경우 위에서 본 것 같이 제약 식의 등호가 성립한다. 즉, 서포트 벡터를 $x^*$라고 할 때, . yi(w⋅x∗+b)−1=0y_i ( mathbf{w} cdot { bf x}^* + b) -1 = 0yi​(w⋅x∗+b)−1=0 . 양변에 $y_i$를 곱하면, $y_i^2 = 1$이므로, | . b=yi−w⋅x∗b = y_i - mathbf{w} cdot { bf x}^*b=yi​−w⋅x∗ . 서포트 벡터가 S개 존재할 경우라면, | . b=1S∑i=1S(yi−w⋅xi∗)b = dfrac{1}{S} sum_{i=1}^S left( y_i - mathbf{w} cdot { bf x}^*_i right)b=S1​i=1∑S​(yi​−w⋅xi∗​) . Limitation . 앞서 본 것을 이른바 hard margin SVM이다. 즉, 서포트 벡터 사이의 마진을 획일적으로 적용하는 분류 알고리듬이다. 하드 마진 SVM은 다음의 두가지 경우에 취약하다. . 데이터에 노이즈가 있을 경우 . 하드 마진의 큰 문제는 아웃라이어에 취약할 수 밖에 없다는 것이다. 풀어서 말하면 제약이 선형이고 그 제약이 강력하다는 데 있다. 트레이닝 데이터에 이런 저런 노이즈가 있을 경우 하드 마진은 아예 계산이 불가능할 수 있다. 이때 사용하는 기법이 soft margin SVM이다. . 데이터 자체가 선형으로 분리되지 않을 경우 . 애초부터 데이터 자체가 선형을 통한 분류를 허용하지 않을 경우에는 비선형 분류를 사용할 수 있다. 이때 동원하는 테크닉이 커널 트릭 (kernel trick)이다. . Soft Margin SVM . 제약을 약간 풀어주는 $ zeta$를 도입하여 최적화 문제를 정식화하면 아래와 같다. . min⁡w,b,ζ12∥w∥2+C∑i=1mζi s.t yi(w⋅xi+b)≥1−ζi for i=1,2,…,m min_{ mathbf{w}, b, { boldsymbol zeta}} dfrac{1}{2} Vert mathbf{w} Vert^2 + C sum_{i=1}^m zeta_i~ text{s.t}~ y_i ( mathbf{w} cdot { bf x}_i + b) geq 1 - zeta_i~ text{for}~ i = 1,2, dotsc, mw,b,ζmin​21​∥w∥2+Ci=1∑m​ζi​ s.t yi​(w⋅xi​+b)≥1−ζi​ for i=1,2,…,m . 문제를 풀면 . max⁡α∑i=1mαi−12∑i=1m∑j=1mαiαjyiyjxi⋅xjs.t. max_{ boldsymbol alpha} sum_{i=1}^m alpha_i - dfrac{1}{2} sum_{i=1}^m sum_{j=1}^{m} alpha_i alpha_j {y_i} {y_j} { bf x}_i cdot { bf x}_j hspace{1em} text{s.t.}αmax​i=1∑m​αi​−21​i=1∑m​j=1∑m​αi​αj​yi​yj​xi​⋅xj​s.t. . 0≤αi≤Cfori=1,2,…,m0 leq alpha_i leq C hspace{1em} text{for} hspace{1em}i=1,2, dotsc, m0≤αi​≤Cfori=1,2,…,m . ∑i=1mαiyi=0 sum_{i=1}^m alpha_i y_i = 0i=1∑m​αi​yi​=0 . What is C . 정규화 파라미터인 $C$를 어떻게 이해할까? 식 그대로 보면, $ boldsymbol zeta$를 얼마나 중요하게 최적화 문제에 반영할 것인지가 중요하다. 다시 말하면 이는 에러를 어떻게 볼 것인가와 연결되어 있다. 만일 $C$가 양의 무한대 값이라면 이는 하드 마진과 동일하다. 만일 $C=0$ 이라면 $ alpha_i=0$가 된다. 이는 사실상 선형 제약이 사라지게 되는 결과가 된다. 따라서 하이퍼플레인이 어떤 것도 분류하기 못하게 될 것이다. 즉, C는 하드 마진과 소프트 마진 사이를 적절하게 조절하는 역할을 수행한다. . Kernel Trick . 마지막에 얻은 극대화 문제를 살펴보면, training set이 관련된 부분이 딱 하나 밖에 없다. ${ bf x}_i cdot { bf x}_j$ 뿐이다. 따라서 이 부분을 유연하게 조정해줌으로써 비선형적 형태의 분류를 만들 수 있는 것이다. 앞서 봤던 하드 마진 SVM은 선형 커널을 사용한다. 즉, . K(xi,xj)=xi⋅xjK({ bf x}_i, { bf x}_j) = { bf x}_i cdot { bf x}_jK(xi​,xj​)=xi​⋅xj​ . 이런 커널만 있을 형태는 없다. 여러가지 함수를 커널로 취할 수 있을 것이다. 가장 많이 사용되는 커널은 RBF(Radial Basis Function) 혹은 가우시안이라고 한다. . K(xi,xj)=exp⁡(−γ∥xi−xj∥)K({ bf x}_i, { bf x}_j) = exp left( - gamma Vert { bf x}_i - { bf x}_j Vert right)K(xi​,xj​)=exp(−γ∥xi​−xj​∥) . $ gamma$ 값이 충분히 작으면 선형 커널과 비슷하게 작동한다. | $ gamma$가 크면 서포트 벡터에게 크게 영향 받는다. | . Resource . 이 자료는 아래 내용을 바탕으로 만들어졌습니다. ﻿ . Basic: LINK | Illustration: LINK | . 내적이라고 번역되기도 하지만 여기서는 그냥 ‘닷 프로덕트’라고 쓰리고 하겠다. &#8617; . | 벡터의 방향에 대해서 약간 갸우뚱하는 분들이 있을지 모르겠다. $ cos theta$를 제대로 정의하기 위해서는 $-({ bf x}{ rm svr} - { bf x}{ rm svl})$, $- mathbf{w}$라고 쓰는 것이 맞을 것이다. 하지만, 둘의 닷 프로덕트를 구하면 서로 상쇄되어 아래 적은 것과 동일하다. &#8617; . |",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/machine-learning/2019/05/06/math-svm.html",
            "relUrl": "/math/machine-learning/2019/05/06/math-svm.html",
            "date": " • May 6, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Weighing Problem of Bachet de Méziriac",
            "content": "Problem . 수리 퍼즐로 유명한 제수이트파 수도사 바셰(Bachet de Méziriac)의 저울 문제를 함 풀어보자. 문제는 다음과 같다. . 1~40 사이의 임의의 무게를 지니는 어떤 짐이 있다고 하자. | 이 짐의 무게를 저울에 달아 무게를 파악하는 것이 목적이다. | 자연수 스케일을 지니는 임의의 추를 이용할 수 있다. | 추를 달 때 저울 양쪽을 모두 이용할 수 있다. 즉, 무게를 파악하는 데 짐을 단 쪽도 추를 올려 놓을 수 있다. | . 이때 1~40 사이의 임의의 무게를 지닌 짐의 무게를 측정하기 위해 필요한 최소한 추는 어떤 것인가? . Key insight . 사실 일종의 인덕션 문제다. 그런데 문제의 핵심에 접근하기는 쉽지는 않다. 우선 표기법부터 정하고 가자. 자연수 $x$라는 무게를 지닌 추는 $x$W으로 표기한다. 즉, $10$W는 10이라는 무게를 지닌 추를 의미한다. 필요한 경우 W앞에 괄호를 붙여 사용하도록 하자. 아래에서 $n$은 해당 상황에서 적절하게 큰 자연수를 나타낸다. . 이제, ${ 1, 2, dotsc, n }$의 무게를 모두 표현할 수 있는 추의 집합 $ omega$가 있다고 하자. | 여기에 $2n+1$의 무게를 지닌 추 $(2n+1)$W 하나를 추가해보자. 추 집합 $ omega$에 $(2n+1)$W 라는 추 하나를 추가했을 때 추가로 표현할 수 있는 무게는 얼마나 확장될까? 먼저 $ omega$에 이 새로운 추를 추가한 추 집합을 $ omega’$이라고 하자. 즉, | . ω+={w∣w∈ω∪{(2n+1)W}} omega^+ = { w | w in omega cup { (2n+1) { rm W} } }ω+={w∣w∈ω∪{(2n+1)W}} . $(2n+1)$W를 추가해 더 측정할 수 있는 무게는 다음과 같다. 당연히 무게 $2n+1$이 추가된다. | ${ 2n+2, 2n+3, dotsc, 3n+1 }$의 무게를 이 추 조합으로 측정할 수 있다. 짐이 해당 범위의 무게를 지닌다면, 짐을 왼쪽에 그리고 오른쪽에 $(2n+1)$W와 $ omega$를 적절하게 올리면 된다. | 다음으로 짐의 무게가 ${ n+1, n+2, dotsc, 2n}$에 속한 경우에도 $ omega$ 그리고 $(2n+1)$W의 조합으로 무게 측정이 가능하다. 이는 $(2n+1)$W를 오른쪽에 두고, 왼쪽에 짐과 $ omega$를 조합함으로써 저울의 균형을 맞출 수 있다. | . | 결론적으로 추 집합 $ omega^+$으로 ${ 1,2, dotsc, n, n+1, n+2, dotsc, 2n, 2n+1, 2n+2, dotsc, 3n+1}$ 무게를 측정할 수 있다. | . Induction . 위의 사실을 알고 있는 상태에서 이제 induction을 통해 해법을 알아보자. 위의 예에서 무게 $n$ 까지 측정할 수 있는 추의 조합 $ underline omega$가 무게의 측정을 위해 필요한 추의 최소 집합이라면, 하나를 추가해 측정할 수 있는 $3n+1$의 무게까지는 기존 $ underline omega$ 에 $(2n+1)$W가 원소로 추가된 집합, 즉 추 집합 $ underline omega^+$가 최소 추의 조합이 된다. $ underline omega$에 더해진 추는 단 하나이기 때문이다. . 만일, $n+1$ 무게까지의 $ underline omega$ 추 조합으로 측정이 가능했다면 이는 애초에 $n$까지 측정하는 최소 추 집합이라는 $ underline omega$의 정의에 위배된다. -만일 $w^*$가 ${ 1, 2, dotsc, n, n+x} (x = 1, 2, …, 2n+1)$를 측정하기 위해 필요한 표현하는 최소의 추 조합의 집합이다. 그리고 정의에 따라서이라면? 이런 경우는 어떻게 생각해야 할까? 이 경우 $ underline omega$은 $n+1$까지 측정하기 위해 필요한는 최소의 추 조합은 아니라고 하자. 이때될 수 없어야 한다. 그렇다면, $n+1$의 무게를 측정하기 위해서는 최소한 하나 이상의 추가 필요하다. 이때 앞서 보았듯이 $(2n+1)$W의 추를 추가함으로써 $n+1$, $n+x$를 모두 측정할 수 있음을 보았표시했다. 즉, 1부터 $n+x$까지 순차적으로 존재하는 자연수 무게를 하나 더 측정하기 위해 필요한 최소 추 집합의 집합에는 $ underline omega^+$가 반드시 속하게 된다. | . 이제 인덕션을 전개해보자. 포함된다. . 무게 $1$을 재기 위한 최소한의 추 조합은 $1$W이다. | $2 times 1+1$의 무게를 지니는 $3$W의 추를 추가하면, $1$W, $3$W의 조합은 ${1,2,3,4}$ 무게를 측정할 수 있다. 여기서 $4$는 $3 times 1+1$에서 비롯한다. | $2 times 4+1$의 무게를 지니는 $9$W의 추를 추가하면, $1$W, $3$W, $9$W의 조합은 ${1,2, dotsc, 13}$ 무게를 측정할 수 있다. $13$은 $3 times 4+1$에서 나온다. | 여기에 $2 times 13+1$의 무게를 지니는 $27$W의 추를 추가하면, $1$W, $3$W, $9$W, $27$W의 조합은 ${1,2, dotsc, 40}$의 무게를 측정할 수 있다. | . 표로 정리해보자. . 추가로 필요한 추의 무게 측정 가능한 무게의 상한 . $1$ | $1$ | . $2 times 1 + 1 = 3$ | $3 times 1 + 1 = 4$ | . $2 times 4 + 1 = 9$ | $3 times 4 + 1 = 13$ | . $2 times 13 + 1 = 27$ | $3 times 13 + 1 = 40$ | .",
            "url": "https://anarinsk.github.io/lostineconomics-v2-1/math/quiz/2018/09/17/Bachet.html",
            "relUrl": "/math/quiz/2018/09/17/Bachet.html",
            "date": " • Sep 17, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Me . 허준석 Junsok Huhh | Ph.D. in Economics. Currently works as a data scientist (somewhere in South Korea). . 안녕하세요. 허준석의 블로그입니다. 쓸데 없고 목적 없는 블로그입니다. 뭐라도 도움이 되면 좋겠습니다. 이 블로그는 v2입니다. 저에 관한 사항 및 이전 블로그의 역사는 아래 “Page history”에서 확인하실 수 있습니다. 블로그의 이름 “lost in economics”는 현재 저 자신의 처지를 나타냅니다. 경제학에서 길을 잃었지만 여전히 그 안에 빠져 있습니다. 블로그는 fastpages로 제작되었습니다. . Hi, I’m Junsok Huhh. If you are interested in me, check my c.v. and contact. The blog name, “lost in economics” has double meaning. In one way, I lost my way in economics, but I am still finding somehting in it. This blog is built with fastpages. . contact . twitter | email . c.v. . long | short . History of lostineconomics.com . v1 built with markdown on github | v0 built with BlogDown on github | .",
          "url": "https://anarinsk.github.io/lostineconomics-v2-1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

}